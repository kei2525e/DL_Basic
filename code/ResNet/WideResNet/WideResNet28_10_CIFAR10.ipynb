{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### ドライブのマウント"
      ],
      "metadata": {
        "id": "m-QbZLFlyjCA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9g8jY7G9ahIa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5184f0a-f04f-4137-8175-a0383a72b084"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Googleドライブをマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ライブラリ・モジュールのインポート"
      ],
      "metadata": {
        "id": "cdzvZUWNzQAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ライブラリの準備\n",
        "!pip install timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from timm.scheduler import CosineLRScheduler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import random\n",
        "import pickle"
      ],
      "metadata": {
        "id": "l8wfSjPuboGS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dc17c30-3572-4227-e8af-9d8b7c127afd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/2.2 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.15.2+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Collecting huggingface-hub (from timm)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors (from timm)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (16.0.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "Installing collected packages: safetensors, huggingface-hub, timm\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 timm-0.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### シード値の設定"
      ],
      "metadata": {
        "id": "_8vKbM9TzkCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# シード値を設定\n",
        "def fix_seed(seed=1234):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "fix_seed(seed=1234)"
      ],
      "metadata": {
        "id": "pGX4Zk1LbtPr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### データセットの準備"
      ],
      "metadata": {
        "id": "9aOdldqfyoSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 平均値と標準偏差を計算するためのCIFAR10データセットの前処理を定義\n",
        "pre_transform_cifar10 = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# 平均値と標準偏差を計算するためのCIFAR10データセットの読み込み\n",
        "pre_train_dataset_cifar10 = datasets.CIFAR10(root='/content/data/', download=True, transform=pre_transform_cifar10)\n",
        "\n",
        "# 平均値と標準偏差を計算するための変数を初期化\n",
        "pre_mean_cifar10 = 0.0\n",
        "pre_std_cifar10 = 0.0\n",
        "pre_total_samples_cifar10 = len(pre_train_dataset_cifar10)\n",
        "\n",
        "# データセットのすべてのデータポイントに対して平均値と標準偏差を計算\n",
        "for data in pre_train_dataset_cifar10:\n",
        "    pre_image, _ = data\n",
        "    pre_mean_cifar10 += pre_image.mean(dim=(1, 2))  # テンソルのチャンネルごとに平均を計算\n",
        "    pre_std_cifar10 += pre_image.std(dim=(1, 2))    # テンソルのチャンネルごとに標準偏差を計算\n",
        "\n",
        "# データセット全体の平均値と標準偏差を計算\n",
        "pre_mean_cifar10 /= pre_total_samples_cifar10\n",
        "pre_std_cifar10 /= pre_total_samples_cifar10\n",
        "\n",
        "print(\"データセット全体の平均値: \", pre_mean_cifar10)\n",
        "print(\"データセット全体の標準偏差: \", pre_std_cifar10)"
      ],
      "metadata": {
        "id": "RLSipTJ0bu1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1c0e65e-2f59-46f1-e188-2515df7f753c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /content/data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:13<00:00, 13046856.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/data/cifar-10-python.tar.gz to /content/data/\n",
            "データセット全体の平均値:  tensor([0.4914, 0.4822, 0.4465])\n",
            "データセット全体の標準偏差:  tensor([0.2023, 0.1994, 0.2010])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習用のCIFAR10データセットの前処理を定義\n",
        "train_transform_cifar10 = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=pre_mean_cifar10, std=pre_std_cifar10)\n",
        "    ])\n",
        "# テスト用のCIFAR10データセットの前処理を定義\n",
        "test_transform_cifar10 = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=pre_mean_cifar10, std=pre_std_cifar10)\n",
        "    ])\n",
        "\n",
        "# 学習用のCIFAR10データセットの読み込み\n",
        "train_dataset_cifar10 = torchvision.datasets.CIFAR10(root='/content/data/', train=True, transform=train_transform_cifar10, download=True)\n",
        "# テスト用のCIFAR10データセットの読み込み\n",
        "test_dataset_cifar10 = torchvision.datasets.CIFAR10(root='/content/data/', train=False, transform=test_transform_cifar10, download=True)\n",
        "\n",
        "# 学習用のCIFAR10データローダーを作成\n",
        "train_loader_cifar10 = torch.utils.data.DataLoader(dataset=train_dataset_cifar10, batch_size=512, shuffle=True, num_workers=2)\n",
        "# テスト用のCIFAR10データローダーを作成\n",
        "test_loader_cifar10 = torch.utils.data.DataLoader(dataset=test_dataset_cifar10, batch_size=512, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "ah3PRB7vb1UG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bebce6c6-a8d6-4a3d-8ca4-f7d58e292b3e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### モデルの実装"
      ],
      "metadata": {
        "id": "upaYuBYxytGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1×1の畳み込みを定義\n",
        "def conv1x1(in_channels, out_channels, stride=1):\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "# 3×3の畳み込みを定義\n",
        "def conv3x3(in_channels, out_channels, stride=1):\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)"
      ],
      "metadata": {
        "id": "uvwDV2h3b54A"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Residual Blocksを定義\n",
        "class BuildingBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = conv3x3(out_channels, out_channels)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # 入力と出力のチャンネル数が異なる場合（strideが1より大きい場合）、ダウンサンプリング\n",
        "        if in_channels != out_channels or stride > 1:\n",
        "            self.shortcut = conv1x1(in_channels, out_channels, stride)\n",
        "        else:\n",
        "            self.shortcut = nn.Sequential()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.bn1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv1(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.conv2(out)\n",
        "        # 残差写像と恒等写像の要素毎の和を計算\n",
        "        out += self.shortcut(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "pibhyT9q7T-K"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, block, depth, k, num_classes=10):\n",
        "        super().__init__()\n",
        "        assert (depth - 4) % 6 == 0, \"depth should be 6n + 4\"\n",
        "        n = (depth - 4) // 6\n",
        "        channels = [16, 16 * k, 32 * k, 64 * k]\n",
        "        self.conv1 = conv3x3(3, channels[0])\n",
        "        # Residual Blocks（1)\n",
        "        self.layer1 = self._make_layer(block, channels[0], channels[1], n)\n",
        "        # Residual Blocks（2）\n",
        "        self.layer2 = self._make_layer(block, channels[1], channels[2], n, stride=2)\n",
        "        # Residual Blocks（3）\n",
        "        self.layer3 = self._make_layer(block, channels[2], channels[3], n, stride=2)\n",
        "        self.bn = nn.BatchNorm2d(channels[3])\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(channels[3], num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                # Heの初期化（正規分布）\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                # 重みを1に初期化\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                # バイアスを0に初期化\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    # Residual Blocksを作成する関数を定義\n",
        "    def _make_layer(self, block, in_channels, out_channels, blocks, stride=1):\n",
        "        layers = []\n",
        "        # 最初の Residual Block（stride=stride）\n",
        "        layers.append(block(in_channels, out_channels, stride))\n",
        "        # 残りの Residual Block（stride=1）\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "QY3GI8Vu7cP2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルを呼び出す関数を定義\n",
        "def wideresnet28_10_10():\n",
        "    return WideResNet(BuildingBlock, 28, 10, num_classes=10)"
      ],
      "metadata": {
        "id": "m3mdYxMccGCq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# パラメータ数を計算する関数を定義\n",
        "def num_params(model):\n",
        "    return sum(x.numel() for x in model.parameters() if x.requires_grad)\n",
        "# デバイスを設定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# モデルを定義\n",
        "model = wideresnet28_10_10().to(device)\n",
        "# モデルのパラメータ数を確認\n",
        "print(\"WideResNet18のパラメータ数: {}\".format(num_params(model)))"
      ],
      "metadata": {
        "id": "8nb6_vBy3t68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e546918a-c73a-4777-89eb-94a872a56b81"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WideResNet18のパラメータ数: 36479194\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 学習と評価（10 epochs）"
      ],
      "metadata": {
        "id": "Lpgehd8xWGGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# デバイスを設定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# モデルを定義\n",
        "model = wideresnet28_10_10().to(device)\n",
        "\n",
        "# 学習率を設定\n",
        "learning_rate = 0.001\n",
        "# 全体のepoch数を設定\n",
        "num_epochs = 10\n",
        "# warm-upするepoch数を設定\n",
        "warmup_epochs = 1\n",
        "\n",
        "# 損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# オプティマイザーを設定\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# スケジューラーを設定\n",
        "scheduler = CosineLRScheduler(optimizer, t_initial=num_epochs, lr_min=1e-4, warmup_t=warmup_epochs, warmup_lr_init=1e-4, warmup_prefix=True)\n",
        "\n",
        "# モデルの学習\n",
        "for epoch in range(num_epochs):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, (images, labels) in enumerate(train_loader_cifar10):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # 順伝播\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # 逆伝播\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # 全データ数\n",
        "        total += labels.size(0)\n",
        "        # 正解数\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度を計算\n",
        "    train_acc = 100 * correct / total\n",
        "    # 正解率精度と損失を確認\n",
        "    print(\"Epoch [{}/{}], Train Accuracy: {} %, Loss: {:.4f}\".format(epoch+1, num_epochs, 100 * correct / total, loss.item()))\n",
        "    # 1エポック終了後にスケジューラーを更新\n",
        "    scheduler.step(epoch)\n",
        "    # 学習率の確認\n",
        "    print(\"Epoch [{}/{}], Learning Rate: {}\".format(epoch+1, num_epochs, optimizer.param_groups[0]['lr']))\n",
        "\n",
        "# モデルを保存\n",
        "torch.save(model, '/content/drive/MyDrive/WideResNet28_10_CIFAR10_10epochs_CLRS_restest.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c7e2e1d-dfe7-4525-9bfc-de04380d2b70",
        "id": "AsJ6TobfWGGp"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Train Accuracy: 36.38 %, Loss: 1.4355\n",
            "Epoch [1/10], Learning Rate: 0.0001\n",
            "Epoch [2/10], Train Accuracy: 52.072 %, Loss: 1.3178\n",
            "Epoch [2/10], Learning Rate: 0.001\n",
            "Epoch [3/10], Train Accuracy: 51.2 %, Loss: 1.1182\n",
            "Epoch [3/10], Learning Rate: 0.0009779754323328191\n",
            "Epoch [4/10], Train Accuracy: 64.816 %, Loss: 1.0247\n",
            "Epoch [4/10], Learning Rate: 0.0009140576474687263\n",
            "Epoch [5/10], Train Accuracy: 70.908 %, Loss: 0.6469\n",
            "Epoch [5/10], Learning Rate: 0.000814503363531613\n",
            "Epoch [6/10], Train Accuracy: 74.9 %, Loss: 0.6625\n",
            "Epoch [6/10], Learning Rate: 0.0006890576474687264\n",
            "Epoch [7/10], Train Accuracy: 78.848 %, Loss: 0.5668\n",
            "Epoch [7/10], Learning Rate: 0.00055\n",
            "Epoch [8/10], Train Accuracy: 82.082 %, Loss: 0.4196\n",
            "Epoch [8/10], Learning Rate: 0.0004109423525312737\n",
            "Epoch [9/10], Train Accuracy: 84.416 %, Loss: 0.4200\n",
            "Epoch [9/10], Learning Rate: 0.00028549663646838715\n",
            "Epoch [10/10], Train Accuracy: 86.144 %, Loss: 0.5471\n",
            "Epoch [10/10], Learning Rate: 0.00018594235253127368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの評価\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for images, labels in test_loader_cifar10:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度の確認\n",
        "    print('Test Accuracy: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b01ff0d-48af-4a3b-b4cf-e62b57d3a671",
        "id": "S88KEnVIWGGp"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 84.71 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUメモリの解放\n",
        "del model\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "GS_HO_pOWGGp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 学習と評価（25 epochs）"
      ],
      "metadata": {
        "id": "woMAh18GWAKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# デバイスを設定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# モデルを定義\n",
        "model = wideresnet28_10_10().to(device)\n",
        "\n",
        "# 学習率を設定\n",
        "learning_rate = 0.001\n",
        "# 全体のepoch数を設定\n",
        "num_epochs = 25\n",
        "# warm-upするepoch数を設定\n",
        "warmup_epochs = 3\n",
        "\n",
        "# 損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# オプティマイザーを設定\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# スケジューラーを設定\n",
        "scheduler = CosineLRScheduler(optimizer, t_initial=num_epochs, lr_min=1e-4, warmup_t=warmup_epochs, warmup_lr_init=1e-4, warmup_prefix=True)\n",
        "\n",
        "# モデルの学習\n",
        "for epoch in range(num_epochs):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, (images, labels) in enumerate(train_loader_cifar10):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # 順伝播\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # 逆伝播\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # 全データ数\n",
        "        total += labels.size(0)\n",
        "        # 正解数\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度を計算\n",
        "    train_acc = 100 * correct / total\n",
        "    # 正解率精度と損失を確認\n",
        "    print(\"Epoch [{}/{}], Train Accuracy: {} %, Loss: {:.4f}\".format(epoch+1, num_epochs, 100 * correct / total, loss.item()))\n",
        "    # 1エポック終了後にスケジューラーを更新\n",
        "    scheduler.step(epoch)\n",
        "    # 学習率の確認\n",
        "    print(\"Epoch [{}/{}], Learning Rate: {}\".format(epoch+1, num_epochs, optimizer.param_groups[0]['lr']))\n",
        "\n",
        "# モデルを保存\n",
        "torch.save(model, '/content/drive/MyDrive/WideResNet28_10_CIFAR10_25epochs_CLRS_restest.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dab7f2f-ea97-40d8-a956-48b81a9d8eae",
        "id": "RFBjh0kJWAK0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/25], Train Accuracy: 36.396 %, Loss: 1.4351\n",
            "Epoch [1/25], Learning Rate: 0.0001\n",
            "Epoch [2/25], Train Accuracy: 52.074 %, Loss: 1.3163\n",
            "Epoch [2/25], Learning Rate: 0.00039999999999999996\n",
            "Epoch [3/25], Train Accuracy: 58.966 %, Loss: 1.0005\n",
            "Epoch [3/25], Learning Rate: 0.0007\n",
            "Epoch [4/25], Train Accuracy: 66.314 %, Loss: 0.9652\n",
            "Epoch [4/25], Learning Rate: 0.001\n",
            "Epoch [5/25], Train Accuracy: 72.866 %, Loss: 0.5944\n",
            "Epoch [5/25], Learning Rate: 0.000996451615591515\n",
            "Epoch [6/25], Train Accuracy: 77.722 %, Loss: 0.6033\n",
            "Epoch [6/25], Learning Rate: 0.000985862422507884\n",
            "Epoch [7/25], Train Accuracy: 80.6 %, Loss: 0.5256\n",
            "Epoch [7/25], Learning Rate: 0.0009683994186497131\n",
            "Epoch [8/25], Train Accuracy: 83.226 %, Loss: 0.3945\n",
            "Epoch [8/25], Learning Rate: 0.0009443380060197386\n",
            "Epoch [9/25], Train Accuracy: 84.802 %, Loss: 0.4254\n",
            "Epoch [9/25], Learning Rate: 0.0009140576474687263\n",
            "Epoch [10/25], Train Accuracy: 86.208 %, Loss: 0.5296\n",
            "Epoch [10/25], Learning Rate: 0.0008780358823396353\n",
            "Epoch [11/25], Train Accuracy: 87.402 %, Loss: 0.3914\n",
            "Epoch [11/25], Learning Rate: 0.0008368407953869105\n",
            "Epoch [12/25], Train Accuracy: 88.378 %, Loss: 0.2713\n",
            "Epoch [12/25], Learning Rate: 0.0007911220577405485\n",
            "Epoch [13/25], Train Accuracy: 89.51 %, Loss: 0.2797\n",
            "Epoch [13/25], Learning Rate: 0.0007416006812042827\n",
            "Epoch [14/25], Train Accuracy: 90.434 %, Loss: 0.3052\n",
            "Epoch [14/25], Learning Rate: 0.0006890576474687264\n",
            "Epoch [15/25], Train Accuracy: 91.396 %, Loss: 0.2305\n",
            "Epoch [15/25], Learning Rate: 0.0006343215915635762\n",
            "Epoch [16/25], Train Accuracy: 91.948 %, Loss: 0.2115\n",
            "Epoch [16/25], Learning Rate: 0.0005782557337881911\n",
            "Epoch [17/25], Train Accuracy: 92.782 %, Loss: 0.1926\n",
            "Epoch [17/25], Learning Rate: 0.000521744266211809\n",
            "Epoch [18/25], Train Accuracy: 93.384 %, Loss: 0.1704\n",
            "Epoch [18/25], Learning Rate: 0.0004656784084364239\n",
            "Epoch [19/25], Train Accuracy: 94.322 %, Loss: 0.2419\n",
            "Epoch [19/25], Learning Rate: 0.0004109423525312738\n",
            "Epoch [20/25], Train Accuracy: 94.76 %, Loss: 0.2158\n",
            "Epoch [20/25], Learning Rate: 0.0003583993187957173\n",
            "Epoch [21/25], Train Accuracy: 95.398 %, Loss: 0.1477\n",
            "Epoch [21/25], Learning Rate: 0.0003088779422594514\n",
            "Epoch [22/25], Train Accuracy: 95.992 %, Loss: 0.1012\n",
            "Epoch [22/25], Learning Rate: 0.0002631592046130896\n",
            "Epoch [23/25], Train Accuracy: 96.664 %, Loss: 0.0983\n",
            "Epoch [23/25], Learning Rate: 0.0002219641176603649\n",
            "Epoch [24/25], Train Accuracy: 96.792 %, Loss: 0.0981\n",
            "Epoch [24/25], Learning Rate: 0.00018594235253127368\n",
            "Epoch [25/25], Train Accuracy: 97.368 %, Loss: 0.0846\n",
            "Epoch [25/25], Learning Rate: 0.0001556619939802614\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの評価\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for images, labels in test_loader_cifar10:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度の確認\n",
        "    print('Test Accuracy: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a94ed1e3-a8be-416f-e336-a7e3c268a386",
        "id": "eQXtA_niWAK1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 91.89 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUメモリの解放\n",
        "del model\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "PbNbjdYZWAK1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 学習と評価（50 epochs）"
      ],
      "metadata": {
        "id": "X6RrdlPd5Zky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# デバイスを設定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# モデルを定義\n",
        "model = wideresnet28_10_10().to(device)\n",
        "\n",
        "# 学習率を設定\n",
        "learning_rate = 0.001\n",
        "# 全体のepoch数を設定\n",
        "num_epochs = 50\n",
        "# warm-upするepoch数を設定\n",
        "warmup_epochs = 5\n",
        "\n",
        "# 損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# オプティマイザーを設定\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# スケジューラーを設定\n",
        "scheduler = CosineLRScheduler(optimizer, t_initial=num_epochs, lr_min=1e-4, warmup_t=warmup_epochs, warmup_lr_init=1e-4, warmup_prefix=True)\n",
        "\n",
        "# モデルの学習\n",
        "for epoch in range(num_epochs):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, (images, labels) in enumerate(train_loader_cifar10):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # 順伝播\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # 逆伝播\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # 全データ数\n",
        "        total += labels.size(0)\n",
        "        # 正解数\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度を計算\n",
        "    train_acc = 100 * correct / total\n",
        "    # 正解率精度と損失を確認\n",
        "    print(\"Epoch [{}/{}], Train Accuracy: {} %, Loss: {:.4f}\".format(epoch+1, num_epochs, 100 * correct / total, loss.item()))\n",
        "    # 1エポック終了後にスケジューラーを更新\n",
        "    scheduler.step(epoch)\n",
        "    # 学習率の確認\n",
        "    print(\"Epoch [{}/{}], Learning Rate: {}\".format(epoch+1, num_epochs, optimizer.param_groups[0]['lr']))\n",
        "\n",
        "# モデルを保存\n",
        "torch.save(model, '/content/drive/MyDrive/WideResNet28_10_CIFAR10_50epochs_CLRS_restest.pth')"
      ],
      "metadata": {
        "id": "vELsvQdc5Zk-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98574c51-6d39-4116-e8ce-ceef053ae745"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Train Accuracy: 37.58 %, Loss: 1.3756\n",
            "Epoch [1/50], Learning Rate: 0.0001\n",
            "Epoch [2/50], Train Accuracy: 53.492 %, Loss: 1.1580\n",
            "Epoch [2/50], Learning Rate: 0.00028\n",
            "Epoch [3/50], Train Accuracy: 61.102 %, Loss: 0.9938\n",
            "Epoch [3/50], Learning Rate: 0.00045999999999999996\n",
            "Epoch [4/50], Train Accuracy: 66.728 %, Loss: 0.8585\n",
            "Epoch [4/50], Learning Rate: 0.0006399999999999999\n",
            "Epoch [5/50], Train Accuracy: 71.458 %, Loss: 0.6273\n",
            "Epoch [5/50], Learning Rate: 0.00082\n",
            "Epoch [6/50], Train Accuracy: 75.99 %, Loss: 0.6748\n",
            "Epoch [6/50], Learning Rate: 0.001\n",
            "Epoch [7/50], Train Accuracy: 78.838 %, Loss: 0.5609\n",
            "Epoch [7/50], Learning Rate: 0.0009991120277927223\n",
            "Epoch [8/50], Train Accuracy: 81.422 %, Loss: 0.5429\n",
            "Epoch [8/50], Learning Rate: 0.000996451615591515\n",
            "Epoch [9/50], Train Accuracy: 83.386 %, Loss: 0.5406\n",
            "Epoch [9/50], Learning Rate: 0.00099202926282791\n",
            "Epoch [10/50], Train Accuracy: 84.988 %, Loss: 0.4859\n",
            "Epoch [10/50], Learning Rate: 0.000985862422507884\n",
            "Epoch [11/50], Train Accuracy: 86.652 %, Loss: 0.3899\n",
            "Epoch [11/50], Learning Rate: 0.0009779754323328191\n",
            "Epoch [12/50], Train Accuracy: 87.898 %, Loss: 0.4534\n",
            "Epoch [12/50], Learning Rate: 0.0009683994186497131\n",
            "Epoch [13/50], Train Accuracy: 88.742 %, Loss: 0.3545\n",
            "Epoch [13/50], Learning Rate: 0.0009571721736097088\n",
            "Epoch [14/50], Train Accuracy: 89.358 %, Loss: 0.3461\n",
            "Epoch [14/50], Learning Rate: 0.0009443380060197386\n",
            "Epoch [15/50], Train Accuracy: 89.926 %, Loss: 0.2451\n",
            "Epoch [15/50], Learning Rate: 0.0009299475664759069\n",
            "Epoch [16/50], Train Accuracy: 90.756 %, Loss: 0.2723\n",
            "Epoch [16/50], Learning Rate: 0.0009140576474687263\n",
            "Epoch [17/50], Train Accuracy: 91.496 %, Loss: 0.3820\n",
            "Epoch [17/50], Learning Rate: 0.0008967309592491052\n",
            "Epoch [18/50], Train Accuracy: 92.21 %, Loss: 0.2113\n",
            "Epoch [18/50], Learning Rate: 0.0008780358823396353\n",
            "Epoch [19/50], Train Accuracy: 92.532 %, Loss: 0.2326\n",
            "Epoch [19/50], Learning Rate: 0.0008580461976679099\n",
            "Epoch [20/50], Train Accuracy: 93.07 %, Loss: 0.2948\n",
            "Epoch [20/50], Learning Rate: 0.0008368407953869105\n",
            "Epoch [21/50], Train Accuracy: 93.434 %, Loss: 0.1988\n",
            "Epoch [21/50], Learning Rate: 0.0008145033635316131\n",
            "Epoch [22/50], Train Accuracy: 94.146 %, Loss: 0.1353\n",
            "Epoch [22/50], Learning Rate: 0.0007911220577405485\n",
            "Epoch [23/50], Train Accuracy: 94.672 %, Loss: 0.1348\n",
            "Epoch [23/50], Learning Rate: 0.0007667891533457719\n",
            "Epoch [24/50], Train Accuracy: 94.77 %, Loss: 0.1314\n",
            "Epoch [24/50], Learning Rate: 0.0007416006812042827\n",
            "Epoch [25/50], Train Accuracy: 95.242 %, Loss: 0.1262\n",
            "Epoch [25/50], Learning Rate: 0.0007156560487081052\n",
            "Epoch [26/50], Train Accuracy: 95.714 %, Loss: 0.1098\n",
            "Epoch [26/50], Learning Rate: 0.0006890576474687264\n",
            "Epoch [27/50], Train Accuracy: 95.862 %, Loss: 0.1116\n",
            "Epoch [27/50], Learning Rate: 0.0006619104492241846\n",
            "Epoch [28/50], Train Accuracy: 96.434 %, Loss: 0.1027\n",
            "Epoch [28/50], Learning Rate: 0.0006343215915635762\n",
            "Epoch [29/50], Train Accuracy: 96.358 %, Loss: 0.0835\n",
            "Epoch [29/50], Learning Rate: 0.000606399955103937\n",
            "Epoch [30/50], Train Accuracy: 96.916 %, Loss: 0.1011\n",
            "Epoch [30/50], Learning Rate: 0.0005782557337881911\n",
            "Epoch [31/50], Train Accuracy: 96.968 %, Loss: 0.0813\n",
            "Epoch [31/50], Learning Rate: 0.00055\n",
            "Epoch [32/50], Train Accuracy: 97.41 %, Loss: 0.0624\n",
            "Epoch [32/50], Learning Rate: 0.000521744266211809\n",
            "Epoch [33/50], Train Accuracy: 97.598 %, Loss: 0.0703\n",
            "Epoch [33/50], Learning Rate: 0.0004936000448960632\n",
            "Epoch [34/50], Train Accuracy: 97.868 %, Loss: 0.1010\n",
            "Epoch [34/50], Learning Rate: 0.0004656784084364239\n",
            "Epoch [35/50], Train Accuracy: 98.156 %, Loss: 0.1055\n",
            "Epoch [35/50], Learning Rate: 0.0004380895507758153\n",
            "Epoch [36/50], Train Accuracy: 98.098 %, Loss: 0.0729\n",
            "Epoch [36/50], Learning Rate: 0.0004109423525312738\n",
            "Epoch [37/50], Train Accuracy: 98.476 %, Loss: 0.0395\n",
            "Epoch [37/50], Learning Rate: 0.000384343951291895\n",
            "Epoch [38/50], Train Accuracy: 98.782 %, Loss: 0.0330\n",
            "Epoch [38/50], Learning Rate: 0.0003583993187957173\n",
            "Epoch [39/50], Train Accuracy: 98.73 %, Loss: 0.0519\n",
            "Epoch [39/50], Learning Rate: 0.0003332108466542281\n",
            "Epoch [40/50], Train Accuracy: 98.94 %, Loss: 0.0336\n",
            "Epoch [40/50], Learning Rate: 0.0003088779422594514\n",
            "Epoch [41/50], Train Accuracy: 99.104 %, Loss: 0.0158\n",
            "Epoch [41/50], Learning Rate: 0.00028549663646838715\n",
            "Epoch [42/50], Train Accuracy: 99.208 %, Loss: 0.0268\n",
            "Epoch [42/50], Learning Rate: 0.0002631592046130896\n",
            "Epoch [43/50], Train Accuracy: 99.28 %, Loss: 0.0438\n",
            "Epoch [43/50], Learning Rate: 0.00024195380233209008\n",
            "Epoch [44/50], Train Accuracy: 99.364 %, Loss: 0.0257\n",
            "Epoch [44/50], Learning Rate: 0.0002219641176603649\n",
            "Epoch [45/50], Train Accuracy: 99.468 %, Loss: 0.0201\n",
            "Epoch [45/50], Learning Rate: 0.00020326904075089488\n",
            "Epoch [46/50], Train Accuracy: 99.558 %, Loss: 0.0070\n",
            "Epoch [46/50], Learning Rate: 0.00018594235253127368\n",
            "Epoch [47/50], Train Accuracy: 99.59 %, Loss: 0.0030\n",
            "Epoch [47/50], Learning Rate: 0.00017005243352409332\n",
            "Epoch [48/50], Train Accuracy: 99.632 %, Loss: 0.0103\n",
            "Epoch [48/50], Learning Rate: 0.0001556619939802614\n",
            "Epoch [49/50], Train Accuracy: 99.632 %, Loss: 0.0094\n",
            "Epoch [49/50], Learning Rate: 0.0001428278263902913\n",
            "Epoch [50/50], Train Accuracy: 99.696 %, Loss: 0.0048\n",
            "Epoch [50/50], Learning Rate: 0.0001316005813502869\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの評価\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for images, labels in test_loader_cifar10:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度の確認\n",
        "    print('Test Accuracy: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "id": "RtGaIvN55Zk_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00f206f6-541b-4a75-e9ec-61dec81c84c1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 93.44 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUメモリの解放\n",
        "del model\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "kE02RIkS5Zk_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 学習と評価（75 epochs）"
      ],
      "metadata": {
        "id": "t-k7-j4nFQXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# デバイスを設定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# モデルを定義\n",
        "model = wideresnet28_10_10().to(device)\n",
        "\n",
        "# 学習率を設定\n",
        "learning_rate = 0.001\n",
        "# 全体のepoch数を設定\n",
        "num_epochs = 75\n",
        "# warm-upするepoch数を設定\n",
        "warmup_epochs = 8\n",
        "\n",
        "# 損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# オプティマイザーを設定\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# スケジューラーを設定\n",
        "scheduler = CosineLRScheduler(optimizer, t_initial=num_epochs, lr_min=1e-4, warmup_t=warmup_epochs, warmup_lr_init=1e-4, warmup_prefix=True)\n",
        "\n",
        "# モデルの学習\n",
        "for epoch in range(num_epochs):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, (images, labels) in enumerate(train_loader_cifar10):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # 順伝播\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # 逆伝播\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # 全データ数\n",
        "        total += labels.size(0)\n",
        "        # 正解数\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度を計算\n",
        "    train_acc = 100 * correct / total\n",
        "    # 正解率精度と損失を確認\n",
        "    print(\"Epoch [{}/{}], Train Accuracy: {} %, Loss: {:.4f}\".format(epoch+1, num_epochs, 100 * correct / total, loss.item()))\n",
        "    # 1エポック終了後にスケジューラーを更新\n",
        "    scheduler.step(epoch)\n",
        "    # 学習率の確認\n",
        "    print(\"Epoch [{}/{}], Learning Rate: {}\".format(epoch+1, num_epochs, optimizer.param_groups[0]['lr']))\n",
        "\n",
        "# モデルを保存\n",
        "torch.save(model, '/content/drive/MyDrive/WideResNet28_10_CIFAR10_75epochs_CLRS_restest.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYIxZQ5GFWuW",
        "outputId": "797b2b1b-af7d-45e1-e76e-7f9c169a3da3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/75], Train Accuracy: 36.652 %, Loss: 1.4163\n",
            "Epoch [1/75], Learning Rate: 0.0001\n",
            "Epoch [2/75], Train Accuracy: 51.378 %, Loss: 1.2332\n",
            "Epoch [2/75], Learning Rate: 0.00021250000000000002\n",
            "Epoch [3/75], Train Accuracy: 59.126 %, Loss: 0.9561\n",
            "Epoch [3/75], Learning Rate: 0.000325\n",
            "Epoch [4/75], Train Accuracy: 65.63 %, Loss: 0.7631\n",
            "Epoch [4/75], Learning Rate: 0.0004375\n",
            "Epoch [5/75], Train Accuracy: 70.884 %, Loss: 0.7252\n",
            "Epoch [5/75], Learning Rate: 0.00055\n",
            "Epoch [6/75], Train Accuracy: 75.128 %, Loss: 0.7504\n",
            "Epoch [6/75], Learning Rate: 0.0006625\n",
            "Epoch [7/75], Train Accuracy: 78.216 %, Loss: 0.6807\n",
            "Epoch [7/75], Learning Rate: 0.0007750000000000001\n",
            "Epoch [8/75], Train Accuracy: 80.732 %, Loss: 0.5211\n",
            "Epoch [8/75], Learning Rate: 0.0008875\n",
            "Epoch [9/75], Train Accuracy: 82.238 %, Loss: 0.4500\n",
            "Epoch [9/75], Learning Rate: 0.001\n",
            "Epoch [10/75], Train Accuracy: 83.852 %, Loss: 0.4488\n",
            "Epoch [10/75], Learning Rate: 0.0009996052735444863\n",
            "Epoch [11/75], Train Accuracy: 85.376 %, Loss: 0.4323\n",
            "Epoch [11/75], Learning Rate: 0.000998421786662277\n",
            "Epoch [12/75], Train Accuracy: 87.086 %, Loss: 0.3477\n",
            "Epoch [12/75], Learning Rate: 0.000996451615591515\n",
            "Epoch [13/75], Train Accuracy: 87.744 %, Loss: 0.3379\n",
            "Epoch [13/75], Learning Rate: 0.0009936982166817271\n",
            "Epoch [14/75], Train Accuracy: 88.706 %, Loss: 0.2492\n",
            "Epoch [14/75], Learning Rate: 0.0009901664203302125\n",
            "Epoch [15/75], Train Accuracy: 89.584 %, Loss: 0.3147\n",
            "Epoch [15/75], Learning Rate: 0.000985862422507884\n",
            "Epoch [16/75], Train Accuracy: 90.418 %, Loss: 0.2533\n",
            "Epoch [16/75], Learning Rate: 0.0009807937738894302\n",
            "Epoch [17/75], Train Accuracy: 91.07 %, Loss: 0.2890\n",
            "Epoch [17/75], Learning Rate: 0.0009749693666068666\n",
            "Epoch [18/75], Train Accuracy: 91.384 %, Loss: 0.2666\n",
            "Epoch [18/75], Learning Rate: 0.0009683994186497131\n",
            "Epoch [19/75], Train Accuracy: 92.05 %, Loss: 0.2680\n",
            "Epoch [19/75], Learning Rate: 0.0009610954559391703\n",
            "Epoch [20/75], Train Accuracy: 92.342 %, Loss: 0.2291\n",
            "Epoch [20/75], Learning Rate: 0.0009530702921077359\n",
            "Epoch [21/75], Train Accuracy: 93.078 %, Loss: 0.2414\n",
            "Epoch [21/75], Learning Rate: 0.0009443380060197386\n",
            "Epoch [22/75], Train Accuracy: 93.414 %, Loss: 0.1645\n",
            "Epoch [22/75], Learning Rate: 0.000934913917072228\n",
            "Epoch [23/75], Train Accuracy: 93.758 %, Loss: 0.1775\n",
            "Epoch [23/75], Learning Rate: 0.0009248145583195447\n",
            "Epoch [24/75], Train Accuracy: 94.262 %, Loss: 0.1762\n",
            "Epoch [24/75], Learning Rate: 0.0009140576474687263\n",
            "Epoch [25/75], Train Accuracy: 94.546 %, Loss: 0.2549\n",
            "Epoch [25/75], Learning Rate: 0.000902662055796628\n",
            "Epoch [26/75], Train Accuracy: 94.98 %, Loss: 0.1402\n",
            "Epoch [26/75], Learning Rate: 0.0008906477750432905\n",
            "Epoch [27/75], Train Accuracy: 95.13 %, Loss: 0.1373\n",
            "Epoch [27/75], Learning Rate: 0.0008780358823396353\n",
            "Epoch [28/75], Train Accuracy: 95.486 %, Loss: 0.1312\n",
            "Epoch [28/75], Learning Rate: 0.0008648485032310146\n",
            "Epoch [29/75], Train Accuracy: 95.674 %, Loss: 0.1556\n",
            "Epoch [29/75], Learning Rate: 0.0008511087728614862\n",
            "Epoch [30/75], Train Accuracy: 96.248 %, Loss: 0.1646\n",
            "Epoch [30/75], Learning Rate: 0.0008368407953869105\n",
            "Epoch [31/75], Train Accuracy: 96.104 %, Loss: 0.0891\n",
            "Epoch [31/75], Learning Rate: 0.0008220696016880688\n",
            "Epoch [32/75], Train Accuracy: 96.594 %, Loss: 0.1079\n",
            "Epoch [32/75], Learning Rate: 0.0008068211054579944\n",
            "Epoch [33/75], Train Accuracy: 96.702 %, Loss: 0.0797\n",
            "Epoch [33/75], Learning Rate: 0.0007911220577405485\n",
            "Epoch [34/75], Train Accuracy: 97.172 %, Loss: 0.0776\n",
            "Epoch [34/75], Learning Rate: 0.0007750000000000001\n",
            "Epoch [35/75], Train Accuracy: 97.262 %, Loss: 0.0714\n",
            "Epoch [35/75], Learning Rate: 0.0007584832158039379\n",
            "Epoch [36/75], Train Accuracy: 97.238 %, Loss: 0.0894\n",
            "Epoch [36/75], Learning Rate: 0.0007416006812042827\n",
            "Epoch [37/75], Train Accuracy: 97.524 %, Loss: 0.0493\n",
            "Epoch [37/75], Learning Rate: 0.0007243820139034463\n",
            "Epoch [38/75], Train Accuracy: 97.426 %, Loss: 0.0890\n",
            "Epoch [38/75], Learning Rate: 0.0007068574212948169\n",
            "Epoch [39/75], Train Accuracy: 97.68 %, Loss: 0.0479\n",
            "Epoch [39/75], Learning Rate: 0.0006890576474687264\n",
            "Epoch [40/75], Train Accuracy: 98.086 %, Loss: 0.0652\n",
            "Epoch [40/75], Learning Rate: 0.0006710139192768696\n",
            "Epoch [41/75], Train Accuracy: 98.046 %, Loss: 0.0483\n",
            "Epoch [41/75], Learning Rate: 0.0006527578915497952\n",
            "Epoch [42/75], Train Accuracy: 98.388 %, Loss: 0.0307\n",
            "Epoch [42/75], Learning Rate: 0.0006343215915635762\n",
            "Epoch [43/75], Train Accuracy: 98.318 %, Loss: 0.0607\n",
            "Epoch [43/75], Learning Rate: 0.0006157373628530854\n",
            "Epoch [44/75], Train Accuracy: 98.42 %, Loss: 0.0321\n",
            "Epoch [44/75], Learning Rate: 0.0005970378084704443\n",
            "Epoch [45/75], Train Accuracy: 98.608 %, Loss: 0.0290\n",
            "Epoch [45/75], Learning Rate: 0.0005782557337881911\n",
            "Epoch [46/75], Train Accuracy: 98.526 %, Loss: 0.0321\n",
            "Epoch [46/75], Learning Rate: 0.0005594240889475107\n",
            "Epoch [47/75], Train Accuracy: 98.652 %, Loss: 0.0445\n",
            "Epoch [47/75], Learning Rate: 0.0005405759110524895\n",
            "Epoch [48/75], Train Accuracy: 98.938 %, Loss: 0.0230\n",
            "Epoch [48/75], Learning Rate: 0.0005217442662118091\n",
            "Epoch [49/75], Train Accuracy: 98.964 %, Loss: 0.0559\n",
            "Epoch [49/75], Learning Rate: 0.000502962191529556\n",
            "Epoch [50/75], Train Accuracy: 98.96 %, Loss: 0.0490\n",
            "Epoch [50/75], Learning Rate: 0.0004842626371469149\n",
            "Epoch [51/75], Train Accuracy: 99.076 %, Loss: 0.0150\n",
            "Epoch [51/75], Learning Rate: 0.0004656784084364239\n",
            "Epoch [52/75], Train Accuracy: 99.132 %, Loss: 0.0152\n",
            "Epoch [52/75], Learning Rate: 0.00044724210845020496\n",
            "Epoch [53/75], Train Accuracy: 99.186 %, Loss: 0.0077\n",
            "Epoch [53/75], Learning Rate: 0.00042898608072313056\n",
            "Epoch [54/75], Train Accuracy: 99.322 %, Loss: 0.0117\n",
            "Epoch [54/75], Learning Rate: 0.0004109423525312737\n",
            "Epoch [55/75], Train Accuracy: 99.386 %, Loss: 0.0394\n",
            "Epoch [55/75], Learning Rate: 0.00039314257870518325\n",
            "Epoch [56/75], Train Accuracy: 99.394 %, Loss: 0.0149\n",
            "Epoch [56/75], Learning Rate: 0.00037561798609655366\n",
            "Epoch [57/75], Train Accuracy: 99.488 %, Loss: 0.0134\n",
            "Epoch [57/75], Learning Rate: 0.0003583993187957173\n",
            "Epoch [58/75], Train Accuracy: 99.588 %, Loss: 0.0085\n",
            "Epoch [58/75], Learning Rate: 0.0003415167841960623\n",
            "Epoch [59/75], Train Accuracy: 99.58 %, Loss: 0.0209\n",
            "Epoch [59/75], Learning Rate: 0.0003249999999999999\n",
            "Epoch [60/75], Train Accuracy: 99.596 %, Loss: 0.0220\n",
            "Epoch [60/75], Learning Rate: 0.0003088779422594516\n",
            "Epoch [61/75], Train Accuracy: 99.63 %, Loss: 0.0141\n",
            "Epoch [61/75], Learning Rate: 0.0002931788945420058\n",
            "Epoch [62/75], Train Accuracy: 99.64 %, Loss: 0.0037\n",
            "Epoch [62/75], Learning Rate: 0.0002779303983119313\n",
            "Epoch [63/75], Train Accuracy: 99.652 %, Loss: 0.0191\n",
            "Epoch [63/75], Learning Rate: 0.0002631592046130896\n",
            "Epoch [64/75], Train Accuracy: 99.744 %, Loss: 0.0034\n",
            "Epoch [64/75], Learning Rate: 0.0002488912271385138\n",
            "Epoch [65/75], Train Accuracy: 99.726 %, Loss: 0.0298\n",
            "Epoch [65/75], Learning Rate: 0.0002351514967689855\n",
            "Epoch [66/75], Train Accuracy: 99.768 %, Loss: 0.0302\n",
            "Epoch [66/75], Learning Rate: 0.0002219641176603649\n",
            "Epoch [67/75], Train Accuracy: 99.768 %, Loss: 0.0019\n",
            "Epoch [67/75], Learning Rate: 0.00020935222495670967\n",
            "Epoch [68/75], Train Accuracy: 99.85 %, Loss: 0.0038\n",
            "Epoch [68/75], Learning Rate: 0.00019733794420337213\n",
            "Epoch [69/75], Train Accuracy: 99.866 %, Loss: 0.0026\n",
            "Epoch [69/75], Learning Rate: 0.00018594235253127368\n",
            "Epoch [70/75], Train Accuracy: 99.818 %, Loss: 0.0048\n",
            "Epoch [70/75], Learning Rate: 0.00017518544168045524\n",
            "Epoch [71/75], Train Accuracy: 99.902 %, Loss: 0.0018\n",
            "Epoch [71/75], Learning Rate: 0.00016508608292777201\n",
            "Epoch [72/75], Train Accuracy: 99.898 %, Loss: 0.0081\n",
            "Epoch [72/75], Learning Rate: 0.0001556619939802614\n",
            "Epoch [73/75], Train Accuracy: 99.888 %, Loss: 0.0092\n",
            "Epoch [73/75], Learning Rate: 0.0001469297078922643\n",
            "Epoch [74/75], Train Accuracy: 99.924 %, Loss: 0.0023\n",
            "Epoch [74/75], Learning Rate: 0.00013890454406082967\n",
            "Epoch [75/75], Train Accuracy: 99.902 %, Loss: 0.0015\n",
            "Epoch [75/75], Learning Rate: 0.0001316005813502869\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの評価\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for images, labels in test_loader_cifar10:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度の確認\n",
        "    print('Test Accuracy: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uElJSN1cFQXc",
        "outputId": "75cd5e9f-0a32-471b-9cff-08b578dccfe5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 94.43 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUメモリの解放\n",
        "del model\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "mdCUTS3CFQXc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 学習と評価（100 epochs）"
      ],
      "metadata": {
        "id": "pJyew3dez2Wc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# デバイスを設定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# モデルを定義\n",
        "model = wideresnet28_10_10().to(device)\n",
        "\n",
        "# 学習率を設定\n",
        "learning_rate = 0.001\n",
        "# 全体のepoch数を設定\n",
        "num_epochs = 100\n",
        "# warm-upするepoch数を設定\n",
        "warmup_epochs = 10\n",
        "\n",
        "# 損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# オプティマイザーを設定\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# スケジューラーを設定\n",
        "scheduler = CosineLRScheduler(optimizer, t_initial=num_epochs, lr_min=1e-4, warmup_t=warmup_epochs, warmup_lr_init=1e-4, warmup_prefix=True)\n",
        "\n",
        "# モデルの学習\n",
        "for epoch in range(num_epochs):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, (images, labels) in enumerate(train_loader_cifar10):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # 順伝播\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # 逆伝播\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # 全データ数\n",
        "        total += labels.size(0)\n",
        "        # 正解数\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度を計算\n",
        "    train_acc = 100 * correct / total\n",
        "    # 正解率精度と損失を確認\n",
        "    print(\"Epoch [{}/{}], Train Accuracy: {} %, Loss: {:.4f}\".format(epoch+1, num_epochs, 100 * correct / total, loss.item()))\n",
        "    # 1エポック終了後にスケジューラーを更新\n",
        "    scheduler.step(epoch)\n",
        "    # 学習率の確認\n",
        "    print(\"Epoch [{}/{}], Learning Rate: {}\".format(epoch+1, num_epochs, optimizer.param_groups[0]['lr']))\n",
        "\n",
        "# モデルを保存\n",
        "torch.save(model, '/content/drive/MyDrive/WideResNet28_10_CIFAR10_100epochs_CLRS_restest.pth')"
      ],
      "metadata": {
        "id": "lkPQYc-Uz2Wc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b50f209-1eff-49bd-8b2b-71becdc89c61"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Accuracy: 36.39 %, Loss: 1.4355\n",
            "Epoch [1/100], Learning Rate: 0.0001\n",
            "Epoch [2/100], Train Accuracy: 52.034 %, Loss: 1.3162\n",
            "Epoch [2/100], Learning Rate: 0.00019\n",
            "Epoch [3/100], Train Accuracy: 59.44 %, Loss: 1.0351\n",
            "Epoch [3/100], Learning Rate: 0.00028\n",
            "Epoch [4/100], Train Accuracy: 65.374 %, Loss: 1.0026\n",
            "Epoch [4/100], Learning Rate: 0.00036999999999999994\n",
            "Epoch [5/100], Train Accuracy: 70.398 %, Loss: 0.7226\n",
            "Epoch [5/100], Learning Rate: 0.00045999999999999996\n",
            "Epoch [6/100], Train Accuracy: 74.708 %, Loss: 0.6900\n",
            "Epoch [6/100], Learning Rate: 0.00055\n",
            "Epoch [7/100], Train Accuracy: 77.746 %, Loss: 0.5677\n",
            "Epoch [7/100], Learning Rate: 0.0006399999999999999\n",
            "Epoch [8/100], Train Accuracy: 80.354 %, Loss: 0.4811\n",
            "Epoch [8/100], Learning Rate: 0.00073\n",
            "Epoch [9/100], Train Accuracy: 81.844 %, Loss: 0.5298\n",
            "Epoch [9/100], Learning Rate: 0.00082\n",
            "Epoch [10/100], Train Accuracy: 83.652 %, Loss: 0.5740\n",
            "Epoch [10/100], Learning Rate: 0.00091\n",
            "Epoch [11/100], Train Accuracy: 84.46 %, Loss: 0.4216\n",
            "Epoch [11/100], Learning Rate: 0.001\n",
            "Epoch [12/100], Train Accuracy: 85.824 %, Loss: 0.2907\n",
            "Epoch [12/100], Learning Rate: 0.0009997779521645793\n",
            "Epoch [13/100], Train Accuracy: 87.006 %, Loss: 0.3748\n",
            "Epoch [13/100], Learning Rate: 0.0009991120277927223\n",
            "Epoch [14/100], Train Accuracy: 88.224 %, Loss: 0.3632\n",
            "Epoch [14/100], Learning Rate: 0.000998002884071386\n",
            "Epoch [15/100], Train Accuracy: 88.818 %, Loss: 0.2499\n",
            "Epoch [15/100], Learning Rate: 0.000996451615591515\n",
            "Epoch [16/100], Train Accuracy: 89.856 %, Loss: 0.2539\n",
            "Epoch [16/100], Learning Rate: 0.000994459753267812\n",
            "Epoch [17/100], Train Accuracy: 90.534 %, Loss: 0.2816\n",
            "Epoch [17/100], Learning Rate: 0.00099202926282791\n",
            "Epoch [18/100], Train Accuracy: 90.984 %, Loss: 0.2585\n",
            "Epoch [18/100], Learning Rate: 0.0009891625428724363\n",
            "Epoch [19/100], Train Accuracy: 91.896 %, Loss: 0.3098\n",
            "Epoch [19/100], Learning Rate: 0.000985862422507884\n",
            "Epoch [20/100], Train Accuracy: 92.338 %, Loss: 0.3180\n",
            "Epoch [20/100], Learning Rate: 0.0009821321585546244\n",
            "Epoch [21/100], Train Accuracy: 92.642 %, Loss: 0.2178\n",
            "Epoch [21/100], Learning Rate: 0.0009779754323328191\n",
            "Epoch [22/100], Train Accuracy: 93.228 %, Loss: 0.2054\n",
            "Epoch [22/100], Learning Rate: 0.0009733963460294015\n",
            "Epoch [23/100], Train Accuracy: 93.72 %, Loss: 0.2251\n",
            "Epoch [23/100], Learning Rate: 0.0009683994186497131\n",
            "Epoch [24/100], Train Accuracy: 93.812 %, Loss: 0.2122\n",
            "Epoch [24/100], Learning Rate: 0.0009629895815577914\n",
            "Epoch [25/100], Train Accuracy: 94.162 %, Loss: 0.1650\n",
            "Epoch [25/100], Learning Rate: 0.0009571721736097088\n",
            "Epoch [26/100], Train Accuracy: 94.63 %, Loss: 0.1942\n",
            "Epoch [26/100], Learning Rate: 0.0009509529358847655\n",
            "Epoch [27/100], Train Accuracy: 95.1 %, Loss: 0.1786\n",
            "Epoch [27/100], Learning Rate: 0.0009443380060197386\n",
            "Epoch [28/100], Train Accuracy: 95.086 %, Loss: 0.0976\n",
            "Epoch [28/100], Learning Rate: 0.0009373339121517746\n",
            "Epoch [29/100], Train Accuracy: 95.468 %, Loss: 0.0961\n",
            "Epoch [29/100], Learning Rate: 0.0009299475664759069\n",
            "Epoch [30/100], Train Accuracy: 95.414 %, Loss: 0.1497\n",
            "Epoch [30/100], Learning Rate: 0.0009221862584235528\n",
            "Epoch [31/100], Train Accuracy: 96.086 %, Loss: 0.0766\n",
            "Epoch [31/100], Learning Rate: 0.0009140576474687263\n",
            "Epoch [32/100], Train Accuracy: 96.244 %, Loss: 0.0730\n",
            "Epoch [32/100], Learning Rate: 0.0009055697555690606\n",
            "Epoch [33/100], Train Accuracy: 96.41 %, Loss: 0.1101\n",
            "Epoch [33/100], Learning Rate: 0.0008967309592491052\n",
            "Epoch [34/100], Train Accuracy: 96.684 %, Loss: 0.0886\n",
            "Epoch [34/100], Learning Rate: 0.0008875499813337069\n",
            "Epoch [35/100], Train Accuracy: 97.01 %, Loss: 0.0713\n",
            "Epoch [35/100], Learning Rate: 0.0008780358823396353\n",
            "Epoch [36/100], Train Accuracy: 97.006 %, Loss: 0.1085\n",
            "Epoch [36/100], Learning Rate: 0.0008681980515339464\n",
            "Epoch [37/100], Train Accuracy: 97.11 %, Loss: 0.0684\n",
            "Epoch [37/100], Learning Rate: 0.0008580461976679099\n",
            "Epoch [38/100], Train Accuracy: 97.198 %, Loss: 0.0738\n",
            "Epoch [38/100], Learning Rate: 0.0008475903393956434\n",
            "Epoch [39/100], Train Accuracy: 97.276 %, Loss: 0.1483\n",
            "Epoch [39/100], Learning Rate: 0.0008368407953869105\n",
            "Epoch [40/100], Train Accuracy: 97.798 %, Loss: 0.0965\n",
            "Epoch [40/100], Learning Rate: 0.0008258081741438395\n",
            "Epoch [41/100], Train Accuracy: 97.844 %, Loss: 0.1012\n",
            "Epoch [41/100], Learning Rate: 0.0008145033635316131\n",
            "Epoch [42/100], Train Accuracy: 97.842 %, Loss: 0.0893\n",
            "Epoch [42/100], Learning Rate: 0.0008029375200334589\n",
            "Epoch [43/100], Train Accuracy: 97.834 %, Loss: 0.0595\n",
            "Epoch [43/100], Learning Rate: 0.0007911220577405485\n",
            "Epoch [44/100], Train Accuracy: 97.998 %, Loss: 0.0636\n",
            "Epoch [44/100], Learning Rate: 0.0007790686370876671\n",
            "Epoch [45/100], Train Accuracy: 97.992 %, Loss: 0.0888\n",
            "Epoch [45/100], Learning Rate: 0.0007667891533457719\n",
            "Epoch [46/100], Train Accuracy: 98.452 %, Loss: 0.0421\n",
            "Epoch [46/100], Learning Rate: 0.000754295724882796\n",
            "Epoch [47/100], Train Accuracy: 98.462 %, Loss: 0.0482\n",
            "Epoch [47/100], Learning Rate: 0.0007416006812042827\n",
            "Epoch [48/100], Train Accuracy: 98.402 %, Loss: 0.0429\n",
            "Epoch [48/100], Learning Rate: 0.0007287165507856513\n",
            "Epoch [49/100], Train Accuracy: 98.338 %, Loss: 0.0170\n",
            "Epoch [49/100], Learning Rate: 0.0007156560487081052\n",
            "Epoch [50/100], Train Accuracy: 98.402 %, Loss: 0.0340\n",
            "Epoch [50/100], Learning Rate: 0.0007024320641103813\n",
            "Epoch [51/100], Train Accuracy: 98.676 %, Loss: 0.0378\n",
            "Epoch [51/100], Learning Rate: 0.0006890576474687264\n",
            "Epoch [52/100], Train Accuracy: 98.56 %, Loss: 0.0665\n",
            "Epoch [52/100], Learning Rate: 0.0006755459977176532\n",
            "Epoch [53/100], Train Accuracy: 98.714 %, Loss: 0.0361\n",
            "Epoch [53/100], Learning Rate: 0.0006619104492241846\n",
            "Epoch [54/100], Train Accuracy: 98.944 %, Loss: 0.0530\n",
            "Epoch [54/100], Learning Rate: 0.0006481644586284443\n",
            "Epoch [55/100], Train Accuracy: 98.99 %, Loss: 0.0275\n",
            "Epoch [55/100], Learning Rate: 0.0006343215915635762\n",
            "Epoch [56/100], Train Accuracy: 98.912 %, Loss: 0.0434\n",
            "Epoch [56/100], Learning Rate: 0.000620395509268104\n",
            "Epoch [57/100], Train Accuracy: 99.008 %, Loss: 0.0140\n",
            "Epoch [57/100], Learning Rate: 0.000606399955103937\n",
            "Epoch [58/100], Train Accuracy: 99.124 %, Loss: 0.0220\n",
            "Epoch [58/100], Learning Rate: 0.0005923487409933315\n",
            "Epoch [59/100], Train Accuracy: 99.138 %, Loss: 0.0157\n",
            "Epoch [59/100], Learning Rate: 0.0005782557337881911\n",
            "Epoch [60/100], Train Accuracy: 99.106 %, Loss: 0.0337\n",
            "Epoch [60/100], Learning Rate: 0.0005641348415851578\n",
            "Epoch [61/100], Train Accuracy: 99.278 %, Loss: 0.0144\n",
            "Epoch [61/100], Learning Rate: 0.00055\n",
            "Epoch [62/100], Train Accuracy: 99.324 %, Loss: 0.0252\n",
            "Epoch [62/100], Learning Rate: 0.0005358651584148423\n",
            "Epoch [63/100], Train Accuracy: 99.32 %, Loss: 0.0287\n",
            "Epoch [63/100], Learning Rate: 0.000521744266211809\n",
            "Epoch [64/100], Train Accuracy: 99.16 %, Loss: 0.0369\n",
            "Epoch [64/100], Learning Rate: 0.0005076512590066686\n",
            "Epoch [65/100], Train Accuracy: 99.362 %, Loss: 0.0455\n",
            "Epoch [65/100], Learning Rate: 0.0004936000448960632\n",
            "Epoch [66/100], Train Accuracy: 99.43 %, Loss: 0.0087\n",
            "Epoch [66/100], Learning Rate: 0.0004796044907318961\n",
            "Epoch [67/100], Train Accuracy: 99.48 %, Loss: 0.0188\n",
            "Epoch [67/100], Learning Rate: 0.0004656784084364239\n",
            "Epoch [68/100], Train Accuracy: 99.5 %, Loss: 0.0123\n",
            "Epoch [68/100], Learning Rate: 0.00045183554137155597\n",
            "Epoch [69/100], Train Accuracy: 99.458 %, Loss: 0.0059\n",
            "Epoch [69/100], Learning Rate: 0.0004380895507758153\n",
            "Epoch [70/100], Train Accuracy: 99.612 %, Loss: 0.0099\n",
            "Epoch [70/100], Learning Rate: 0.00042445400228234684\n",
            "Epoch [71/100], Train Accuracy: 99.638 %, Loss: 0.0402\n",
            "Epoch [71/100], Learning Rate: 0.0004109423525312738\n",
            "Epoch [72/100], Train Accuracy: 99.526 %, Loss: 0.0115\n",
            "Epoch [72/100], Learning Rate: 0.0003975679358896189\n",
            "Epoch [73/100], Train Accuracy: 99.642 %, Loss: 0.0035\n",
            "Epoch [73/100], Learning Rate: 0.000384343951291895\n",
            "Epoch [74/100], Train Accuracy: 99.644 %, Loss: 0.0136\n",
            "Epoch [74/100], Learning Rate: 0.00037128344921434864\n",
            "Epoch [75/100], Train Accuracy: 99.71 %, Loss: 0.0083\n",
            "Epoch [75/100], Learning Rate: 0.0003583993187957173\n",
            "Epoch [76/100], Train Accuracy: 99.706 %, Loss: 0.0120\n",
            "Epoch [76/100], Learning Rate: 0.00034570427511720393\n",
            "Epoch [77/100], Train Accuracy: 99.732 %, Loss: 0.0033\n",
            "Epoch [77/100], Learning Rate: 0.0003332108466542281\n",
            "Epoch [78/100], Train Accuracy: 99.786 %, Loss: 0.0055\n",
            "Epoch [78/100], Learning Rate: 0.0003209313629123329\n",
            "Epoch [79/100], Train Accuracy: 99.746 %, Loss: 0.0027\n",
            "Epoch [79/100], Learning Rate: 0.0003088779422594514\n",
            "Epoch [80/100], Train Accuracy: 99.81 %, Loss: 0.0137\n",
            "Epoch [80/100], Learning Rate: 0.0002970624799665412\n",
            "Epoch [81/100], Train Accuracy: 99.74 %, Loss: 0.0085\n",
            "Epoch [81/100], Learning Rate: 0.00028549663646838715\n",
            "Epoch [82/100], Train Accuracy: 99.764 %, Loss: 0.0014\n",
            "Epoch [82/100], Learning Rate: 0.00027419182585616055\n",
            "Epoch [83/100], Train Accuracy: 99.816 %, Loss: 0.0172\n",
            "Epoch [83/100], Learning Rate: 0.0002631592046130896\n",
            "Epoch [84/100], Train Accuracy: 99.81 %, Loss: 0.0008\n",
            "Epoch [84/100], Learning Rate: 0.00025240966060435674\n",
            "Epoch [85/100], Train Accuracy: 99.88 %, Loss: 0.0019\n",
            "Epoch [85/100], Learning Rate: 0.00024195380233209008\n",
            "Epoch [86/100], Train Accuracy: 99.886 %, Loss: 0.0078\n",
            "Epoch [86/100], Learning Rate: 0.00023180194846605365\n",
            "Epoch [87/100], Train Accuracy: 99.856 %, Loss: 0.0074\n",
            "Epoch [87/100], Learning Rate: 0.0002219641176603649\n",
            "Epoch [88/100], Train Accuracy: 99.91 %, Loss: 0.0030\n",
            "Epoch [88/100], Learning Rate: 0.0002124500186662932\n",
            "Epoch [89/100], Train Accuracy: 99.91 %, Loss: 0.0008\n",
            "Epoch [89/100], Learning Rate: 0.00020326904075089488\n",
            "Epoch [90/100], Train Accuracy: 99.924 %, Loss: 0.0006\n",
            "Epoch [90/100], Learning Rate: 0.00019443024443093932\n",
            "Epoch [91/100], Train Accuracy: 99.9 %, Loss: 0.0038\n",
            "Epoch [91/100], Learning Rate: 0.00018594235253127368\n",
            "Epoch [92/100], Train Accuracy: 99.944 %, Loss: 0.0011\n",
            "Epoch [92/100], Learning Rate: 0.00017781374157644722\n",
            "Epoch [93/100], Train Accuracy: 99.938 %, Loss: 0.0030\n",
            "Epoch [93/100], Learning Rate: 0.00017005243352409332\n",
            "Epoch [94/100], Train Accuracy: 99.932 %, Loss: 0.0020\n",
            "Epoch [94/100], Learning Rate: 0.0001626660878482253\n",
            "Epoch [95/100], Train Accuracy: 99.906 %, Loss: 0.0059\n",
            "Epoch [95/100], Learning Rate: 0.0001556619939802614\n",
            "Epoch [96/100], Train Accuracy: 99.942 %, Loss: 0.0057\n",
            "Epoch [96/100], Learning Rate: 0.0001490470641152345\n",
            "Epoch [97/100], Train Accuracy: 99.926 %, Loss: 0.0011\n",
            "Epoch [97/100], Learning Rate: 0.0001428278263902913\n",
            "Epoch [98/100], Train Accuracy: 99.952 %, Loss: 0.0002\n",
            "Epoch [98/100], Learning Rate: 0.00013701041844220858\n",
            "Epoch [99/100], Train Accuracy: 99.94 %, Loss: 0.0007\n",
            "Epoch [99/100], Learning Rate: 0.0001316005813502869\n",
            "Epoch [100/100], Train Accuracy: 99.938 %, Loss: 0.0004\n",
            "Epoch [100/100], Learning Rate: 0.00012660365397059855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの評価\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for images, labels in test_loader_cifar10:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度の確認\n",
        "    print('Test Accuracy: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "id": "1-yK4fD1z2Wd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51218fd9-92cd-41f6-ef11-16e45f667b77"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 94.27 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUメモリの解放\n",
        "del model\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "J-woiaozz2Wd"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}
