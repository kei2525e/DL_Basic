{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "m-QbZLFlyjCA",
        "cdzvZUWNzQAY",
        "_8vKbM9TzkCb",
        "9aOdldqfyoSt",
        "upaYuBYxytGA",
        "nac63oxOH5wc"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### ドライブのマウント"
      ],
      "metadata": {
        "id": "m-QbZLFlyjCA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9g8jY7G9ahIa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16cd2145-e66c-48c2-83d6-7a2ad44c9054"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Googleドライブをマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ライブラリ・モジュールのインポート"
      ],
      "metadata": {
        "id": "cdzvZUWNzQAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ライブラリの準備\n",
        "!pip install timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from timm.scheduler import CosineLRScheduler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import copy\n",
        "import random\n",
        "import pickle"
      ],
      "metadata": {
        "id": "l8wfSjPuboGS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1480e6d8-3d9e-4d22-8190-46cb609b49df"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.15.2+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Collecting huggingface-hub (from timm)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors (from timm)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (16.0.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "Installing collected packages: safetensors, huggingface-hub, timm\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 timm-0.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### シード値の設定"
      ],
      "metadata": {
        "id": "_8vKbM9TzkCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# シード値を設定\n",
        "def fix_seed(seed=1234):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "fix_seed(seed=1234)"
      ],
      "metadata": {
        "id": "pGX4Zk1LbtPr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### データセットの準備"
      ],
      "metadata": {
        "id": "9aOdldqfyoSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 平均値と標準偏差を計算するためのCIFAR10データセットの前処理を定義\n",
        "pre_transform_cifar10 = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# 平均値と標準偏差を計算するためのCIFAR10データセットの読み込み\n",
        "pre_train_dataset_cifar10 = datasets.CIFAR10(root='/content/data/', download=True, transform=pre_transform_cifar10)\n",
        "\n",
        "# 平均値と標準偏差を計算するための変数を初期化\n",
        "pre_mean_cifar10 = 0.0\n",
        "pre_std_cifar10 = 0.0\n",
        "pre_total_samples_cifar10 = len(pre_train_dataset_cifar10)\n",
        "\n",
        "# データセットのすべてのデータポイントに対して平均値と標準偏差を計算\n",
        "for data in pre_train_dataset_cifar10:\n",
        "    pre_image, _ = data\n",
        "    pre_mean_cifar10 += pre_image.mean(dim=(1, 2))  # テンソルのチャンネルごとに平均を計算\n",
        "    pre_std_cifar10 += pre_image.std(dim=(1, 2))    # テンソルのチャンネルごとに標準偏差を計算\n",
        "\n",
        "# データセット全体の平均値と標準偏差を計算\n",
        "pre_mean_cifar10 /= pre_total_samples_cifar10\n",
        "pre_std_cifar10 /= pre_total_samples_cifar10\n",
        "\n",
        "print(\"データセット全体の平均値: \", pre_mean_cifar10)\n",
        "print(\"データセット全体の標準偏差: \", pre_std_cifar10)"
      ],
      "metadata": {
        "id": "RLSipTJ0bu1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad7b131d-9831-4b77-c303-0bf7ae403654"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /content/data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 59960799.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/data/cifar-10-python.tar.gz to /content/data/\n",
            "データセット全体の平均値:  tensor([0.4914, 0.4822, 0.4465])\n",
            "データセット全体の標準偏差:  tensor([0.2023, 0.1994, 0.2010])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習用のCIFAR10データセットの前処理を定義\n",
        "train_transform_cifar10 = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=pre_mean_cifar10, std=pre_std_cifar10)\n",
        "    ])\n",
        "# テスト用のCIFAR10データセットの前処理を定義\n",
        "test_transform_cifar10 = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=pre_mean_cifar10, std=pre_std_cifar10)\n",
        "    ])\n",
        "\n",
        "# 学習用のCIFAR10データセットの読み込み\n",
        "train_dataset_cifar10 = torchvision.datasets.CIFAR10(root='/content/data/', train=True, transform=train_transform_cifar10, download=True)\n",
        "# テスト用のCIFAR10データセットの読み込み\n",
        "test_dataset_cifar10 = torchvision.datasets.CIFAR10(root='/content/data/', train=False, transform=test_transform_cifar10, download=True)\n",
        "\n",
        "# 学習用のCIFAR10データローダーを作成\n",
        "train_loader_cifar10 = torch.utils.data.DataLoader(dataset=train_dataset_cifar10, batch_size=512, shuffle=True, num_workers=2)\n",
        "# テスト用のCIFAR10データローダーを作成\n",
        "test_loader_cifar10 = torch.utils.data.DataLoader(dataset=test_dataset_cifar10, batch_size=512, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "ah3PRB7vb1UG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98e2dc1a-e823-4592-8b70-57a13c6f42c3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### モデルの実装"
      ],
      "metadata": {
        "id": "upaYuBYxytGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1×1の畳み込みを定義\n",
        "def conv1x1(in_channels, out_channels, stride=1):\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "# 3×3の畳み込みを定義\n",
        "def conv3x3(in_channels, out_channels, stride=1):\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)"
      ],
      "metadata": {
        "id": "uvwDV2h3b54A"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Residual Blocksを定義\n",
        "class BuildingBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = conv3x3(out_channels, out_channels)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # 入力と出力のチャンネル数が異なる場合（strideが1より大きい場合）、ダウンサンプリング\n",
        "        if in_channels != out_channels or stride > 1:\n",
        "            self.shortcut = conv1x1(in_channels, out_channels, stride)\n",
        "        else:\n",
        "            self.shortcut = nn.Sequential()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.bn1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv1(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.conv2(out)\n",
        "        # 残差写像と恒等写像の要素毎の和を計算\n",
        "        out += self.shortcut(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "pibhyT9q7T-K"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, block, depth, k, num_classes=10):\n",
        "        super().__init__()\n",
        "        assert (depth - 4) % 6 == 0, \"depth should be 6n + 4\"\n",
        "        n = (depth - 4) // 6\n",
        "        channels = [16, 16 * k, 32 * k, 64 * k]\n",
        "        self.conv1 = conv3x3(3, channels[0])\n",
        "        # Residual Blocks（1)\n",
        "        self.layer1 = self._make_layer(block, channels[0], channels[1], n)\n",
        "        # Residual Blocks（2）\n",
        "        self.layer2 = self._make_layer(block, channels[1], channels[2], n, stride=2)\n",
        "        # Residual Blocks（3）\n",
        "        self.layer3 = self._make_layer(block, channels[2], channels[3], n, stride=2)\n",
        "        self.bn = nn.BatchNorm2d(channels[3])\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(channels[3], num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                # Heの初期化（正規分布）\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                # 重みを1に初期化\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                # バイアスを0に初期化\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    # Residual Blocksを作成する関数を定義\n",
        "    def _make_layer(self, block, in_channels, out_channels, blocks, stride=1):\n",
        "        layers = []\n",
        "        # 最初の Residual Block（stride=stride）\n",
        "        layers.append(block(in_channels, out_channels, stride))\n",
        "        # 残りの Residual Block（stride=1）\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "QY3GI8Vu7cP2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 学習と評価（50 epochs, learning_rate=0.001）"
      ],
      "metadata": {
        "id": "og2QjiTglmAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# デバイスを設定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# モデルを定義（一番精度の高かったモデルを採用）\n",
        "model = torch.load('/content/drive/MyDrive/WideResNet28_10_CIFAR100_100epochs_CLRS_restest.pth')\n",
        "# 最終層を変更\n",
        "model.fc = nn.Linear(model.fc.in_features, 10)\n",
        "# 最終層以外のパラメータの勾配計算を停止\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "# 最終層のパラメータのみを学習するように設定\n",
        "for param in model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "# モデルをデバイスに転送\n",
        "model = model.to(device)\n",
        "# 学習前のモデルの重みを保存\n",
        "model_init = copy.deepcopy(model)\n",
        "\n",
        "# 学習率を設定\n",
        "learning_rate = 0.001\n",
        "# 全体のepoch数を設定\n",
        "num_epochs = 50\n",
        "# warm-upするepoch数を設定\n",
        "warmup_epochs = 5\n",
        "\n",
        "# 損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# オプティマイザーを設定\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=learning_rate)\n",
        "# スケジューラーを設定\n",
        "scheduler = CosineLRScheduler(optimizer, t_initial=num_epochs, lr_min=1e-4, warmup_t=warmup_epochs, warmup_lr_init=1e-4, warmup_prefix=True)\n",
        "\n",
        "# モデルの学習\n",
        "for epoch in range(num_epochs):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, (images, labels) in enumerate(train_loader_cifar10):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # 順伝播\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # 逆伝播\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # 全データ数\n",
        "        total += labels.size(0)\n",
        "        # 正解数\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度を計算\n",
        "    train_acc = 100 * correct / total\n",
        "    # 正解率精度と損失を確認\n",
        "    print(\"Epoch [{}/{}], Train Accuracy: {} %, Loss: {:.4f}\".format(epoch+1, num_epochs, 100 * correct / total, loss.item()))\n",
        "    # 1エポック終了後にスケジューラーを更新\n",
        "    scheduler.step(epoch)\n",
        "    # 学習率の確認\n",
        "    print(\"Epoch [{}/{}], Learning Rate: {}\".format(epoch+1, num_epochs, optimizer.param_groups[0]['lr']))\n",
        "\n",
        "# モデルを保存\n",
        "torch.save(model, '/content/drive/MyDrive/WideResNet28_10_CIFAR10_from_CIFAR100_100epochs_CLRS_50epochs_lr0.001_CLRS_restest.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f5dd57d-5c6a-45a0-dd65-bd3de6c76802",
        "id": "R7Wq45KklmAS"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Train Accuracy: 35.046 %, Loss: 1.5633\n",
            "Epoch [1/50], Learning Rate: 0.0001\n",
            "Epoch [2/50], Train Accuracy: 63.176 %, Loss: 1.2959\n",
            "Epoch [2/50], Learning Rate: 0.00028\n",
            "Epoch [3/50], Train Accuracy: 69.97 %, Loss: 0.9292\n",
            "Epoch [3/50], Learning Rate: 0.00045999999999999996\n",
            "Epoch [4/50], Train Accuracy: 73.084 %, Loss: 0.7587\n",
            "Epoch [4/50], Learning Rate: 0.0006399999999999999\n",
            "Epoch [5/50], Train Accuracy: 74.832 %, Loss: 0.7092\n",
            "Epoch [5/50], Learning Rate: 0.00082\n",
            "Epoch [6/50], Train Accuracy: 75.95 %, Loss: 0.8344\n",
            "Epoch [6/50], Learning Rate: 0.001\n",
            "Epoch [7/50], Train Accuracy: 76.81 %, Loss: 0.6227\n",
            "Epoch [7/50], Learning Rate: 0.0009991120277927223\n",
            "Epoch [8/50], Train Accuracy: 77.492 %, Loss: 0.6027\n",
            "Epoch [8/50], Learning Rate: 0.000996451615591515\n",
            "Epoch [9/50], Train Accuracy: 78.05 %, Loss: 0.6591\n",
            "Epoch [9/50], Learning Rate: 0.00099202926282791\n",
            "Epoch [10/50], Train Accuracy: 78.426 %, Loss: 0.5836\n",
            "Epoch [10/50], Learning Rate: 0.000985862422507884\n",
            "Epoch [11/50], Train Accuracy: 78.854 %, Loss: 0.6272\n",
            "Epoch [11/50], Learning Rate: 0.0009779754323328191\n",
            "Epoch [12/50], Train Accuracy: 79.266 %, Loss: 0.5591\n",
            "Epoch [12/50], Learning Rate: 0.0009683994186497131\n",
            "Epoch [13/50], Train Accuracy: 79.662 %, Loss: 0.5352\n",
            "Epoch [13/50], Learning Rate: 0.0009571721736097088\n",
            "Epoch [14/50], Train Accuracy: 79.756 %, Loss: 0.6435\n",
            "Epoch [14/50], Learning Rate: 0.0009443380060197386\n",
            "Epoch [15/50], Train Accuracy: 80.072 %, Loss: 0.6316\n",
            "Epoch [15/50], Learning Rate: 0.0009299475664759069\n",
            "Epoch [16/50], Train Accuracy: 79.99 %, Loss: 0.5899\n",
            "Epoch [16/50], Learning Rate: 0.0009140576474687263\n",
            "Epoch [17/50], Train Accuracy: 80.384 %, Loss: 0.6092\n",
            "Epoch [17/50], Learning Rate: 0.0008967309592491052\n",
            "Epoch [18/50], Train Accuracy: 80.618 %, Loss: 0.5866\n",
            "Epoch [18/50], Learning Rate: 0.0008780358823396353\n",
            "Epoch [19/50], Train Accuracy: 80.488 %, Loss: 0.5712\n",
            "Epoch [19/50], Learning Rate: 0.0008580461976679099\n",
            "Epoch [20/50], Train Accuracy: 80.738 %, Loss: 0.5681\n",
            "Epoch [20/50], Learning Rate: 0.0008368407953869105\n",
            "Epoch [21/50], Train Accuracy: 80.838 %, Loss: 0.5099\n",
            "Epoch [21/50], Learning Rate: 0.0008145033635316131\n",
            "Epoch [22/50], Train Accuracy: 81.08 %, Loss: 0.5805\n",
            "Epoch [22/50], Learning Rate: 0.0007911220577405485\n",
            "Epoch [23/50], Train Accuracy: 80.936 %, Loss: 0.5031\n",
            "Epoch [23/50], Learning Rate: 0.0007667891533457719\n",
            "Epoch [24/50], Train Accuracy: 81.07 %, Loss: 0.5165\n",
            "Epoch [24/50], Learning Rate: 0.0007416006812042827\n",
            "Epoch [25/50], Train Accuracy: 81.346 %, Loss: 0.5597\n",
            "Epoch [25/50], Learning Rate: 0.0007156560487081052\n",
            "Epoch [26/50], Train Accuracy: 81.328 %, Loss: 0.5061\n",
            "Epoch [26/50], Learning Rate: 0.0006890576474687264\n",
            "Epoch [27/50], Train Accuracy: 81.392 %, Loss: 0.5514\n",
            "Epoch [27/50], Learning Rate: 0.0006619104492241846\n",
            "Epoch [28/50], Train Accuracy: 81.548 %, Loss: 0.5197\n",
            "Epoch [28/50], Learning Rate: 0.0006343215915635762\n",
            "Epoch [29/50], Train Accuracy: 81.616 %, Loss: 0.4855\n",
            "Epoch [29/50], Learning Rate: 0.000606399955103937\n",
            "Epoch [30/50], Train Accuracy: 81.454 %, Loss: 0.4967\n",
            "Epoch [30/50], Learning Rate: 0.0005782557337881911\n",
            "Epoch [31/50], Train Accuracy: 81.83 %, Loss: 0.5567\n",
            "Epoch [31/50], Learning Rate: 0.00055\n",
            "Epoch [32/50], Train Accuracy: 81.66 %, Loss: 0.5155\n",
            "Epoch [32/50], Learning Rate: 0.000521744266211809\n",
            "Epoch [33/50], Train Accuracy: 81.718 %, Loss: 0.5305\n",
            "Epoch [33/50], Learning Rate: 0.0004936000448960632\n",
            "Epoch [34/50], Train Accuracy: 81.756 %, Loss: 0.5046\n",
            "Epoch [34/50], Learning Rate: 0.0004656784084364239\n",
            "Epoch [35/50], Train Accuracy: 81.804 %, Loss: 0.4985\n",
            "Epoch [35/50], Learning Rate: 0.0004380895507758153\n",
            "Epoch [36/50], Train Accuracy: 81.958 %, Loss: 0.5562\n",
            "Epoch [36/50], Learning Rate: 0.0004109423525312738\n",
            "Epoch [37/50], Train Accuracy: 81.96 %, Loss: 0.5261\n",
            "Epoch [37/50], Learning Rate: 0.000384343951291895\n",
            "Epoch [38/50], Train Accuracy: 82.118 %, Loss: 0.5011\n",
            "Epoch [38/50], Learning Rate: 0.0003583993187957173\n",
            "Epoch [39/50], Train Accuracy: 82.082 %, Loss: 0.4741\n",
            "Epoch [39/50], Learning Rate: 0.0003332108466542281\n",
            "Epoch [40/50], Train Accuracy: 81.93 %, Loss: 0.5372\n",
            "Epoch [40/50], Learning Rate: 0.0003088779422594514\n",
            "Epoch [41/50], Train Accuracy: 82.084 %, Loss: 0.4729\n",
            "Epoch [41/50], Learning Rate: 0.00028549663646838715\n",
            "Epoch [42/50], Train Accuracy: 82.256 %, Loss: 0.5589\n",
            "Epoch [42/50], Learning Rate: 0.0002631592046130896\n",
            "Epoch [43/50], Train Accuracy: 82.174 %, Loss: 0.5046\n",
            "Epoch [43/50], Learning Rate: 0.00024195380233209008\n",
            "Epoch [44/50], Train Accuracy: 82.16 %, Loss: 0.5507\n",
            "Epoch [44/50], Learning Rate: 0.0002219641176603649\n",
            "Epoch [45/50], Train Accuracy: 82.324 %, Loss: 0.5118\n",
            "Epoch [45/50], Learning Rate: 0.00020326904075089488\n",
            "Epoch [46/50], Train Accuracy: 82.074 %, Loss: 0.4825\n",
            "Epoch [46/50], Learning Rate: 0.00018594235253127368\n",
            "Epoch [47/50], Train Accuracy: 82.208 %, Loss: 0.4827\n",
            "Epoch [47/50], Learning Rate: 0.00017005243352409332\n",
            "Epoch [48/50], Train Accuracy: 82.288 %, Loss: 0.5204\n",
            "Epoch [48/50], Learning Rate: 0.0001556619939802614\n",
            "Epoch [49/50], Train Accuracy: 82.376 %, Loss: 0.5436\n",
            "Epoch [49/50], Learning Rate: 0.0001428278263902913\n",
            "Epoch [50/50], Train Accuracy: 82.412 %, Loss: 0.5007\n",
            "Epoch [50/50], Learning Rate: 0.0001316005813502869\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習前後のモデルの最終層以外の重みを比較する関数を作成\n",
        "def check_weight_change_except_last(model, model_init):\n",
        "    for (name, param), (name_init, param_init) in zip(model.named_parameters(), model_init.named_parameters()):\n",
        "        # 最終層を除く\n",
        "        if name != 'fc.weight' and name != 'fc.bias':\n",
        "            weights_changed = (param.data != param_init.data).any()\n",
        "            if weights_changed:\n",
        "                return print('最終層以外の重みが変化しています')\n",
        "    return print('最終層以外の重みは変化していません')\n",
        "\n",
        "# 学習前後のモデルの最終層の重みを比較する関数を作成\n",
        "def check_weight_change_last(model, model_init):\n",
        "    weights_changed = (model.fc.weight.data != model_init.fc.weight.data).any() or \\\n",
        "                      (model.fc.bias.data != model_init.fc.bias.data).any()\n",
        "    if weights_changed:\n",
        "        return print('最終層の重みは変化しています')\n",
        "    else:\n",
        "        return print('最終層の重みは変化していません')\n",
        "\n",
        "# 学習前後でモデルの最終層以外の重みが変化していないかを確認\n",
        "check_weight_change_except_last(model, model_init)\n",
        "# 学習前後でモデルの最終層の重みが変化しているかを確認\n",
        "check_weight_change_last(model, model_init)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2e030df-b426-4f9c-b641-14fbbdf5b3de",
        "id": "33mK4A8ZlmAS"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "最終層以外の重みは変化していません\n",
            "最終層の重みは変化しています\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの評価\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for images, labels in test_loader_cifar10:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度の確認\n",
        "    print('Test Accuracy: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c59ae2ca-b404-499b-aae4-930a4e6c1470",
        "id": "r-Zb_5_YlmAS"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 81.63 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUメモリの解放\n",
        "del model, model_init\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "ycsJDkRvlmAT"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 学習と評価（50 epochs, learning_rate=0.005）"
      ],
      "metadata": {
        "id": "eUhTuFxulmAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# デバイスを設定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# モデルを定義（一番精度の高かったモデルを採用）\n",
        "model = torch.load('/content/drive/MyDrive/WideResNet28_10_CIFAR100_100epochs_CLRS_restest.pth')\n",
        "# 最終層を変更\n",
        "model.fc = nn.Linear(model.fc.in_features, 10)\n",
        "# 最終層以外のパラメータの勾配計算を停止\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "# 最終層のパラメータのみを学習するように設定\n",
        "for param in model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "# モデルをデバイスに転送\n",
        "model = model.to(device)\n",
        "# 学習前のモデルの重みを保存\n",
        "model_init = copy.deepcopy(model)\n",
        "\n",
        "# 学習率を設定\n",
        "learning_rate = 0.005\n",
        "# 全体のepoch数を設定\n",
        "num_epochs = 50\n",
        "# warm-upするepoch数を設定\n",
        "warmup_epochs = 5\n",
        "\n",
        "# 損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# オプティマイザーを設定\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=learning_rate)\n",
        "# スケジューラーを設定\n",
        "scheduler = CosineLRScheduler(optimizer, t_initial=num_epochs, lr_min=1e-4, warmup_t=warmup_epochs, warmup_lr_init=1e-4, warmup_prefix=True)\n",
        "\n",
        "# モデルの学習\n",
        "for epoch in range(num_epochs):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, (images, labels) in enumerate(train_loader_cifar10):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # 順伝播\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # 逆伝播\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # 全データ数\n",
        "        total += labels.size(0)\n",
        "        # 正解数\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度を計算\n",
        "    train_acc = 100 * correct / total\n",
        "    # 正解率精度と損失を確認\n",
        "    print(\"Epoch [{}/{}], Train Accuracy: {} %, Loss: {:.4f}\".format(epoch+1, num_epochs, 100 * correct / total, loss.item()))\n",
        "    # 1エポック終了後にスケジューラーを更新\n",
        "    scheduler.step(epoch)\n",
        "    # 学習率の確認\n",
        "    print(\"Epoch [{}/{}], Learning Rate: {}\".format(epoch+1, num_epochs, optimizer.param_groups[0]['lr']))\n",
        "\n",
        "# モデルを保存\n",
        "torch.save(model, '/content/drive/MyDrive/WideResNet28_10_CIFAR10_from_CIFAR100_100epochs_CLRS_50epochs_lr0.005_CLRS_restest.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad943ba4-3568-47fa-b7ab-cc819224bfae",
        "id": "Ng2jGj5LlmAT"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Train Accuracy: 38.904 %, Loss: 1.5551\n",
            "Epoch [1/50], Learning Rate: 0.0001\n",
            "Epoch [2/50], Train Accuracy: 64.088 %, Loss: 1.2670\n",
            "Epoch [2/50], Learning Rate: 0.00108\n",
            "Epoch [3/50], Train Accuracy: 72.642 %, Loss: 0.6987\n",
            "Epoch [3/50], Learning Rate: 0.0020599999999999998\n",
            "Epoch [4/50], Train Accuracy: 75.552 %, Loss: 0.5956\n",
            "Epoch [4/50], Learning Rate: 0.0030399999999999997\n",
            "Epoch [5/50], Train Accuracy: 77.306 %, Loss: 0.6163\n",
            "Epoch [5/50], Learning Rate: 0.00402\n",
            "Epoch [6/50], Train Accuracy: 78.066 %, Loss: 0.5797\n",
            "Epoch [6/50], Learning Rate: 0.005\n",
            "Epoch [7/50], Train Accuracy: 78.576 %, Loss: 0.6233\n",
            "Epoch [7/50], Learning Rate: 0.004995165484649265\n",
            "Epoch [8/50], Train Accuracy: 79.374 %, Loss: 0.6044\n",
            "Epoch [8/50], Learning Rate: 0.004980681018220471\n",
            "Epoch [9/50], Train Accuracy: 79.844 %, Loss: 0.5719\n",
            "Epoch [9/50], Learning Rate: 0.004956603764285287\n",
            "Epoch [10/50], Train Accuracy: 80.128 %, Loss: 0.5341\n",
            "Epoch [10/50], Learning Rate: 0.0049230287447651466\n",
            "Epoch [11/50], Train Accuracy: 80.27 %, Loss: 0.5545\n",
            "Epoch [11/50], Learning Rate: 0.004880088464923126\n",
            "Epoch [12/50], Train Accuracy: 81.156 %, Loss: 0.4853\n",
            "Epoch [12/50], Learning Rate: 0.004827952390426216\n",
            "Epoch [13/50], Train Accuracy: 81.18 %, Loss: 0.5969\n",
            "Epoch [13/50], Learning Rate: 0.004766826278541748\n",
            "Epoch [14/50], Train Accuracy: 81.2 %, Loss: 0.5008\n",
            "Epoch [14/50], Learning Rate: 0.004696951366107466\n",
            "Epoch [15/50], Train Accuracy: 81.424 %, Loss: 0.4692\n",
            "Epoch [15/50], Learning Rate: 0.004618603417479937\n",
            "Epoch [16/50], Train Accuracy: 81.396 %, Loss: 0.5454\n",
            "Epoch [16/50], Learning Rate: 0.004532091636218621\n",
            "Epoch [17/50], Train Accuracy: 81.86 %, Loss: 0.4858\n",
            "Epoch [17/50], Learning Rate: 0.004437757444800684\n",
            "Epoch [18/50], Train Accuracy: 81.892 %, Loss: 0.5151\n",
            "Epoch [18/50], Learning Rate: 0.004335973137182458\n",
            "Epoch [19/50], Train Accuracy: 81.712 %, Loss: 0.4712\n",
            "Epoch [19/50], Learning Rate: 0.004227140409525288\n",
            "Epoch [20/50], Train Accuracy: 81.756 %, Loss: 0.4686\n",
            "Epoch [20/50], Learning Rate: 0.00411168877488429\n",
            "Epoch [21/50], Train Accuracy: 81.878 %, Loss: 0.4224\n",
            "Epoch [21/50], Learning Rate: 0.00399007386811656\n",
            "Epoch [22/50], Train Accuracy: 81.83 %, Loss: 0.5895\n",
            "Epoch [22/50], Learning Rate: 0.0038627756476985412\n",
            "Epoch [23/50], Train Accuracy: 82.22 %, Loss: 0.5288\n",
            "Epoch [23/50], Learning Rate: 0.003730296501549202\n",
            "Epoch [24/50], Train Accuracy: 82.398 %, Loss: 0.5887\n",
            "Epoch [24/50], Learning Rate: 0.003593159264334428\n",
            "Epoch [25/50], Train Accuracy: 82.38 %, Loss: 0.5434\n",
            "Epoch [25/50], Learning Rate: 0.003451905154077461\n",
            "Epoch [26/50], Train Accuracy: 82.538 %, Loss: 0.4977\n",
            "Epoch [26/50], Learning Rate: 0.003307091636218621\n",
            "Epoch [27/50], Train Accuracy: 82.846 %, Loss: 0.4912\n",
            "Epoch [27/50], Learning Rate: 0.0031592902235538936\n",
            "Epoch [28/50], Train Accuracy: 82.824 %, Loss: 0.5337\n",
            "Epoch [28/50], Learning Rate: 0.0030090842207350257\n",
            "Epoch [29/50], Train Accuracy: 82.532 %, Loss: 0.5935\n",
            "Epoch [29/50], Learning Rate: 0.002857066422232546\n",
            "Epoch [30/50], Train Accuracy: 82.924 %, Loss: 0.4597\n",
            "Epoch [30/50], Learning Rate: 0.0027038367728468176\n",
            "Epoch [31/50], Train Accuracy: 82.872 %, Loss: 0.5111\n",
            "Epoch [31/50], Learning Rate: 0.0025499999999999997\n",
            "Epoch [32/50], Train Accuracy: 82.964 %, Loss: 0.4371\n",
            "Epoch [32/50], Learning Rate: 0.002396163227153182\n",
            "Epoch [33/50], Train Accuracy: 82.94 %, Loss: 0.4997\n",
            "Epoch [33/50], Learning Rate: 0.0022429335777674544\n",
            "Epoch [34/50], Train Accuracy: 83.108 %, Loss: 0.4595\n",
            "Epoch [34/50], Learning Rate: 0.0020909157792649746\n",
            "Epoch [35/50], Train Accuracy: 83.214 %, Loss: 0.5147\n",
            "Epoch [35/50], Learning Rate: 0.0019407097764461056\n",
            "Epoch [36/50], Train Accuracy: 83.234 %, Loss: 0.5126\n",
            "Epoch [36/50], Learning Rate: 0.0017929083637813795\n",
            "Epoch [37/50], Train Accuracy: 83.238 %, Loss: 0.3962\n",
            "Epoch [37/50], Learning Rate: 0.0016480948459225396\n",
            "Epoch [38/50], Train Accuracy: 83.324 %, Loss: 0.4776\n",
            "Epoch [38/50], Learning Rate: 0.001506840735665572\n",
            "Epoch [39/50], Train Accuracy: 83.414 %, Loss: 0.4870\n",
            "Epoch [39/50], Learning Rate: 0.0013697034984507972\n",
            "Epoch [40/50], Train Accuracy: 83.576 %, Loss: 0.4303\n",
            "Epoch [40/50], Learning Rate: 0.0012372243523014576\n",
            "Epoch [41/50], Train Accuracy: 83.33 %, Loss: 0.4801\n",
            "Epoch [41/50], Learning Rate: 0.001109926131883441\n",
            "Epoch [42/50], Train Accuracy: 83.414 %, Loss: 0.4539\n",
            "Epoch [42/50], Learning Rate: 0.00098831122511571\n",
            "Epoch [43/50], Train Accuracy: 83.512 %, Loss: 0.4608\n",
            "Epoch [43/50], Learning Rate: 0.0008728595904747127\n",
            "Epoch [44/50], Train Accuracy: 83.572 %, Loss: 0.4515\n",
            "Epoch [44/50], Learning Rate: 0.0007640268628175422\n",
            "Epoch [45/50], Train Accuracy: 83.576 %, Loss: 0.4975\n",
            "Epoch [45/50], Learning Rate: 0.0006622425551993166\n",
            "Epoch [46/50], Train Accuracy: 83.518 %, Loss: 0.4793\n",
            "Epoch [46/50], Learning Rate: 0.000567908363781379\n",
            "Epoch [47/50], Train Accuracy: 83.854 %, Loss: 0.5119\n",
            "Epoch [47/50], Learning Rate: 0.0004813965825200636\n",
            "Epoch [48/50], Train Accuracy: 83.878 %, Loss: 0.5181\n",
            "Epoch [48/50], Learning Rate: 0.0004030486338925342\n",
            "Epoch [49/50], Train Accuracy: 83.9 %, Loss: 0.4660\n",
            "Epoch [49/50], Learning Rate: 0.0003331737214582526\n",
            "Epoch [50/50], Train Accuracy: 83.842 %, Loss: 0.4944\n",
            "Epoch [50/50], Learning Rate: 0.0002720476095737842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習前後のモデルの最終層以外の重みを比較する関数を作成\n",
        "def check_weight_change_except_last(model, model_init):\n",
        "    for (name, param), (name_init, param_init) in zip(model.named_parameters(), model_init.named_parameters()):\n",
        "        # 最終層を除く\n",
        "        if name != 'fc.weight' and name != 'fc.bias':\n",
        "            weights_changed = (param.data != param_init.data).any()\n",
        "            if weights_changed:\n",
        "                return print('最終層以外の重みが変化しています')\n",
        "    return print('最終層以外の重みは変化していません')\n",
        "\n",
        "# 学習前後のモデルの最終層の重みを比較する関数を作成\n",
        "def check_weight_change_last(model, model_init):\n",
        "    weights_changed = (model.fc.weight.data != model_init.fc.weight.data).any() or \\\n",
        "                      (model.fc.bias.data != model_init.fc.bias.data).any()\n",
        "    if weights_changed:\n",
        "        return print('最終層の重みは変化しています')\n",
        "    else:\n",
        "        return print('最終層の重みは変化していません')\n",
        "\n",
        "# 学習前後でモデルの最終層以外の重みが変化していないかを確認\n",
        "check_weight_change_except_last(model, model_init)\n",
        "# 学習前後でモデルの最終層の重みが変化しているかを確認\n",
        "check_weight_change_last(model, model_init)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4cf98b7-38c6-4d9a-e8a1-ea4098687340",
        "id": "646b5ii2lmAT"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "最終層以外の重みは変化していません\n",
            "最終層の重みは変化しています\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの評価\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for images, labels in test_loader_cifar10:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度の確認\n",
        "    print('Test Accuracy: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74a1503f-6a1d-4745-f411-91e3c5746040",
        "id": "ZUSn9hhblmAT"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 82.61 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUメモリの解放\n",
        "del model, model_init\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "7K26VmqKlmAU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 学習と評価（50 epochs, learning_rate=0.01）"
      ],
      "metadata": {
        "id": "X--hbdT1lmAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# デバイスを設定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# モデルを定義（一番精度の高かったモデルを採用）\n",
        "model = torch.load('/content/drive/MyDrive/WideResNet28_10_CIFAR100_100epochs_CLRS_restest.pth')\n",
        "# 最終層を変更\n",
        "model.fc = nn.Linear(model.fc.in_features, 10)\n",
        "# 最終層以外のパラメータの勾配計算を停止\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "# 最終層のパラメータのみを学習するように設定\n",
        "for param in model.fc.parameters():\n",
        "    param.requires_grad = True\n",
        "# モデルをデバイスに転送\n",
        "model = model.to(device)\n",
        "# 学習前のモデルの重みを保存\n",
        "model_init = copy.deepcopy(model)\n",
        "\n",
        "# 学習率を設定\n",
        "learning_rate = 0.01\n",
        "# 全体のepoch数を設定\n",
        "num_epochs = 50\n",
        "# warm-upするepoch数を設定\n",
        "warmup_epochs = 5\n",
        "\n",
        "# 損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# オプティマイザーを設定\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=learning_rate)\n",
        "# スケジューラーを設定\n",
        "scheduler = CosineLRScheduler(optimizer, t_initial=num_epochs, lr_min=1e-4, warmup_t=warmup_epochs, warmup_lr_init=1e-4, warmup_prefix=True)\n",
        "\n",
        "# モデルの学習\n",
        "for epoch in range(num_epochs):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, (images, labels) in enumerate(train_loader_cifar10):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # 順伝播\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # 逆伝播\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # 全データ数\n",
        "        total += labels.size(0)\n",
        "        # 正解数\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度を計算\n",
        "    train_acc = 100 * correct / total\n",
        "    # 正解率精度と損失を確認\n",
        "    print(\"Epoch [{}/{}], Train Accuracy: {} %, Loss: {:.4f}\".format(epoch+1, num_epochs, 100 * correct / total, loss.item()))\n",
        "    # 1エポック終了後にスケジューラーを更新\n",
        "    scheduler.step(epoch)\n",
        "    # 学習率の確認\n",
        "    print(\"Epoch [{}/{}], Learning Rate: {}\".format(epoch+1, num_epochs, optimizer.param_groups[0]['lr']))\n",
        "\n",
        "# モデルを保存\n",
        "torch.save(model, '/content/drive/MyDrive/WideResNet28_10_CIFAR10_from_CIFAR100_100epochs_CLRS_50epochs_lr0.01_CLRS_restest.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98b12607-630f-476d-f90a-9f4ec92e4672",
        "id": "6UHMRiUzlmAU"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Train Accuracy: 35.488 %, Loss: 1.5631\n",
            "Epoch [1/50], Learning Rate: 0.0001\n",
            "Epoch [2/50], Train Accuracy: 62.308 %, Loss: 1.2168\n",
            "Epoch [2/50], Learning Rate: 0.00208\n",
            "Epoch [3/50], Train Accuracy: 73.29 %, Loss: 0.7418\n",
            "Epoch [3/50], Learning Rate: 0.00406\n",
            "Epoch [4/50], Train Accuracy: 76.604 %, Loss: 0.5714\n",
            "Epoch [4/50], Learning Rate: 0.00604\n",
            "Epoch [5/50], Train Accuracy: 77.574 %, Loss: 0.5302\n",
            "Epoch [5/50], Learning Rate: 0.00802\n",
            "Epoch [6/50], Train Accuracy: 78.052 %, Loss: 0.6043\n",
            "Epoch [6/50], Learning Rate: 0.01\n",
            "Epoch [7/50], Train Accuracy: 78.35 %, Loss: 0.6700\n",
            "Epoch [7/50], Learning Rate: 0.009990232305719944\n",
            "Epoch [8/50], Train Accuracy: 79.122 %, Loss: 0.5951\n",
            "Epoch [8/50], Learning Rate: 0.009960967771506667\n",
            "Epoch [9/50], Train Accuracy: 79.176 %, Loss: 0.5601\n",
            "Epoch [9/50], Learning Rate: 0.00991232189110701\n",
            "Epoch [10/50], Train Accuracy: 80.138 %, Loss: 0.4514\n",
            "Epoch [10/50], Learning Rate: 0.009844486647586723\n",
            "Epoch [11/50], Train Accuracy: 80.248 %, Loss: 0.5786\n",
            "Epoch [11/50], Learning Rate: 0.009757729755661011\n",
            "Epoch [12/50], Train Accuracy: 80.558 %, Loss: 0.4784\n",
            "Epoch [12/50], Learning Rate: 0.009652393605146845\n",
            "Epoch [13/50], Train Accuracy: 80.79 %, Loss: 0.5324\n",
            "Epoch [13/50], Learning Rate: 0.009528893909706798\n",
            "Epoch [14/50], Train Accuracy: 81.226 %, Loss: 0.4853\n",
            "Epoch [14/50], Learning Rate: 0.009387718066217124\n",
            "Epoch [15/50], Train Accuracy: 81.482 %, Loss: 0.6966\n",
            "Epoch [15/50], Learning Rate: 0.009229423231234974\n",
            "Epoch [16/50], Train Accuracy: 81.146 %, Loss: 0.4815\n",
            "Epoch [16/50], Learning Rate: 0.00905463412215599\n",
            "Epoch [17/50], Train Accuracy: 81.678 %, Loss: 0.4753\n",
            "Epoch [17/50], Learning Rate: 0.008864040551740157\n",
            "Epoch [18/50], Train Accuracy: 80.998 %, Loss: 0.4780\n",
            "Epoch [18/50], Learning Rate: 0.008658394705735987\n",
            "Epoch [19/50], Train Accuracy: 81.93 %, Loss: 0.5707\n",
            "Epoch [19/50], Learning Rate: 0.00843850817434701\n",
            "Epoch [20/50], Train Accuracy: 81.762 %, Loss: 0.5274\n",
            "Epoch [20/50], Learning Rate: 0.008205248749256015\n",
            "Epoch [21/50], Train Accuracy: 81.744 %, Loss: 0.5259\n",
            "Epoch [21/50], Learning Rate: 0.007959536998847744\n",
            "Epoch [22/50], Train Accuracy: 81.778 %, Loss: 0.5382\n",
            "Epoch [22/50], Learning Rate: 0.007702342635146034\n",
            "Epoch [23/50], Train Accuracy: 82.138 %, Loss: 0.4571\n",
            "Epoch [23/50], Learning Rate: 0.007434680686803491\n",
            "Epoch [24/50], Train Accuracy: 81.762 %, Loss: 0.5828\n",
            "Epoch [24/50], Learning Rate: 0.0071576074932471105\n",
            "Epoch [25/50], Train Accuracy: 82.464 %, Loss: 0.5127\n",
            "Epoch [25/50], Learning Rate: 0.006872216535789157\n",
            "Epoch [26/50], Train Accuracy: 82.01 %, Loss: 0.6236\n",
            "Epoch [26/50], Learning Rate: 0.006579634122155991\n",
            "Epoch [27/50], Train Accuracy: 82.336 %, Loss: 0.5432\n",
            "Epoch [27/50], Learning Rate: 0.0062810149414660316\n",
            "Epoch [28/50], Train Accuracy: 82.32 %, Loss: 0.4306\n",
            "Epoch [28/50], Learning Rate: 0.005977537507199339\n",
            "Epoch [29/50], Train Accuracy: 82.914 %, Loss: 0.4671\n",
            "Epoch [29/50], Learning Rate: 0.005670399506143308\n",
            "Epoch [30/50], Train Accuracy: 82.48 %, Loss: 0.5265\n",
            "Epoch [30/50], Learning Rate: 0.005360813071670102\n",
            "Epoch [31/50], Train Accuracy: 83.044 %, Loss: 0.5164\n",
            "Epoch [31/50], Learning Rate: 0.005050000000000001\n",
            "Epoch [32/50], Train Accuracy: 82.67 %, Loss: 0.4489\n",
            "Epoch [32/50], Learning Rate: 0.004739186928329899\n",
            "Epoch [33/50], Train Accuracy: 82.748 %, Loss: 0.5013\n",
            "Epoch [33/50], Learning Rate: 0.004429600493856695\n",
            "Epoch [34/50], Train Accuracy: 82.962 %, Loss: 0.5533\n",
            "Epoch [34/50], Learning Rate: 0.0041224624928006635\n",
            "Epoch [35/50], Train Accuracy: 83.024 %, Loss: 0.4191\n",
            "Epoch [35/50], Learning Rate: 0.0038189850585339685\n",
            "Epoch [36/50], Train Accuracy: 83.092 %, Loss: 0.4765\n",
            "Epoch [36/50], Learning Rate: 0.003520365877844012\n",
            "Epoch [37/50], Train Accuracy: 83.114 %, Loss: 0.3973\n",
            "Epoch [37/50], Learning Rate: 0.0032277834642108457\n",
            "Epoch [38/50], Train Accuracy: 82.98 %, Loss: 0.4260\n",
            "Epoch [38/50], Learning Rate: 0.0029423925067528904\n",
            "Epoch [39/50], Train Accuracy: 83.438 %, Loss: 0.6329\n",
            "Epoch [39/50], Learning Rate: 0.002665319313196509\n",
            "Epoch [40/50], Train Accuracy: 83.518 %, Loss: 0.4562\n",
            "Epoch [40/50], Learning Rate: 0.0023976573648539653\n",
            "Epoch [41/50], Train Accuracy: 83.616 %, Loss: 0.4272\n",
            "Epoch [41/50], Learning Rate: 0.0021404630011522584\n",
            "Epoch [42/50], Train Accuracy: 83.684 %, Loss: 0.3958\n",
            "Epoch [42/50], Learning Rate: 0.001894751250743986\n",
            "Epoch [43/50], Train Accuracy: 83.764 %, Loss: 0.4446\n",
            "Epoch [43/50], Learning Rate: 0.001661491825652991\n",
            "Epoch [44/50], Train Accuracy: 83.728 %, Loss: 0.3835\n",
            "Epoch [44/50], Learning Rate: 0.001441605294264014\n",
            "Epoch [45/50], Train Accuracy: 83.826 %, Loss: 0.4262\n",
            "Epoch [45/50], Learning Rate: 0.001235959448259844\n",
            "Epoch [46/50], Train Accuracy: 83.75 %, Loss: 0.4537\n",
            "Epoch [46/50], Learning Rate: 0.0010453658778440107\n",
            "Epoch [47/50], Train Accuracy: 84.0 %, Loss: 0.4808\n",
            "Epoch [47/50], Learning Rate: 0.0008705767687650265\n",
            "Epoch [48/50], Train Accuracy: 83.974 %, Loss: 0.4866\n",
            "Epoch [48/50], Learning Rate: 0.0007122819337828754\n",
            "Epoch [49/50], Train Accuracy: 84.104 %, Loss: 0.4733\n",
            "Epoch [49/50], Learning Rate: 0.0005711060902932043\n",
            "Epoch [50/50], Train Accuracy: 84.076 %, Loss: 0.4592\n",
            "Epoch [50/50], Learning Rate: 0.00044760639485315584\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習前後のモデルの最終層以外の重みを比較する関数を作成\n",
        "def check_weight_change_except_last(model, model_init):\n",
        "    for (name, param), (name_init, param_init) in zip(model.named_parameters(), model_init.named_parameters()):\n",
        "        # 最終層を除く\n",
        "        if name != 'fc.weight' and name != 'fc.bias':\n",
        "            weights_changed = (param.data != param_init.data).any()\n",
        "            if weights_changed:\n",
        "                return print('最終層以外の重みが変化しています')\n",
        "    return print('最終層以外の重みは変化していません')\n",
        "\n",
        "# 学習前後のモデルの最終層の重みを比較する関数を作成\n",
        "def check_weight_change_last(model, model_init):\n",
        "    weights_changed = (model.fc.weight.data != model_init.fc.weight.data).any() or \\\n",
        "                      (model.fc.bias.data != model_init.fc.bias.data).any()\n",
        "    if weights_changed:\n",
        "        return print('最終層の重みは変化しています')\n",
        "    else:\n",
        "        return print('最終層の重みは変化していません')\n",
        "\n",
        "# 学習前後でモデルの最終層以外の重みが変化していないかを確認\n",
        "check_weight_change_except_last(model, model_init)\n",
        "# 学習前後でモデルの最終層の重みが変化しているかを確認\n",
        "check_weight_change_last(model, model_init)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "729df88b-a3ca-406a-b077-d900d19dbd86",
        "id": "HeJa4oQflmAU"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "最終層以外の重みは変化していません\n",
            "最終層の重みは変化しています\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの評価\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for images, labels in test_loader_cifar10:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度の確認\n",
        "    print('Test Accuracy: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcfe8baa-9fcb-4b49-ddea-63b9a62671d6",
        "id": "c3RwHNk2lmAU"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 82.89 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUメモリの解放\n",
        "del model, model_init\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "sp6Y8zpAlmAU"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}