{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "m-QbZLFlyjCA",
        "cdzvZUWNzQAY",
        "_8vKbM9TzkCb",
        "9aOdldqfyoSt",
        "4rSZ0nNXXi1A",
        "5JcyLv8zSrgN",
        "WICw1ew6SlWY",
        "q0lXUSFPkU-4",
        "wVweRGZbj9_H",
        "jQuP9L3Bxccy",
        "pJyew3dez2Wc",
        "_KprKABkwBxe",
        "FVN0ulM5TFEi",
        "wc65N7AtS-xa",
        "89pXntXAk7YN",
        "wdS4vAI5kfg_",
        "VFsN4qoY1qFw",
        "B6yICeOH1qFz",
        "xHXKrn9M6L_s"
      ],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### ドライブのマウント"
      ],
      "metadata": {
        "id": "m-QbZLFlyjCA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9g8jY7G9ahIa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d753e1ed-d055-4c35-8527-95e55b30a659"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Googleドライブをマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ライブラリ・モジュールのインポート"
      ],
      "metadata": {
        "id": "cdzvZUWNzQAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ライブラリの準備\n",
        "!pip install timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.nn.functional as F\n",
        "import torch.autograd as autograd\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "from timm.scheduler import CosineLRScheduler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import copy\n",
        "import random\n",
        "import pickle"
      ],
      "metadata": {
        "id": "l8wfSjPuboGS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f48b01bb-af4d-49c1-aaf8-c91be5991cbc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.15.2+cu118)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n",
            "Collecting huggingface-hub (from timm)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors (from timm)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (16.0.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "Installing collected packages: safetensors, huggingface-hub, timm\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 timm-0.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### シード値の設定"
      ],
      "metadata": {
        "id": "_8vKbM9TzkCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# シード値を設定\n",
        "def fix_seed(seed=1234):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "fix_seed(seed=1234)"
      ],
      "metadata": {
        "id": "pGX4Zk1LbtPr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### データセットの準備"
      ],
      "metadata": {
        "id": "9aOdldqfyoSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 平均値と標準偏差を計算するためのCIFAR10データセットの前処理を定義\n",
        "pre_transform_cifar10 = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# 平均値と標準偏差を計算するためのCIFAR10データセットの読み込み\n",
        "pre_train_dataset_cifar10 = datasets.CIFAR10(root='/content/data/', download=True, transform=pre_transform_cifar10)\n",
        "\n",
        "# 平均値と標準偏差を計算するための変数を初期化\n",
        "pre_mean_cifar10 = 0.0\n",
        "pre_std_cifar10 = 0.0\n",
        "pre_total_samples_cifar10 = len(pre_train_dataset_cifar10)\n",
        "\n",
        "# データセットのすべてのデータポイントに対して平均値と標準偏差を計算\n",
        "for data in pre_train_dataset_cifar10:\n",
        "    pre_image, _ = data\n",
        "    pre_mean_cifar10 += pre_image.mean(dim=(1, 2))  # テンソルのチャンネルごとに平均を計算\n",
        "    pre_std_cifar10 += pre_image.std(dim=(1, 2))    # テンソルのチャンネルごとに標準偏差を計算\n",
        "\n",
        "# データセット全体の平均値と標準偏差を計算\n",
        "pre_mean_cifar10 /= pre_total_samples_cifar10\n",
        "pre_std_cifar10 /= pre_total_samples_cifar10\n",
        "\n",
        "print(\"データセット全体の平均値: \", pre_mean_cifar10)\n",
        "print(\"データセット全体の標準偏差: \", pre_std_cifar10)"
      ],
      "metadata": {
        "id": "RLSipTJ0bu1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0138714d-a0de-4ffa-e918-79921dab1751"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /content/data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:06<00:00, 27256233.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting /content/data/cifar-10-python.tar.gz to /content/data/\n",
            "データセット全体の平均値:  tensor([0.4914, 0.4822, 0.4465])\n",
            "データセット全体の標準偏差:  tensor([0.2023, 0.1994, 0.2010])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習用のCIFAR10データセットの前処理を定義\n",
        "train_transform_cifar10 = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=pre_mean_cifar10, std=pre_std_cifar10)\n",
        "    ])\n",
        "# テスト用のCIFAR10データセットの前処理を定義\n",
        "test_transform_cifar10 = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=pre_mean_cifar10, std=pre_std_cifar10)\n",
        "    ])\n",
        "\n",
        "# 学習用のCIFAR10データセットの読み込み\n",
        "train_dataset_cifar10 = torchvision.datasets.CIFAR10(root='/content/data/', train=True, transform=train_transform_cifar10, download=True)\n",
        "# テスト用のCIFAR10データセットの読み込み\n",
        "test_dataset_cifar10 = torchvision.datasets.CIFAR10(root='/content/data/', train=False, transform=test_transform_cifar10, download=True)\n",
        "\n",
        "# 学習用のCIFAR10データローダーを作成\n",
        "train_loader_cifar10 = torch.utils.data.DataLoader(dataset=train_dataset_cifar10, batch_size=512, shuffle=True, num_workers=2)\n",
        "# テスト用のCIFAR10データローダーを作成\n",
        "test_loader_cifar10 = torch.utils.data.DataLoader(dataset=test_dataset_cifar10, batch_size=512, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "id": "ah3PRB7vb1UG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "666b7507-c637-430f-85b1-1672f8cbda77"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## モデルの実装"
      ],
      "metadata": {
        "id": "4rSZ0nNXXi1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1×1のサブネットワーク獲得用の畳み込みを定義\n",
        "def supermaskconv1x1(in_channels, out_channels, stride=1):\n",
        "    return SupermaskConv(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
        "\n",
        "# 3×3のサブネットワーク獲得用の畳み込みを定義\n",
        "def supermaskconv3x3(in_channels, out_channels, stride=1):\n",
        "    return SupermaskConv(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)"
      ],
      "metadata": {
        "id": "LGXnnKFiXoBI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# サブネットワーク獲得用のバッチ正則化として、非アフィン正則化を使用する（学習可能なパラメータを使用しない）\n",
        "class NonAffineBatchNorm(nn.BatchNorm2d):\n",
        "    def __init__(self, dim):\n",
        "        super(NonAffineBatchNorm, self).__init__(dim, affine=False)"
      ],
      "metadata": {
        "id": "lsTzgaTYXum7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 各重みにスコアを付与したスコアをソートしてtop k%を使用することでサブネットワークを獲得する\n",
        "class GetSubnet(autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, scores, k):\n",
        "        # スコアを複製する\n",
        "        out = scores.clone()\n",
        "        # スコアを昇順でソートする\n",
        "        _, idx = scores.flatten().sort()\n",
        "        # top k%以下のスコアの数\n",
        "        j = int((1 - k) * scores.numel())\n",
        "        # flat_outとoutは同じメモリを参照する（flat_outを変更するとoutにも影響する）\n",
        "        flat_out = out.flatten()\n",
        "        # top k%の要素を1にする\n",
        "        flat_out[idx[j:]] = 1\n",
        "        # top k%以外の要素を0にする\n",
        "        flat_out[idx[:j]] = 0\n",
        "        return out\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, g):\n",
        "        # 逆伝播時に勾配gをそのまま伝える\n",
        "        return g, None"
      ],
      "metadata": {
        "id": "MWW1Jx9JXzGI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# サブネットワーク獲得用の畳み込みを定義\n",
        "class SupermaskConv(nn.Conv2d):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        # 重みと同じ形状のスコアを用意\n",
        "        self.scores = nn.Parameter(torch.Tensor(self.weight.size()))\n",
        "        # スコアを一様分布で初期化\n",
        "        nn.init.kaiming_uniform_(self.scores, a=math.sqrt(5))\n",
        "        # 重みの勾配を無効化する\n",
        "        self.weight.requires_grad = False\n",
        "\n",
        "    def _init_conv(self):\n",
        "        # 重みを一様分布で初期化\n",
        "        if self.init == 'kaiming_uniform':\n",
        "            nn.init.kaiming_uniform_(self.weight, mode='fan_out', nonlinearity='relu')\n",
        "        # 重みを正規分布で初期化\n",
        "        elif self.init == 'kaiming_normal':\n",
        "            nn.init.kaiming_normal_(self.weight, mode='fan_out', nonlinearity='relu')\n",
        "        # 重みを符号つき定数で初期化\n",
        "        elif self.init == 'signed_constant':\n",
        "            fan = nn.init._calculate_correct_fan(self.weight, mode='fan_out')\n",
        "            gain = nn.init.calculate_gain('relu')\n",
        "            std = gain / math.sqrt(fan)\n",
        "            self.weight.data = self.weight.data.sign() * std\n",
        "\n",
        "    def set_init(self, init):\n",
        "        # 初期化手法を設定\n",
        "        self.init = init\n",
        "\n",
        "    def set_prune_rate(self, prune_rate):\n",
        "        # 刈り込み率を設定\n",
        "        self.prune_rate = prune_rate\n",
        "\n",
        "    @property\n",
        "    def clamped_scores(self):\n",
        "        # スコアとして非負の値を返すようにする（重要度を表す）\n",
        "        return self.scores.abs()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # サブネットワークを獲得\n",
        "        subnet = GetSubnet.apply(self.clamped_scores, 1 - self.prune_rate)\n",
        "        # サブネットワークでマスク\n",
        "        w = self.weight * subnet\n",
        "        x = F.conv2d(x, w, self.bias, self.stride, self.padding, self.dilation, self.groups)\n",
        "        return x"
      ],
      "metadata": {
        "id": "GMJZoKQHZQu2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# サブネットワーク獲得用のResidual Blocksを定義\n",
        "class SupermaskBasicBlock(nn.Module):\n",
        "    # 入力チャンネル数に対する出力チャンネル数の倍率\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv1 = supermaskconv3x3(in_channels, channels, stride)\n",
        "        self.bn1 = NonAffineBatchNorm(channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = supermaskconv3x3(channels, channels)\n",
        "        self.bn2 = NonAffineBatchNorm(channels)\n",
        "        # 入力と出力のチャンネル数が異なる場合（strideが1より大きい場合）、ダウンサンプリング\n",
        "        if in_channels != channels * self.expansion or stride > 1:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                supermaskconv1x1(in_channels, channels * self.expansion, stride),\n",
        "                NonAffineBatchNorm(channels * self.expansion)\n",
        "            )\n",
        "        else:\n",
        "            self.shortcut = nn.Sequential()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        # 残差写像と恒等写像の和を計算\n",
        "        out += self.shortcut(x)\n",
        "        out = self.relu(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "OsMxFg0KX2ri"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# サブネットワーク獲得用のResNetを定義\n",
        "class SupermaskResNet(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = SupermaskConv(3, self.in_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = NonAffineBatchNorm(self.in_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        # Residual Blocks(in_channels=64)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0], stride=1)\n",
        "        # Residual Blocks(in_channels=128)\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        # Residual Blocks(in_channels=256)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        # Residual Blocks(in_channels=512)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "        # 重みの初期化\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                # 初期化手法を設定\n",
        "                m.set_init(init)\n",
        "                # 刈り込み率を設定\n",
        "                m.set_prune_rate(prune_rate)\n",
        "                # 重みを初期化\n",
        "                m._init_conv()\n",
        "\n",
        "    # Residual Blocksを作成する関数を定義\n",
        "    def _make_layer(self, block, channels, blocks, stride):\n",
        "        layers = []\n",
        "        # 最初のResidual Block（stride=stride）\n",
        "        layers.append(block(self.in_channels, channels, stride))\n",
        "        # 残りのResidual Block（stride=1）\n",
        "        self.in_channels = channels * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.in_channels, channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "dfamezExX5c5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルを呼び出す関数を定義\n",
        "def supermaskresnet18_10():\n",
        "    return SupermaskResNet(SupermaskBasicBlock, [2, 2, 2, 2], num_classes=10)"
      ],
      "metadata": {
        "id": "UAl9b_sZbp3V"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 学習と評価（100 epochs, init=kaiming_uniform）"
      ],
      "metadata": {
        "id": "b5CnW4IW4iww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prune_rate=0.999"
      ],
      "metadata": {
        "id": "5JcyLv8zSrgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 刈り込み率を設定\n",
        "prune_rate = 0.999\n",
        "# 初期化手法を設定\n",
        "init = 'kaiming_uniform'\n",
        "\n",
        "# デバイスを設定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# モデルを定義\n",
        "model = supermaskresnet18_10().to(device)\n",
        "# 学習前のモデルの重みを保存\n",
        "model_init = copy.deepcopy(model)\n",
        "\n",
        "# 学習率を設定\n",
        "learning_rate = 0.001\n",
        "# 全体のepoch数を設定\n",
        "num_epochs = 100\n",
        "# warm-upするepoch数を設定\n",
        "warmup_epochs = 10\n",
        "\n",
        "# 損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# オプティマイザーを設定\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# スケジューラーを設定\n",
        "scheduler = CosineLRScheduler(optimizer, t_initial=num_epochs, lr_min=1e-4, warmup_t=warmup_epochs, warmup_lr_init=1e-4, warmup_prefix=True)\n",
        "\n",
        "# モデルの学習\n",
        "for epoch in range(num_epochs):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, (images, labels) in enumerate(train_loader_cifar10):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # 順伝播\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # 逆伝播\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # 全データ数\n",
        "        total += labels.size(0)\n",
        "        # 正解数\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度を計算\n",
        "    train_acc = 100 * correct / total\n",
        "    # 正解率精度と損失を確認\n",
        "    print(\"Epoch [{}/{}], Train Accuracy: {} %, Loss: {:.4f}\".format(epoch+1, num_epochs, 100 * correct / total, loss.item()))\n",
        "    # 1エポック終了後にスケジューラーを更新\n",
        "    scheduler.step(epoch)\n",
        "    # 学習率の確認\n",
        "    print(\"Epoch [{}/{}], Learning Rate: {}\".format(epoch+1, num_epochs, optimizer.param_groups[0]['lr']))\n",
        "\n",
        "# モデルを保存\n",
        "torch.save(model, '/content/drive/MyDrive/SupermaskResNet18_CIFAR10_100epochs_pr0.999_uniform_CLRS_restest.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0e1a6fe-651d-4dae-d5d2-849bb8431701",
        "id": "VVrTwkz1SrgY"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Accuracy: 17.336 %, Loss: 2.1483\n",
            "Epoch [1/100], Learning Rate: 0.0001\n",
            "Epoch [2/100], Train Accuracy: 20.944 %, Loss: 2.0780\n",
            "Epoch [2/100], Learning Rate: 0.00019\n",
            "Epoch [3/100], Train Accuracy: 24.04 %, Loss: 1.9844\n",
            "Epoch [3/100], Learning Rate: 0.00028\n",
            "Epoch [4/100], Train Accuracy: 25.19 %, Loss: 1.9681\n",
            "Epoch [4/100], Learning Rate: 0.00036999999999999994\n",
            "Epoch [5/100], Train Accuracy: 26.954 %, Loss: 2.0125\n",
            "Epoch [5/100], Learning Rate: 0.00045999999999999996\n",
            "Epoch [6/100], Train Accuracy: 28.576 %, Loss: 1.9016\n",
            "Epoch [6/100], Learning Rate: 0.00055\n",
            "Epoch [7/100], Train Accuracy: 29.868 %, Loss: 1.9469\n",
            "Epoch [7/100], Learning Rate: 0.0006399999999999999\n",
            "Epoch [8/100], Train Accuracy: 28.722 %, Loss: 2.2297\n",
            "Epoch [8/100], Learning Rate: 0.00073\n",
            "Epoch [9/100], Train Accuracy: 30.648 %, Loss: 1.9444\n",
            "Epoch [9/100], Learning Rate: 0.00082\n",
            "Epoch [10/100], Train Accuracy: 31.112 %, Loss: 2.5934\n",
            "Epoch [10/100], Learning Rate: 0.00091\n",
            "Epoch [11/100], Train Accuracy: 31.604 %, Loss: 1.8600\n",
            "Epoch [11/100], Learning Rate: 0.001\n",
            "Epoch [12/100], Train Accuracy: 31.396 %, Loss: 1.7935\n",
            "Epoch [12/100], Learning Rate: 0.0009997779521645793\n",
            "Epoch [13/100], Train Accuracy: 31.306 %, Loss: 1.9299\n",
            "Epoch [13/100], Learning Rate: 0.0009991120277927223\n",
            "Epoch [14/100], Train Accuracy: 32.444 %, Loss: 1.8421\n",
            "Epoch [14/100], Learning Rate: 0.000998002884071386\n",
            "Epoch [15/100], Train Accuracy: 32.642 %, Loss: 1.8133\n",
            "Epoch [15/100], Learning Rate: 0.000996451615591515\n",
            "Epoch [16/100], Train Accuracy: 32.988 %, Loss: 1.7963\n",
            "Epoch [16/100], Learning Rate: 0.000994459753267812\n",
            "Epoch [17/100], Train Accuracy: 33.12 %, Loss: 1.9411\n",
            "Epoch [17/100], Learning Rate: 0.00099202926282791\n",
            "Epoch [18/100], Train Accuracy: 34.028 %, Loss: 1.8056\n",
            "Epoch [18/100], Learning Rate: 0.0009891625428724363\n",
            "Epoch [19/100], Train Accuracy: 33.904 %, Loss: 1.8160\n",
            "Epoch [19/100], Learning Rate: 0.000985862422507884\n",
            "Epoch [20/100], Train Accuracy: 34.194 %, Loss: 1.8033\n",
            "Epoch [20/100], Learning Rate: 0.0009821321585546244\n",
            "Epoch [21/100], Train Accuracy: 33.692 %, Loss: 1.7588\n",
            "Epoch [21/100], Learning Rate: 0.0009779754323328191\n",
            "Epoch [22/100], Train Accuracy: 33.92 %, Loss: 1.8120\n",
            "Epoch [22/100], Learning Rate: 0.0009733963460294015\n",
            "Epoch [23/100], Train Accuracy: 34.054 %, Loss: 1.8400\n",
            "Epoch [23/100], Learning Rate: 0.0009683994186497131\n",
            "Epoch [24/100], Train Accuracy: 33.778 %, Loss: 1.7954\n",
            "Epoch [24/100], Learning Rate: 0.0009629895815577914\n",
            "Epoch [25/100], Train Accuracy: 33.948 %, Loss: 1.7968\n",
            "Epoch [25/100], Learning Rate: 0.0009571721736097088\n",
            "Epoch [26/100], Train Accuracy: 34.614 %, Loss: 2.0750\n",
            "Epoch [26/100], Learning Rate: 0.0009509529358847655\n",
            "Epoch [27/100], Train Accuracy: 34.284 %, Loss: 1.7637\n",
            "Epoch [27/100], Learning Rate: 0.0009443380060197386\n",
            "Epoch [28/100], Train Accuracy: 34.546 %, Loss: 1.7620\n",
            "Epoch [28/100], Learning Rate: 0.0009373339121517746\n",
            "Epoch [29/100], Train Accuracy: 34.4 %, Loss: 1.8143\n",
            "Epoch [29/100], Learning Rate: 0.0009299475664759069\n",
            "Epoch [30/100], Train Accuracy: 35.122 %, Loss: 1.7061\n",
            "Epoch [30/100], Learning Rate: 0.0009221862584235528\n",
            "Epoch [31/100], Train Accuracy: 35.176 %, Loss: 1.8191\n",
            "Epoch [31/100], Learning Rate: 0.0009140576474687263\n",
            "Epoch [32/100], Train Accuracy: 34.916 %, Loss: 1.7782\n",
            "Epoch [32/100], Learning Rate: 0.0009055697555690606\n",
            "Epoch [33/100], Train Accuracy: 35.138 %, Loss: 1.8339\n",
            "Epoch [33/100], Learning Rate: 0.0008967309592491052\n",
            "Epoch [34/100], Train Accuracy: 34.942 %, Loss: 1.7233\n",
            "Epoch [34/100], Learning Rate: 0.0008875499813337069\n",
            "Epoch [35/100], Train Accuracy: 35.484 %, Loss: 1.7861\n",
            "Epoch [35/100], Learning Rate: 0.0008780358823396353\n",
            "Epoch [36/100], Train Accuracy: 35.47 %, Loss: 1.7059\n",
            "Epoch [36/100], Learning Rate: 0.0008681980515339464\n",
            "Epoch [37/100], Train Accuracy: 35.23 %, Loss: 1.8285\n",
            "Epoch [37/100], Learning Rate: 0.0008580461976679099\n",
            "Epoch [38/100], Train Accuracy: 35.402 %, Loss: 1.7800\n",
            "Epoch [38/100], Learning Rate: 0.0008475903393956434\n",
            "Epoch [39/100], Train Accuracy: 35.252 %, Loss: 1.8570\n",
            "Epoch [39/100], Learning Rate: 0.0008368407953869105\n",
            "Epoch [40/100], Train Accuracy: 35.2 %, Loss: 1.8214\n",
            "Epoch [40/100], Learning Rate: 0.0008258081741438395\n",
            "Epoch [41/100], Train Accuracy: 35.842 %, Loss: 1.7904\n",
            "Epoch [41/100], Learning Rate: 0.0008145033635316131\n",
            "Epoch [42/100], Train Accuracy: 35.16 %, Loss: 1.8322\n",
            "Epoch [42/100], Learning Rate: 0.0008029375200334589\n",
            "Epoch [43/100], Train Accuracy: 35.374 %, Loss: 1.7526\n",
            "Epoch [43/100], Learning Rate: 0.0007911220577405485\n",
            "Epoch [44/100], Train Accuracy: 35.494 %, Loss: 1.7031\n",
            "Epoch [44/100], Learning Rate: 0.0007790686370876671\n",
            "Epoch [45/100], Train Accuracy: 35.246 %, Loss: 1.7591\n",
            "Epoch [45/100], Learning Rate: 0.0007667891533457719\n",
            "Epoch [46/100], Train Accuracy: 35.578 %, Loss: 1.7815\n",
            "Epoch [46/100], Learning Rate: 0.000754295724882796\n",
            "Epoch [47/100], Train Accuracy: 35.006 %, Loss: 2.2706\n",
            "Epoch [47/100], Learning Rate: 0.0007416006812042827\n",
            "Epoch [48/100], Train Accuracy: 35.22 %, Loss: 1.7429\n",
            "Epoch [48/100], Learning Rate: 0.0007287165507856513\n",
            "Epoch [49/100], Train Accuracy: 35.614 %, Loss: 1.8504\n",
            "Epoch [49/100], Learning Rate: 0.0007156560487081052\n",
            "Epoch [50/100], Train Accuracy: 35.864 %, Loss: 1.7983\n",
            "Epoch [50/100], Learning Rate: 0.0007024320641103813\n",
            "Epoch [51/100], Train Accuracy: 35.596 %, Loss: 1.7772\n",
            "Epoch [51/100], Learning Rate: 0.0006890576474687264\n",
            "Epoch [52/100], Train Accuracy: 35.266 %, Loss: 1.7879\n",
            "Epoch [52/100], Learning Rate: 0.0006755459977176532\n",
            "Epoch [53/100], Train Accuracy: 35.106 %, Loss: 1.7980\n",
            "Epoch [53/100], Learning Rate: 0.0006619104492241846\n",
            "Epoch [54/100], Train Accuracy: 35.446 %, Loss: 1.8560\n",
            "Epoch [54/100], Learning Rate: 0.0006481644586284443\n",
            "Epoch [55/100], Train Accuracy: 35.794 %, Loss: 1.8711\n",
            "Epoch [55/100], Learning Rate: 0.0006343215915635762\n",
            "Epoch [56/100], Train Accuracy: 35.372 %, Loss: 1.8082\n",
            "Epoch [56/100], Learning Rate: 0.000620395509268104\n",
            "Epoch [57/100], Train Accuracy: 35.14 %, Loss: 1.7108\n",
            "Epoch [57/100], Learning Rate: 0.000606399955103937\n",
            "Epoch [58/100], Train Accuracy: 35.688 %, Loss: 1.7698\n",
            "Epoch [58/100], Learning Rate: 0.0005923487409933315\n",
            "Epoch [59/100], Train Accuracy: 35.666 %, Loss: 1.7770\n",
            "Epoch [59/100], Learning Rate: 0.0005782557337881911\n",
            "Epoch [60/100], Train Accuracy: 35.556 %, Loss: 1.7804\n",
            "Epoch [60/100], Learning Rate: 0.0005641348415851578\n",
            "Epoch [61/100], Train Accuracy: 35.86 %, Loss: 1.8016\n",
            "Epoch [61/100], Learning Rate: 0.00055\n",
            "Epoch [62/100], Train Accuracy: 35.66 %, Loss: 1.7308\n",
            "Epoch [62/100], Learning Rate: 0.0005358651584148423\n",
            "Epoch [63/100], Train Accuracy: 35.66 %, Loss: 1.7255\n",
            "Epoch [63/100], Learning Rate: 0.000521744266211809\n",
            "Epoch [64/100], Train Accuracy: 35.334 %, Loss: 1.7986\n",
            "Epoch [64/100], Learning Rate: 0.0005076512590066686\n",
            "Epoch [65/100], Train Accuracy: 35.764 %, Loss: 1.6674\n",
            "Epoch [65/100], Learning Rate: 0.0004936000448960632\n",
            "Epoch [66/100], Train Accuracy: 35.932 %, Loss: 1.7893\n",
            "Epoch [66/100], Learning Rate: 0.0004796044907318961\n",
            "Epoch [67/100], Train Accuracy: 35.982 %, Loss: 1.7839\n",
            "Epoch [67/100], Learning Rate: 0.0004656784084364239\n",
            "Epoch [68/100], Train Accuracy: 36.322 %, Loss: 1.6944\n",
            "Epoch [68/100], Learning Rate: 0.00045183554137155597\n",
            "Epoch [69/100], Train Accuracy: 36.182 %, Loss: 1.7052\n",
            "Epoch [69/100], Learning Rate: 0.0004380895507758153\n",
            "Epoch [70/100], Train Accuracy: 35.712 %, Loss: 1.7799\n",
            "Epoch [70/100], Learning Rate: 0.00042445400228234684\n",
            "Epoch [71/100], Train Accuracy: 35.958 %, Loss: 1.8034\n",
            "Epoch [71/100], Learning Rate: 0.0004109423525312738\n",
            "Epoch [72/100], Train Accuracy: 35.56 %, Loss: 1.7862\n",
            "Epoch [72/100], Learning Rate: 0.0003975679358896189\n",
            "Epoch [73/100], Train Accuracy: 35.946 %, Loss: 1.8476\n",
            "Epoch [73/100], Learning Rate: 0.000384343951291895\n",
            "Epoch [74/100], Train Accuracy: 35.846 %, Loss: 1.6665\n",
            "Epoch [74/100], Learning Rate: 0.00037128344921434864\n",
            "Epoch [75/100], Train Accuracy: 35.942 %, Loss: 1.7294\n",
            "Epoch [75/100], Learning Rate: 0.0003583993187957173\n",
            "Epoch [76/100], Train Accuracy: 36.234 %, Loss: 1.6782\n",
            "Epoch [76/100], Learning Rate: 0.00034570427511720393\n",
            "Epoch [77/100], Train Accuracy: 36.022 %, Loss: 1.7459\n",
            "Epoch [77/100], Learning Rate: 0.0003332108466542281\n",
            "Epoch [78/100], Train Accuracy: 36.146 %, Loss: 1.6879\n",
            "Epoch [78/100], Learning Rate: 0.0003209313629123329\n",
            "Epoch [79/100], Train Accuracy: 36.108 %, Loss: 1.7610\n",
            "Epoch [79/100], Learning Rate: 0.0003088779422594514\n",
            "Epoch [80/100], Train Accuracy: 36.298 %, Loss: 1.7052\n",
            "Epoch [80/100], Learning Rate: 0.0002970624799665412\n",
            "Epoch [81/100], Train Accuracy: 35.958 %, Loss: 1.7829\n",
            "Epoch [81/100], Learning Rate: 0.00028549663646838715\n",
            "Epoch [82/100], Train Accuracy: 36.314 %, Loss: 1.7509\n",
            "Epoch [82/100], Learning Rate: 0.00027419182585616055\n",
            "Epoch [83/100], Train Accuracy: 36.486 %, Loss: 1.7484\n",
            "Epoch [83/100], Learning Rate: 0.0002631592046130896\n",
            "Epoch [84/100], Train Accuracy: 36.326 %, Loss: 1.7576\n",
            "Epoch [84/100], Learning Rate: 0.00025240966060435674\n",
            "Epoch [85/100], Train Accuracy: 36.464 %, Loss: 1.7305\n",
            "Epoch [85/100], Learning Rate: 0.00024195380233209008\n",
            "Epoch [86/100], Train Accuracy: 36.318 %, Loss: 1.7446\n",
            "Epoch [86/100], Learning Rate: 0.00023180194846605365\n",
            "Epoch [87/100], Train Accuracy: 36.618 %, Loss: 1.7626\n",
            "Epoch [87/100], Learning Rate: 0.0002219641176603649\n",
            "Epoch [88/100], Train Accuracy: 36.084 %, Loss: 1.7646\n",
            "Epoch [88/100], Learning Rate: 0.0002124500186662932\n",
            "Epoch [89/100], Train Accuracy: 36.562 %, Loss: 1.7100\n",
            "Epoch [89/100], Learning Rate: 0.00020326904075089488\n",
            "Epoch [90/100], Train Accuracy: 36.422 %, Loss: 1.7443\n",
            "Epoch [90/100], Learning Rate: 0.00019443024443093932\n",
            "Epoch [91/100], Train Accuracy: 36.448 %, Loss: 1.7258\n",
            "Epoch [91/100], Learning Rate: 0.00018594235253127368\n",
            "Epoch [92/100], Train Accuracy: 36.592 %, Loss: 1.7573\n",
            "Epoch [92/100], Learning Rate: 0.00017781374157644722\n",
            "Epoch [93/100], Train Accuracy: 35.98 %, Loss: 1.7142\n",
            "Epoch [93/100], Learning Rate: 0.00017005243352409332\n",
            "Epoch [94/100], Train Accuracy: 36.69 %, Loss: 1.8002\n",
            "Epoch [94/100], Learning Rate: 0.0001626660878482253\n",
            "Epoch [95/100], Train Accuracy: 36.608 %, Loss: 1.6915\n",
            "Epoch [95/100], Learning Rate: 0.0001556619939802614\n",
            "Epoch [96/100], Train Accuracy: 36.504 %, Loss: 1.7547\n",
            "Epoch [96/100], Learning Rate: 0.0001490470641152345\n",
            "Epoch [97/100], Train Accuracy: 37.024 %, Loss: 1.7324\n",
            "Epoch [97/100], Learning Rate: 0.0001428278263902913\n",
            "Epoch [98/100], Train Accuracy: 36.778 %, Loss: 1.6716\n",
            "Epoch [98/100], Learning Rate: 0.00013701041844220858\n",
            "Epoch [99/100], Train Accuracy: 36.81 %, Loss: 1.7235\n",
            "Epoch [99/100], Learning Rate: 0.0001316005813502869\n",
            "Epoch [100/100], Train Accuracy: 36.282 %, Loss: 1.7806\n",
            "Epoch [100/100], Learning Rate: 0.00012660365397059855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習前後のモデルの重みを比較する関数を作成\n",
        "def check_weight_change(model, model_init):\n",
        "    weights_changed = (model.conv1.state_dict()['weight'] != model_init.conv1.state_dict()['weight']).any()\n",
        "    if weights_changed:\n",
        "        return print('モデルの重みが変化しています')\n",
        "    else:\n",
        "        return print('モデルの重みは変化していません')\n",
        "# 学習前後でモデルの重みが変化していないかを確認\n",
        "check_weight_change(model, model_init)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49b81b3e-796a-446f-a5a6-b888c78bf0ef",
        "id": "rEtL1Io6SrgY"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "モデルの重みは変化していません\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの評価\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for images, labels in test_loader_cifar10:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度の確認\n",
        "    print('Test Accuracy: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d770587d-d91d-460d-f38d-cdfec7223911",
        "id": "P-83tFGMSrgY"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 29.16 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUメモリの解放\n",
        "del model, model_init\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "H0EfBwiqSrgY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prune_rate=0.995"
      ],
      "metadata": {
        "id": "WICw1ew6SlWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 刈り込み率を設定\n",
        "prune_rate = 0.995\n",
        "# 初期化手法を設定\n",
        "init = 'kaiming_uniform'\n",
        "\n",
        "# デバイスを設定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# モデルを定義\n",
        "model = supermaskresnet18_10().to(device)\n",
        "# 学習前のモデルの重みを保存\n",
        "model_init = copy.deepcopy(model)\n",
        "\n",
        "# 学習率を設定\n",
        "learning_rate = 0.001\n",
        "# 全体のepoch数を設定\n",
        "num_epochs = 100\n",
        "# warm-upするepoch数を設定\n",
        "warmup_epochs = 10\n",
        "\n",
        "# 損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# オプティマイザーを設定\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# スケジューラーを設定\n",
        "scheduler = CosineLRScheduler(optimizer, t_initial=num_epochs, lr_min=1e-4, warmup_t=warmup_epochs, warmup_lr_init=1e-4, warmup_prefix=True)\n",
        "\n",
        "# モデルの学習\n",
        "for epoch in range(num_epochs):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, (images, labels) in enumerate(train_loader_cifar10):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # 順伝播\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # 逆伝播\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # 全データ数\n",
        "        total += labels.size(0)\n",
        "        # 正解数\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度を計算\n",
        "    train_acc = 100 * correct / total\n",
        "    # 正解率精度と損失を確認\n",
        "    print(\"Epoch [{}/{}], Train Accuracy: {} %, Loss: {:.4f}\".format(epoch+1, num_epochs, 100 * correct / total, loss.item()))\n",
        "    # 1エポック終了後にスケジューラーを更新\n",
        "    scheduler.step(epoch)\n",
        "    # 学習率の確認\n",
        "    print(\"Epoch [{}/{}], Learning Rate: {}\".format(epoch+1, num_epochs, optimizer.param_groups[0]['lr']))\n",
        "\n",
        "# モデルを保存\n",
        "torch.save(model, '/content/drive/MyDrive/SupermaskResNet18_CIFAR10_100epochs_pr0.995_uniform_CLRS_restest.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b0c136c-60a5-4d0d-ce87-2c284b32595d",
        "id": "sUP-RS90SlWh"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Accuracy: 24.838 %, Loss: 1.9270\n",
            "Epoch [1/100], Learning Rate: 0.0001\n",
            "Epoch [2/100], Train Accuracy: 32.234 %, Loss: 1.7247\n",
            "Epoch [2/100], Learning Rate: 0.00019\n",
            "Epoch [3/100], Train Accuracy: 33.602 %, Loss: 1.6725\n",
            "Epoch [3/100], Learning Rate: 0.00028\n",
            "Epoch [4/100], Train Accuracy: 35.144 %, Loss: 1.6848\n",
            "Epoch [4/100], Learning Rate: 0.00036999999999999994\n",
            "Epoch [5/100], Train Accuracy: 37.35 %, Loss: 1.7020\n",
            "Epoch [5/100], Learning Rate: 0.00045999999999999996\n",
            "Epoch [6/100], Train Accuracy: 38.742 %, Loss: 1.6094\n",
            "Epoch [6/100], Learning Rate: 0.00055\n",
            "Epoch [7/100], Train Accuracy: 40.11 %, Loss: 1.6658\n",
            "Epoch [7/100], Learning Rate: 0.0006399999999999999\n",
            "Epoch [8/100], Train Accuracy: 40.86 %, Loss: 1.5785\n",
            "Epoch [8/100], Learning Rate: 0.00073\n",
            "Epoch [9/100], Train Accuracy: 41.172 %, Loss: 1.6587\n",
            "Epoch [9/100], Learning Rate: 0.00082\n",
            "Epoch [10/100], Train Accuracy: 41.956 %, Loss: 1.5864\n",
            "Epoch [10/100], Learning Rate: 0.00091\n",
            "Epoch [11/100], Train Accuracy: 43.194 %, Loss: 1.5220\n",
            "Epoch [11/100], Learning Rate: 0.001\n",
            "Epoch [12/100], Train Accuracy: 43.638 %, Loss: 1.6605\n",
            "Epoch [12/100], Learning Rate: 0.0009997779521645793\n",
            "Epoch [13/100], Train Accuracy: 43.782 %, Loss: 1.4935\n",
            "Epoch [13/100], Learning Rate: 0.0009991120277927223\n",
            "Epoch [14/100], Train Accuracy: 44.542 %, Loss: 1.4565\n",
            "Epoch [14/100], Learning Rate: 0.000998002884071386\n",
            "Epoch [15/100], Train Accuracy: 44.884 %, Loss: 1.5382\n",
            "Epoch [15/100], Learning Rate: 0.000996451615591515\n",
            "Epoch [16/100], Train Accuracy: 45.526 %, Loss: 1.4889\n",
            "Epoch [16/100], Learning Rate: 0.000994459753267812\n",
            "Epoch [17/100], Train Accuracy: 45.414 %, Loss: 1.5781\n",
            "Epoch [17/100], Learning Rate: 0.00099202926282791\n",
            "Epoch [18/100], Train Accuracy: 45.218 %, Loss: 1.4523\n",
            "Epoch [18/100], Learning Rate: 0.0009891625428724363\n",
            "Epoch [19/100], Train Accuracy: 46.41 %, Loss: 1.4584\n",
            "Epoch [19/100], Learning Rate: 0.000985862422507884\n",
            "Epoch [20/100], Train Accuracy: 46.256 %, Loss: 1.4381\n",
            "Epoch [20/100], Learning Rate: 0.0009821321585546244\n",
            "Epoch [21/100], Train Accuracy: 46.428 %, Loss: 1.5495\n",
            "Epoch [21/100], Learning Rate: 0.0009779754323328191\n",
            "Epoch [22/100], Train Accuracy: 46.768 %, Loss: 1.5490\n",
            "Epoch [22/100], Learning Rate: 0.0009733963460294015\n",
            "Epoch [23/100], Train Accuracy: 46.364 %, Loss: 1.4231\n",
            "Epoch [23/100], Learning Rate: 0.0009683994186497131\n",
            "Epoch [24/100], Train Accuracy: 46.22 %, Loss: 1.5960\n",
            "Epoch [24/100], Learning Rate: 0.0009629895815577914\n",
            "Epoch [25/100], Train Accuracy: 47.232 %, Loss: 1.5811\n",
            "Epoch [25/100], Learning Rate: 0.0009571721736097088\n",
            "Epoch [26/100], Train Accuracy: 46.866 %, Loss: 1.4791\n",
            "Epoch [26/100], Learning Rate: 0.0009509529358847655\n",
            "Epoch [27/100], Train Accuracy: 47.65 %, Loss: 1.4288\n",
            "Epoch [27/100], Learning Rate: 0.0009443380060197386\n",
            "Epoch [28/100], Train Accuracy: 47.726 %, Loss: 1.4639\n",
            "Epoch [28/100], Learning Rate: 0.0009373339121517746\n",
            "Epoch [29/100], Train Accuracy: 48.676 %, Loss: 1.4061\n",
            "Epoch [29/100], Learning Rate: 0.0009299475664759069\n",
            "Epoch [30/100], Train Accuracy: 47.268 %, Loss: 1.5299\n",
            "Epoch [30/100], Learning Rate: 0.0009221862584235528\n",
            "Epoch [31/100], Train Accuracy: 47.872 %, Loss: 1.4693\n",
            "Epoch [31/100], Learning Rate: 0.0009140576474687263\n",
            "Epoch [32/100], Train Accuracy: 48.514 %, Loss: 1.5520\n",
            "Epoch [32/100], Learning Rate: 0.0009055697555690606\n",
            "Epoch [33/100], Train Accuracy: 48.35 %, Loss: 1.3789\n",
            "Epoch [33/100], Learning Rate: 0.0008967309592491052\n",
            "Epoch [34/100], Train Accuracy: 48.844 %, Loss: 1.5377\n",
            "Epoch [34/100], Learning Rate: 0.0008875499813337069\n",
            "Epoch [35/100], Train Accuracy: 48.788 %, Loss: 1.3160\n",
            "Epoch [35/100], Learning Rate: 0.0008780358823396353\n",
            "Epoch [36/100], Train Accuracy: 48.3 %, Loss: 1.4734\n",
            "Epoch [36/100], Learning Rate: 0.0008681980515339464\n",
            "Epoch [37/100], Train Accuracy: 49.26 %, Loss: 1.4520\n",
            "Epoch [37/100], Learning Rate: 0.0008580461976679099\n",
            "Epoch [38/100], Train Accuracy: 48.79 %, Loss: 1.4310\n",
            "Epoch [38/100], Learning Rate: 0.0008475903393956434\n",
            "Epoch [39/100], Train Accuracy: 49.12 %, Loss: 1.4302\n",
            "Epoch [39/100], Learning Rate: 0.0008368407953869105\n",
            "Epoch [40/100], Train Accuracy: 49.518 %, Loss: 1.4423\n",
            "Epoch [40/100], Learning Rate: 0.0008258081741438395\n",
            "Epoch [41/100], Train Accuracy: 49.33 %, Loss: 1.3563\n",
            "Epoch [41/100], Learning Rate: 0.0008145033635316131\n",
            "Epoch [42/100], Train Accuracy: 49.39 %, Loss: 1.3423\n",
            "Epoch [42/100], Learning Rate: 0.0008029375200334589\n",
            "Epoch [43/100], Train Accuracy: 49.84 %, Loss: 1.4475\n",
            "Epoch [43/100], Learning Rate: 0.0007911220577405485\n",
            "Epoch [44/100], Train Accuracy: 50.392 %, Loss: 1.2214\n",
            "Epoch [44/100], Learning Rate: 0.0007790686370876671\n",
            "Epoch [45/100], Train Accuracy: 50.084 %, Loss: 1.3835\n",
            "Epoch [45/100], Learning Rate: 0.0007667891533457719\n",
            "Epoch [46/100], Train Accuracy: 50.034 %, Loss: 1.5125\n",
            "Epoch [46/100], Learning Rate: 0.000754295724882796\n",
            "Epoch [47/100], Train Accuracy: 50.192 %, Loss: 1.3919\n",
            "Epoch [47/100], Learning Rate: 0.0007416006812042827\n",
            "Epoch [48/100], Train Accuracy: 50.074 %, Loss: 1.3470\n",
            "Epoch [48/100], Learning Rate: 0.0007287165507856513\n",
            "Epoch [49/100], Train Accuracy: 50.822 %, Loss: 1.2611\n",
            "Epoch [49/100], Learning Rate: 0.0007156560487081052\n",
            "Epoch [50/100], Train Accuracy: 50.404 %, Loss: 1.3818\n",
            "Epoch [50/100], Learning Rate: 0.0007024320641103813\n",
            "Epoch [51/100], Train Accuracy: 50.526 %, Loss: 1.3794\n",
            "Epoch [51/100], Learning Rate: 0.0006890576474687264\n",
            "Epoch [52/100], Train Accuracy: 50.868 %, Loss: 1.3806\n",
            "Epoch [52/100], Learning Rate: 0.0006755459977176532\n",
            "Epoch [53/100], Train Accuracy: 51.022 %, Loss: 1.3927\n",
            "Epoch [53/100], Learning Rate: 0.0006619104492241846\n",
            "Epoch [54/100], Train Accuracy: 51.196 %, Loss: 1.3753\n",
            "Epoch [54/100], Learning Rate: 0.0006481644586284443\n",
            "Epoch [55/100], Train Accuracy: 51.102 %, Loss: 1.4664\n",
            "Epoch [55/100], Learning Rate: 0.0006343215915635762\n",
            "Epoch [56/100], Train Accuracy: 51.166 %, Loss: 1.2391\n",
            "Epoch [56/100], Learning Rate: 0.000620395509268104\n",
            "Epoch [57/100], Train Accuracy: 50.772 %, Loss: 1.3814\n",
            "Epoch [57/100], Learning Rate: 0.000606399955103937\n",
            "Epoch [58/100], Train Accuracy: 51.596 %, Loss: 1.2937\n",
            "Epoch [58/100], Learning Rate: 0.0005923487409933315\n",
            "Epoch [59/100], Train Accuracy: 51.186 %, Loss: 1.4048\n",
            "Epoch [59/100], Learning Rate: 0.0005782557337881911\n",
            "Epoch [60/100], Train Accuracy: 51.584 %, Loss: 1.3336\n",
            "Epoch [60/100], Learning Rate: 0.0005641348415851578\n",
            "Epoch [61/100], Train Accuracy: 51.164 %, Loss: 1.3544\n",
            "Epoch [61/100], Learning Rate: 0.00055\n",
            "Epoch [62/100], Train Accuracy: 51.772 %, Loss: 1.1988\n",
            "Epoch [62/100], Learning Rate: 0.0005358651584148423\n",
            "Epoch [63/100], Train Accuracy: 51.62 %, Loss: 1.3241\n",
            "Epoch [63/100], Learning Rate: 0.000521744266211809\n",
            "Epoch [64/100], Train Accuracy: 51.074 %, Loss: 1.4407\n",
            "Epoch [64/100], Learning Rate: 0.0005076512590066686\n",
            "Epoch [65/100], Train Accuracy: 51.692 %, Loss: 1.3697\n",
            "Epoch [65/100], Learning Rate: 0.0004936000448960632\n",
            "Epoch [66/100], Train Accuracy: 51.798 %, Loss: 1.3906\n",
            "Epoch [66/100], Learning Rate: 0.0004796044907318961\n",
            "Epoch [67/100], Train Accuracy: 51.818 %, Loss: 1.4059\n",
            "Epoch [67/100], Learning Rate: 0.0004656784084364239\n",
            "Epoch [68/100], Train Accuracy: 52.028 %, Loss: 1.2408\n",
            "Epoch [68/100], Learning Rate: 0.00045183554137155597\n",
            "Epoch [69/100], Train Accuracy: 52.072 %, Loss: 1.8075\n",
            "Epoch [69/100], Learning Rate: 0.0004380895507758153\n",
            "Epoch [70/100], Train Accuracy: 51.686 %, Loss: 1.3894\n",
            "Epoch [70/100], Learning Rate: 0.00042445400228234684\n",
            "Epoch [71/100], Train Accuracy: 52.272 %, Loss: 1.2973\n",
            "Epoch [71/100], Learning Rate: 0.0004109423525312738\n",
            "Epoch [72/100], Train Accuracy: 52.508 %, Loss: 1.3228\n",
            "Epoch [72/100], Learning Rate: 0.0003975679358896189\n",
            "Epoch [73/100], Train Accuracy: 51.978 %, Loss: 1.3025\n",
            "Epoch [73/100], Learning Rate: 0.000384343951291895\n",
            "Epoch [74/100], Train Accuracy: 52.916 %, Loss: 1.3554\n",
            "Epoch [74/100], Learning Rate: 0.00037128344921434864\n",
            "Epoch [75/100], Train Accuracy: 52.598 %, Loss: 1.2715\n",
            "Epoch [75/100], Learning Rate: 0.0003583993187957173\n",
            "Epoch [76/100], Train Accuracy: 52.91 %, Loss: 1.3739\n",
            "Epoch [76/100], Learning Rate: 0.00034570427511720393\n",
            "Epoch [77/100], Train Accuracy: 52.854 %, Loss: 1.3933\n",
            "Epoch [77/100], Learning Rate: 0.0003332108466542281\n",
            "Epoch [78/100], Train Accuracy: 52.666 %, Loss: 1.2755\n",
            "Epoch [78/100], Learning Rate: 0.0003209313629123329\n",
            "Epoch [79/100], Train Accuracy: 53.144 %, Loss: 1.2457\n",
            "Epoch [79/100], Learning Rate: 0.0003088779422594514\n",
            "Epoch [80/100], Train Accuracy: 52.762 %, Loss: 1.3514\n",
            "Epoch [80/100], Learning Rate: 0.0002970624799665412\n",
            "Epoch [81/100], Train Accuracy: 52.534 %, Loss: 1.2393\n",
            "Epoch [81/100], Learning Rate: 0.00028549663646838715\n",
            "Epoch [82/100], Train Accuracy: 53.228 %, Loss: 1.3379\n",
            "Epoch [82/100], Learning Rate: 0.00027419182585616055\n",
            "Epoch [83/100], Train Accuracy: 52.768 %, Loss: 1.2839\n",
            "Epoch [83/100], Learning Rate: 0.0002631592046130896\n",
            "Epoch [84/100], Train Accuracy: 53.624 %, Loss: 1.2310\n",
            "Epoch [84/100], Learning Rate: 0.00025240966060435674\n",
            "Epoch [85/100], Train Accuracy: 53.616 %, Loss: 1.2694\n",
            "Epoch [85/100], Learning Rate: 0.00024195380233209008\n",
            "Epoch [86/100], Train Accuracy: 53.52 %, Loss: 1.3954\n",
            "Epoch [86/100], Learning Rate: 0.00023180194846605365\n",
            "Epoch [87/100], Train Accuracy: 53.428 %, Loss: 1.2250\n",
            "Epoch [87/100], Learning Rate: 0.0002219641176603649\n",
            "Epoch [88/100], Train Accuracy: 53.754 %, Loss: 1.2719\n",
            "Epoch [88/100], Learning Rate: 0.0002124500186662932\n",
            "Epoch [89/100], Train Accuracy: 53.276 %, Loss: 1.3613\n",
            "Epoch [89/100], Learning Rate: 0.00020326904075089488\n",
            "Epoch [90/100], Train Accuracy: 53.44 %, Loss: 1.2637\n",
            "Epoch [90/100], Learning Rate: 0.00019443024443093932\n",
            "Epoch [91/100], Train Accuracy: 53.528 %, Loss: 1.4100\n",
            "Epoch [91/100], Learning Rate: 0.00018594235253127368\n",
            "Epoch [92/100], Train Accuracy: 53.506 %, Loss: 1.3535\n",
            "Epoch [92/100], Learning Rate: 0.00017781374157644722\n",
            "Epoch [93/100], Train Accuracy: 53.4 %, Loss: 1.2725\n",
            "Epoch [93/100], Learning Rate: 0.00017005243352409332\n",
            "Epoch [94/100], Train Accuracy: 53.628 %, Loss: 1.2565\n",
            "Epoch [94/100], Learning Rate: 0.0001626660878482253\n",
            "Epoch [95/100], Train Accuracy: 54.226 %, Loss: 1.2273\n",
            "Epoch [95/100], Learning Rate: 0.0001556619939802614\n",
            "Epoch [96/100], Train Accuracy: 53.632 %, Loss: 1.3987\n",
            "Epoch [96/100], Learning Rate: 0.0001490470641152345\n",
            "Epoch [97/100], Train Accuracy: 53.908 %, Loss: 1.3161\n",
            "Epoch [97/100], Learning Rate: 0.0001428278263902913\n",
            "Epoch [98/100], Train Accuracy: 54.122 %, Loss: 1.3605\n",
            "Epoch [98/100], Learning Rate: 0.00013701041844220858\n",
            "Epoch [99/100], Train Accuracy: 54.092 %, Loss: 1.2468\n",
            "Epoch [99/100], Learning Rate: 0.0001316005813502869\n",
            "Epoch [100/100], Train Accuracy: 54.07 %, Loss: 1.2860\n",
            "Epoch [100/100], Learning Rate: 0.00012660365397059855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習前後のモデルの重みを比較する関数を作成\n",
        "def check_weight_change(model, model_init):\n",
        "    weights_changed = (model.conv1.state_dict()['weight'] != model_init.conv1.state_dict()['weight']).any()\n",
        "    if weights_changed:\n",
        "        return print('モデルの重みが変化しています')\n",
        "    else:\n",
        "        return print('モデルの重みは変化していません')\n",
        "# 学習前後でモデルの重みが変化していないかを確認\n",
        "check_weight_change(model, model_init)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34426d76-7060-4e46-f496-25b71263fba5",
        "id": "E37-zkaTSlWi"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "モデルの重みは変化していません\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの評価\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for images, labels in test_loader_cifar10:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度の確認\n",
        "    print('Test Accuracy: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5f7f12b-88a7-4853-a27a-60725f303346",
        "id": "bkLNl3pFSlWi"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 45.69 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUメモリの解放\n",
        "del model, model_init\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "nKtpj2MySlWi"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prune_rate=0.99"
      ],
      "metadata": {
        "id": "q0lXUSFPkU-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 刈り込み率を設定\n",
        "prune_rate = 0.99\n",
        "# 初期化手法を設定\n",
        "init = 'kaiming_uniform'\n",
        "\n",
        "# デバイスを設定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# モデルを定義\n",
        "model = supermaskresnet18_10().to(device)\n",
        "# 学習前のモデルの重みを保存\n",
        "model_init = copy.deepcopy(model)\n",
        "\n",
        "# 学習率を設定\n",
        "learning_rate = 0.001\n",
        "# 全体のepoch数を設定\n",
        "num_epochs = 100\n",
        "# warm-upするepoch数を設定\n",
        "warmup_epochs = 10\n",
        "\n",
        "# 損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# オプティマイザーを設定\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# スケジューラーを設定\n",
        "scheduler = CosineLRScheduler(optimizer, t_initial=num_epochs, lr_min=1e-4, warmup_t=warmup_epochs, warmup_lr_init=1e-4, warmup_prefix=True)\n",
        "\n",
        "# モデルの学習\n",
        "for epoch in range(num_epochs):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, (images, labels) in enumerate(train_loader_cifar10):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # 順伝播\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # 逆伝播\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # 全データ数\n",
        "        total += labels.size(0)\n",
        "        # 正解数\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度を計算\n",
        "    train_acc = 100 * correct / total\n",
        "    # 正解率精度と損失を確認\n",
        "    print(\"Epoch [{}/{}], Train Accuracy: {} %, Loss: {:.4f}\".format(epoch+1, num_epochs, 100 * correct / total, loss.item()))\n",
        "    # 1エポック終了後にスケジューラーを更新\n",
        "    scheduler.step(epoch)\n",
        "    # 学習率の確認\n",
        "    print(\"Epoch [{}/{}], Learning Rate: {}\".format(epoch+1, num_epochs, optimizer.param_groups[0]['lr']))\n",
        "\n",
        "# モデルを保存\n",
        "torch.save(model, '/content/drive/MyDrive/SupermaskResNet18_CIFAR10_100epochs_pr0.99_uniform_CLRS_restest.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abf3fb95-1717-4946-b22a-1783cffc397b",
        "id": "kljInc-DkU_F"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Accuracy: 29.01 %, Loss: 1.7104\n",
            "Epoch [1/100], Learning Rate: 0.0001\n",
            "Epoch [2/100], Train Accuracy: 36.924 %, Loss: 1.7044\n",
            "Epoch [2/100], Learning Rate: 0.00019\n",
            "Epoch [3/100], Train Accuracy: 38.728 %, Loss: 1.6013\n",
            "Epoch [3/100], Learning Rate: 0.00028\n",
            "Epoch [4/100], Train Accuracy: 40.15 %, Loss: 1.6059\n",
            "Epoch [4/100], Learning Rate: 0.00036999999999999994\n",
            "Epoch [5/100], Train Accuracy: 41.464 %, Loss: 1.6567\n",
            "Epoch [5/100], Learning Rate: 0.00045999999999999996\n",
            "Epoch [6/100], Train Accuracy: 42.354 %, Loss: 1.5319\n",
            "Epoch [6/100], Learning Rate: 0.00055\n",
            "Epoch [7/100], Train Accuracy: 44.206 %, Loss: 1.5632\n",
            "Epoch [7/100], Learning Rate: 0.0006399999999999999\n",
            "Epoch [8/100], Train Accuracy: 45.128 %, Loss: 1.4351\n",
            "Epoch [8/100], Learning Rate: 0.00073\n",
            "Epoch [9/100], Train Accuracy: 46.078 %, Loss: 1.4743\n",
            "Epoch [9/100], Learning Rate: 0.00082\n",
            "Epoch [10/100], Train Accuracy: 46.586 %, Loss: 1.4918\n",
            "Epoch [10/100], Learning Rate: 0.00091\n",
            "Epoch [11/100], Train Accuracy: 47.522 %, Loss: 1.5174\n",
            "Epoch [11/100], Learning Rate: 0.001\n",
            "Epoch [12/100], Train Accuracy: 48.04 %, Loss: 1.3278\n",
            "Epoch [12/100], Learning Rate: 0.0009997779521645793\n",
            "Epoch [13/100], Train Accuracy: 48.61 %, Loss: 1.4916\n",
            "Epoch [13/100], Learning Rate: 0.0009991120277927223\n",
            "Epoch [14/100], Train Accuracy: 49.664 %, Loss: 1.2417\n",
            "Epoch [14/100], Learning Rate: 0.000998002884071386\n",
            "Epoch [15/100], Train Accuracy: 50.286 %, Loss: 1.4353\n",
            "Epoch [15/100], Learning Rate: 0.000996451615591515\n",
            "Epoch [16/100], Train Accuracy: 50.378 %, Loss: 1.3747\n",
            "Epoch [16/100], Learning Rate: 0.000994459753267812\n",
            "Epoch [17/100], Train Accuracy: 50.9 %, Loss: 1.3854\n",
            "Epoch [17/100], Learning Rate: 0.00099202926282791\n",
            "Epoch [18/100], Train Accuracy: 51.286 %, Loss: 1.3195\n",
            "Epoch [18/100], Learning Rate: 0.0009891625428724363\n",
            "Epoch [19/100], Train Accuracy: 52.058 %, Loss: 1.4204\n",
            "Epoch [19/100], Learning Rate: 0.000985862422507884\n",
            "Epoch [20/100], Train Accuracy: 52.25 %, Loss: 1.3300\n",
            "Epoch [20/100], Learning Rate: 0.0009821321585546244\n",
            "Epoch [21/100], Train Accuracy: 52.81 %, Loss: 1.2391\n",
            "Epoch [21/100], Learning Rate: 0.0009779754323328191\n",
            "Epoch [22/100], Train Accuracy: 52.92 %, Loss: 1.4511\n",
            "Epoch [22/100], Learning Rate: 0.0009733963460294015\n",
            "Epoch [23/100], Train Accuracy: 53.476 %, Loss: 1.4289\n",
            "Epoch [23/100], Learning Rate: 0.0009683994186497131\n",
            "Epoch [24/100], Train Accuracy: 54.136 %, Loss: 1.3020\n",
            "Epoch [24/100], Learning Rate: 0.0009629895815577914\n",
            "Epoch [25/100], Train Accuracy: 54.188 %, Loss: 1.2879\n",
            "Epoch [25/100], Learning Rate: 0.0009571721736097088\n",
            "Epoch [26/100], Train Accuracy: 53.774 %, Loss: 1.2418\n",
            "Epoch [26/100], Learning Rate: 0.0009509529358847655\n",
            "Epoch [27/100], Train Accuracy: 53.954 %, Loss: 1.2354\n",
            "Epoch [27/100], Learning Rate: 0.0009443380060197386\n",
            "Epoch [28/100], Train Accuracy: 53.768 %, Loss: 1.3251\n",
            "Epoch [28/100], Learning Rate: 0.0009373339121517746\n",
            "Epoch [29/100], Train Accuracy: 54.624 %, Loss: 1.2539\n",
            "Epoch [29/100], Learning Rate: 0.0009299475664759069\n",
            "Epoch [30/100], Train Accuracy: 54.5 %, Loss: 1.1505\n",
            "Epoch [30/100], Learning Rate: 0.0009221862584235528\n",
            "Epoch [31/100], Train Accuracy: 54.176 %, Loss: 1.3648\n",
            "Epoch [31/100], Learning Rate: 0.0009140576474687263\n",
            "Epoch [32/100], Train Accuracy: 54.852 %, Loss: 1.3175\n",
            "Epoch [32/100], Learning Rate: 0.0009055697555690606\n",
            "Epoch [33/100], Train Accuracy: 55.432 %, Loss: 1.3499\n",
            "Epoch [33/100], Learning Rate: 0.0008967309592491052\n",
            "Epoch [34/100], Train Accuracy: 55.834 %, Loss: 1.1584\n",
            "Epoch [34/100], Learning Rate: 0.0008875499813337069\n",
            "Epoch [35/100], Train Accuracy: 55.508 %, Loss: 1.1921\n",
            "Epoch [35/100], Learning Rate: 0.0008780358823396353\n",
            "Epoch [36/100], Train Accuracy: 55.678 %, Loss: 1.2285\n",
            "Epoch [36/100], Learning Rate: 0.0008681980515339464\n",
            "Epoch [37/100], Train Accuracy: 56.01 %, Loss: 1.1816\n",
            "Epoch [37/100], Learning Rate: 0.0008580461976679099\n",
            "Epoch [38/100], Train Accuracy: 56.318 %, Loss: 1.2664\n",
            "Epoch [38/100], Learning Rate: 0.0008475903393956434\n",
            "Epoch [39/100], Train Accuracy: 56.362 %, Loss: 1.1133\n",
            "Epoch [39/100], Learning Rate: 0.0008368407953869105\n",
            "Epoch [40/100], Train Accuracy: 56.746 %, Loss: 1.2011\n",
            "Epoch [40/100], Learning Rate: 0.0008258081741438395\n",
            "Epoch [41/100], Train Accuracy: 56.596 %, Loss: 1.1771\n",
            "Epoch [41/100], Learning Rate: 0.0008145033635316131\n",
            "Epoch [42/100], Train Accuracy: 57.304 %, Loss: 1.1046\n",
            "Epoch [42/100], Learning Rate: 0.0008029375200334589\n",
            "Epoch [43/100], Train Accuracy: 57.506 %, Loss: 1.1529\n",
            "Epoch [43/100], Learning Rate: 0.0007911220577405485\n",
            "Epoch [44/100], Train Accuracy: 57.87 %, Loss: 1.2075\n",
            "Epoch [44/100], Learning Rate: 0.0007790686370876671\n",
            "Epoch [45/100], Train Accuracy: 58.596 %, Loss: 1.1514\n",
            "Epoch [45/100], Learning Rate: 0.0007667891533457719\n",
            "Epoch [46/100], Train Accuracy: 58.314 %, Loss: 1.1885\n",
            "Epoch [46/100], Learning Rate: 0.000754295724882796\n",
            "Epoch [47/100], Train Accuracy: 57.992 %, Loss: 1.1593\n",
            "Epoch [47/100], Learning Rate: 0.0007416006812042827\n",
            "Epoch [48/100], Train Accuracy: 57.982 %, Loss: 1.2445\n",
            "Epoch [48/100], Learning Rate: 0.0007287165507856513\n",
            "Epoch [49/100], Train Accuracy: 58.422 %, Loss: 1.1281\n",
            "Epoch [49/100], Learning Rate: 0.0007156560487081052\n",
            "Epoch [50/100], Train Accuracy: 58.632 %, Loss: 1.0990\n",
            "Epoch [50/100], Learning Rate: 0.0007024320641103813\n",
            "Epoch [51/100], Train Accuracy: 58.89 %, Loss: 1.2378\n",
            "Epoch [51/100], Learning Rate: 0.0006890576474687264\n",
            "Epoch [52/100], Train Accuracy: 58.768 %, Loss: 1.1562\n",
            "Epoch [52/100], Learning Rate: 0.0006755459977176532\n",
            "Epoch [53/100], Train Accuracy: 58.786 %, Loss: 1.0898\n",
            "Epoch [53/100], Learning Rate: 0.0006619104492241846\n",
            "Epoch [54/100], Train Accuracy: 58.982 %, Loss: 1.0977\n",
            "Epoch [54/100], Learning Rate: 0.0006481644586284443\n",
            "Epoch [55/100], Train Accuracy: 59.038 %, Loss: 1.2039\n",
            "Epoch [55/100], Learning Rate: 0.0006343215915635762\n",
            "Epoch [56/100], Train Accuracy: 59.056 %, Loss: 1.1807\n",
            "Epoch [56/100], Learning Rate: 0.000620395509268104\n",
            "Epoch [57/100], Train Accuracy: 59.942 %, Loss: 1.0575\n",
            "Epoch [57/100], Learning Rate: 0.000606399955103937\n",
            "Epoch [58/100], Train Accuracy: 59.904 %, Loss: 1.1225\n",
            "Epoch [58/100], Learning Rate: 0.0005923487409933315\n",
            "Epoch [59/100], Train Accuracy: 59.744 %, Loss: 1.1362\n",
            "Epoch [59/100], Learning Rate: 0.0005782557337881911\n",
            "Epoch [60/100], Train Accuracy: 59.828 %, Loss: 1.2096\n",
            "Epoch [60/100], Learning Rate: 0.0005641348415851578\n",
            "Epoch [61/100], Train Accuracy: 60.238 %, Loss: 1.1327\n",
            "Epoch [61/100], Learning Rate: 0.00055\n",
            "Epoch [62/100], Train Accuracy: 59.618 %, Loss: 1.2187\n",
            "Epoch [62/100], Learning Rate: 0.0005358651584148423\n",
            "Epoch [63/100], Train Accuracy: 59.724 %, Loss: 1.1334\n",
            "Epoch [63/100], Learning Rate: 0.000521744266211809\n",
            "Epoch [64/100], Train Accuracy: 60.204 %, Loss: 1.1403\n",
            "Epoch [64/100], Learning Rate: 0.0005076512590066686\n",
            "Epoch [65/100], Train Accuracy: 60.346 %, Loss: 1.1566\n",
            "Epoch [65/100], Learning Rate: 0.0004936000448960632\n",
            "Epoch [66/100], Train Accuracy: 60.41 %, Loss: 1.1290\n",
            "Epoch [66/100], Learning Rate: 0.0004796044907318961\n",
            "Epoch [67/100], Train Accuracy: 60.17 %, Loss: 1.1431\n",
            "Epoch [67/100], Learning Rate: 0.0004656784084364239\n",
            "Epoch [68/100], Train Accuracy: 60.612 %, Loss: 1.1531\n",
            "Epoch [68/100], Learning Rate: 0.00045183554137155597\n",
            "Epoch [69/100], Train Accuracy: 61.01 %, Loss: 1.0307\n",
            "Epoch [69/100], Learning Rate: 0.0004380895507758153\n",
            "Epoch [70/100], Train Accuracy: 60.74 %, Loss: 1.1338\n",
            "Epoch [70/100], Learning Rate: 0.00042445400228234684\n",
            "Epoch [71/100], Train Accuracy: 60.7 %, Loss: 1.1321\n",
            "Epoch [71/100], Learning Rate: 0.0004109423525312738\n",
            "Epoch [72/100], Train Accuracy: 60.656 %, Loss: 1.1197\n",
            "Epoch [72/100], Learning Rate: 0.0003975679358896189\n",
            "Epoch [73/100], Train Accuracy: 60.968 %, Loss: 1.1118\n",
            "Epoch [73/100], Learning Rate: 0.000384343951291895\n",
            "Epoch [74/100], Train Accuracy: 61.138 %, Loss: 1.2090\n",
            "Epoch [74/100], Learning Rate: 0.00037128344921434864\n",
            "Epoch [75/100], Train Accuracy: 61.338 %, Loss: 1.0894\n",
            "Epoch [75/100], Learning Rate: 0.0003583993187957173\n",
            "Epoch [76/100], Train Accuracy: 61.096 %, Loss: 0.9371\n",
            "Epoch [76/100], Learning Rate: 0.00034570427511720393\n",
            "Epoch [77/100], Train Accuracy: 60.752 %, Loss: 1.0576\n",
            "Epoch [77/100], Learning Rate: 0.0003332108466542281\n",
            "Epoch [78/100], Train Accuracy: 61.018 %, Loss: 1.1184\n",
            "Epoch [78/100], Learning Rate: 0.0003209313629123329\n",
            "Epoch [79/100], Train Accuracy: 61.246 %, Loss: 1.0979\n",
            "Epoch [79/100], Learning Rate: 0.0003088779422594514\n",
            "Epoch [80/100], Train Accuracy: 61.17 %, Loss: 1.0401\n",
            "Epoch [80/100], Learning Rate: 0.0002970624799665412\n",
            "Epoch [81/100], Train Accuracy: 61.424 %, Loss: 1.0806\n",
            "Epoch [81/100], Learning Rate: 0.00028549663646838715\n",
            "Epoch [82/100], Train Accuracy: 61.548 %, Loss: 1.0815\n",
            "Epoch [82/100], Learning Rate: 0.00027419182585616055\n",
            "Epoch [83/100], Train Accuracy: 61.49 %, Loss: 1.1192\n",
            "Epoch [83/100], Learning Rate: 0.0002631592046130896\n",
            "Epoch [84/100], Train Accuracy: 61.834 %, Loss: 1.0828\n",
            "Epoch [84/100], Learning Rate: 0.00025240966060435674\n",
            "Epoch [85/100], Train Accuracy: 62.124 %, Loss: 1.1511\n",
            "Epoch [85/100], Learning Rate: 0.00024195380233209008\n",
            "Epoch [86/100], Train Accuracy: 62.414 %, Loss: 1.0149\n",
            "Epoch [86/100], Learning Rate: 0.00023180194846605365\n",
            "Epoch [87/100], Train Accuracy: 62.012 %, Loss: 1.0821\n",
            "Epoch [87/100], Learning Rate: 0.0002219641176603649\n",
            "Epoch [88/100], Train Accuracy: 61.876 %, Loss: 1.0804\n",
            "Epoch [88/100], Learning Rate: 0.0002124500186662932\n",
            "Epoch [89/100], Train Accuracy: 62.096 %, Loss: 1.0819\n",
            "Epoch [89/100], Learning Rate: 0.00020326904075089488\n",
            "Epoch [90/100], Train Accuracy: 61.708 %, Loss: 1.1340\n",
            "Epoch [90/100], Learning Rate: 0.00019443024443093932\n",
            "Epoch [91/100], Train Accuracy: 61.93 %, Loss: 1.1460\n",
            "Epoch [91/100], Learning Rate: 0.00018594235253127368\n",
            "Epoch [92/100], Train Accuracy: 62.176 %, Loss: 1.0288\n",
            "Epoch [92/100], Learning Rate: 0.00017781374157644722\n",
            "Epoch [93/100], Train Accuracy: 62.352 %, Loss: 1.0699\n",
            "Epoch [93/100], Learning Rate: 0.00017005243352409332\n",
            "Epoch [94/100], Train Accuracy: 62.248 %, Loss: 1.0703\n",
            "Epoch [94/100], Learning Rate: 0.0001626660878482253\n",
            "Epoch [95/100], Train Accuracy: 62.57 %, Loss: 1.0974\n",
            "Epoch [95/100], Learning Rate: 0.0001556619939802614\n",
            "Epoch [96/100], Train Accuracy: 62.552 %, Loss: 1.0597\n",
            "Epoch [96/100], Learning Rate: 0.0001490470641152345\n",
            "Epoch [97/100], Train Accuracy: 62.492 %, Loss: 1.0781\n",
            "Epoch [97/100], Learning Rate: 0.0001428278263902913\n",
            "Epoch [98/100], Train Accuracy: 62.922 %, Loss: 1.0403\n",
            "Epoch [98/100], Learning Rate: 0.00013701041844220858\n",
            "Epoch [99/100], Train Accuracy: 62.544 %, Loss: 1.1566\n",
            "Epoch [99/100], Learning Rate: 0.0001316005813502869\n",
            "Epoch [100/100], Train Accuracy: 62.62 %, Loss: 1.1670\n",
            "Epoch [100/100], Learning Rate: 0.00012660365397059855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習前後のモデルの重みを比較する関数を作成\n",
        "def check_weight_change(model, model_init):\n",
        "    weights_changed = (model.conv1.state_dict()['weight'] != model_init.conv1.state_dict()['weight']).any()\n",
        "    if weights_changed:\n",
        "        return print('モデルの重みが変化しています')\n",
        "    else:\n",
        "        return print('モデルの重みは変化していません')\n",
        "# 学習前後でモデルの重みが変化していないかを確認\n",
        "check_weight_change(model, model_init)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5446daf5-ee7e-46ab-97da-174496da426e",
        "id": "7OqoxvPokU_F"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "モデルの重みは変化していません\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの評価\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for images, labels in test_loader_cifar10:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度の確認\n",
        "    print('Test Accuracy: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53e6848a-d476-4ab8-a290-15ecd7d30aa4",
        "id": "55ToVK03kU_F"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 51.88 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUメモリの解放\n",
        "del model, model_init\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "XrV7kTINkU_F"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prune_rate=0.95"
      ],
      "metadata": {
        "id": "wVweRGZbj9_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 刈り込み率を設定\n",
        "prune_rate = 0.95\n",
        "# 初期化手法を設定\n",
        "init = 'kaiming_uniform'\n",
        "\n",
        "# デバイスを設定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# モデルを定義\n",
        "model = supermaskresnet18_10().to(device)\n",
        "# 学習前のモデルの重みを保存\n",
        "model_init = copy.deepcopy(model)\n",
        "\n",
        "# 学習率を設定\n",
        "learning_rate = 0.001\n",
        "# 全体のepoch数を設定\n",
        "num_epochs = 100\n",
        "# warm-upするepoch数を設定\n",
        "warmup_epochs = 10\n",
        "\n",
        "# 損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# オプティマイザーを設定\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# スケジューラーを設定\n",
        "scheduler = CosineLRScheduler(optimizer, t_initial=num_epochs, lr_min=1e-4, warmup_t=warmup_epochs, warmup_lr_init=1e-4, warmup_prefix=True)\n",
        "\n",
        "# モデルの学習\n",
        "for epoch in range(num_epochs):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, (images, labels) in enumerate(train_loader_cifar10):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # 順伝播\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # 逆伝播\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # 全データ数\n",
        "        total += labels.size(0)\n",
        "        # 正解数\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度を計算\n",
        "    train_acc = 100 * correct / total\n",
        "    # 正解率精度と損失を確認\n",
        "    print(\"Epoch [{}/{}], Train Accuracy: {} %, Loss: {:.4f}\".format(epoch+1, num_epochs, 100 * correct / total, loss.item()))\n",
        "    # 1エポック終了後にスケジューラーを更新\n",
        "    scheduler.step(epoch)\n",
        "    # 学習率の確認\n",
        "    print(\"Epoch [{}/{}], Learning Rate: {}\".format(epoch+1, num_epochs, optimizer.param_groups[0]['lr']))\n",
        "\n",
        "# モデルを保存\n",
        "torch.save(model, '/content/drive/MyDrive/SupermaskResNet18_CIFAR10_100epochs_pr0.95_uniform_CLRS_restest.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e412d14-4888-4393-823a-4fc599235815",
        "id": "78xA6M_6j9_V"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Accuracy: 34.418 %, Loss: 1.5912\n",
            "Epoch [1/100], Learning Rate: 0.0001\n",
            "Epoch [2/100], Train Accuracy: 44.486 %, Loss: 1.5065\n",
            "Epoch [2/100], Learning Rate: 0.00019\n",
            "Epoch [3/100], Train Accuracy: 45.966 %, Loss: 1.4093\n",
            "Epoch [3/100], Learning Rate: 0.00028\n",
            "Epoch [4/100], Train Accuracy: 49.29 %, Loss: 1.3149\n",
            "Epoch [4/100], Learning Rate: 0.00036999999999999994\n",
            "Epoch [5/100], Train Accuracy: 51.006 %, Loss: 1.4019\n",
            "Epoch [5/100], Learning Rate: 0.00045999999999999996\n",
            "Epoch [6/100], Train Accuracy: 52.542 %, Loss: 1.3014\n",
            "Epoch [6/100], Learning Rate: 0.00055\n",
            "Epoch [7/100], Train Accuracy: 54.28 %, Loss: 1.3606\n",
            "Epoch [7/100], Learning Rate: 0.0006399999999999999\n",
            "Epoch [8/100], Train Accuracy: 55.398 %, Loss: 1.2748\n",
            "Epoch [8/100], Learning Rate: 0.00073\n",
            "Epoch [9/100], Train Accuracy: 57.298 %, Loss: 1.1963\n",
            "Epoch [9/100], Learning Rate: 0.00082\n",
            "Epoch [10/100], Train Accuracy: 58.302 %, Loss: 1.1961\n",
            "Epoch [10/100], Learning Rate: 0.00091\n",
            "Epoch [11/100], Train Accuracy: 59.514 %, Loss: 1.0571\n",
            "Epoch [11/100], Learning Rate: 0.001\n",
            "Epoch [12/100], Train Accuracy: 59.48 %, Loss: 1.1654\n",
            "Epoch [12/100], Learning Rate: 0.0009997779521645793\n",
            "Epoch [13/100], Train Accuracy: 60.614 %, Loss: 1.0944\n",
            "Epoch [13/100], Learning Rate: 0.0009991120277927223\n",
            "Epoch [14/100], Train Accuracy: 61.936 %, Loss: 1.0639\n",
            "Epoch [14/100], Learning Rate: 0.000998002884071386\n",
            "Epoch [15/100], Train Accuracy: 63.04 %, Loss: 0.9213\n",
            "Epoch [15/100], Learning Rate: 0.000996451615591515\n",
            "Epoch [16/100], Train Accuracy: 63.862 %, Loss: 0.9296\n",
            "Epoch [16/100], Learning Rate: 0.000994459753267812\n",
            "Epoch [17/100], Train Accuracy: 64.492 %, Loss: 0.9527\n",
            "Epoch [17/100], Learning Rate: 0.00099202926282791\n",
            "Epoch [18/100], Train Accuracy: 65.098 %, Loss: 0.9354\n",
            "Epoch [18/100], Learning Rate: 0.0009891625428724363\n",
            "Epoch [19/100], Train Accuracy: 65.242 %, Loss: 0.9362\n",
            "Epoch [19/100], Learning Rate: 0.000985862422507884\n",
            "Epoch [20/100], Train Accuracy: 66.036 %, Loss: 0.8437\n",
            "Epoch [20/100], Learning Rate: 0.0009821321585546244\n",
            "Epoch [21/100], Train Accuracy: 67.03 %, Loss: 0.9187\n",
            "Epoch [21/100], Learning Rate: 0.0009779754323328191\n",
            "Epoch [22/100], Train Accuracy: 67.308 %, Loss: 0.9385\n",
            "Epoch [22/100], Learning Rate: 0.0009733963460294015\n",
            "Epoch [23/100], Train Accuracy: 67.438 %, Loss: 1.0023\n",
            "Epoch [23/100], Learning Rate: 0.0009683994186497131\n",
            "Epoch [24/100], Train Accuracy: 68.226 %, Loss: 0.9238\n",
            "Epoch [24/100], Learning Rate: 0.0009629895815577914\n",
            "Epoch [25/100], Train Accuracy: 68.166 %, Loss: 0.8573\n",
            "Epoch [25/100], Learning Rate: 0.0009571721736097088\n",
            "Epoch [26/100], Train Accuracy: 67.936 %, Loss: 1.0473\n",
            "Epoch [26/100], Learning Rate: 0.0009509529358847655\n",
            "Epoch [27/100], Train Accuracy: 68.838 %, Loss: 0.9441\n",
            "Epoch [27/100], Learning Rate: 0.0009443380060197386\n",
            "Epoch [28/100], Train Accuracy: 69.002 %, Loss: 1.0181\n",
            "Epoch [28/100], Learning Rate: 0.0009373339121517746\n",
            "Epoch [29/100], Train Accuracy: 69.17 %, Loss: 0.9101\n",
            "Epoch [29/100], Learning Rate: 0.0009299475664759069\n",
            "Epoch [30/100], Train Accuracy: 69.462 %, Loss: 0.8440\n",
            "Epoch [30/100], Learning Rate: 0.0009221862584235528\n",
            "Epoch [31/100], Train Accuracy: 69.424 %, Loss: 0.8968\n",
            "Epoch [31/100], Learning Rate: 0.0009140576474687263\n",
            "Epoch [32/100], Train Accuracy: 69.946 %, Loss: 0.9323\n",
            "Epoch [32/100], Learning Rate: 0.0009055697555690606\n",
            "Epoch [33/100], Train Accuracy: 70.13 %, Loss: 0.8919\n",
            "Epoch [33/100], Learning Rate: 0.0008967309592491052\n",
            "Epoch [34/100], Train Accuracy: 70.448 %, Loss: 0.7382\n",
            "Epoch [34/100], Learning Rate: 0.0008875499813337069\n",
            "Epoch [35/100], Train Accuracy: 70.704 %, Loss: 0.8326\n",
            "Epoch [35/100], Learning Rate: 0.0008780358823396353\n",
            "Epoch [36/100], Train Accuracy: 71.038 %, Loss: 0.7726\n",
            "Epoch [36/100], Learning Rate: 0.0008681980515339464\n",
            "Epoch [37/100], Train Accuracy: 71.516 %, Loss: 0.7802\n",
            "Epoch [37/100], Learning Rate: 0.0008580461976679099\n",
            "Epoch [38/100], Train Accuracy: 71.428 %, Loss: 0.7912\n",
            "Epoch [38/100], Learning Rate: 0.0008475903393956434\n",
            "Epoch [39/100], Train Accuracy: 71.518 %, Loss: 0.8724\n",
            "Epoch [39/100], Learning Rate: 0.0008368407953869105\n",
            "Epoch [40/100], Train Accuracy: 71.85 %, Loss: 0.8264\n",
            "Epoch [40/100], Learning Rate: 0.0008258081741438395\n",
            "Epoch [41/100], Train Accuracy: 72.006 %, Loss: 0.8595\n",
            "Epoch [41/100], Learning Rate: 0.0008145033635316131\n",
            "Epoch [42/100], Train Accuracy: 72.178 %, Loss: 0.7086\n",
            "Epoch [42/100], Learning Rate: 0.0008029375200334589\n",
            "Epoch [43/100], Train Accuracy: 72.67 %, Loss: 0.7338\n",
            "Epoch [43/100], Learning Rate: 0.0007911220577405485\n",
            "Epoch [44/100], Train Accuracy: 72.574 %, Loss: 0.7887\n",
            "Epoch [44/100], Learning Rate: 0.0007790686370876671\n",
            "Epoch [45/100], Train Accuracy: 72.414 %, Loss: 0.8424\n",
            "Epoch [45/100], Learning Rate: 0.0007667891533457719\n",
            "Epoch [46/100], Train Accuracy: 72.934 %, Loss: 0.8234\n",
            "Epoch [46/100], Learning Rate: 0.000754295724882796\n",
            "Epoch [47/100], Train Accuracy: 73.014 %, Loss: 0.7631\n",
            "Epoch [47/100], Learning Rate: 0.0007416006812042827\n",
            "Epoch [48/100], Train Accuracy: 73.316 %, Loss: 0.8466\n",
            "Epoch [48/100], Learning Rate: 0.0007287165507856513\n",
            "Epoch [49/100], Train Accuracy: 72.966 %, Loss: 0.9758\n",
            "Epoch [49/100], Learning Rate: 0.0007156560487081052\n",
            "Epoch [50/100], Train Accuracy: 73.22 %, Loss: 0.7652\n",
            "Epoch [50/100], Learning Rate: 0.0007024320641103813\n",
            "Epoch [51/100], Train Accuracy: 73.468 %, Loss: 0.8561\n",
            "Epoch [51/100], Learning Rate: 0.0006890576474687264\n",
            "Epoch [52/100], Train Accuracy: 73.8 %, Loss: 0.7700\n",
            "Epoch [52/100], Learning Rate: 0.0006755459977176532\n",
            "Epoch [53/100], Train Accuracy: 74.058 %, Loss: 0.7975\n",
            "Epoch [53/100], Learning Rate: 0.0006619104492241846\n",
            "Epoch [54/100], Train Accuracy: 74.114 %, Loss: 0.7261\n",
            "Epoch [54/100], Learning Rate: 0.0006481644586284443\n",
            "Epoch [55/100], Train Accuracy: 74.374 %, Loss: 0.7461\n",
            "Epoch [55/100], Learning Rate: 0.0006343215915635762\n",
            "Epoch [56/100], Train Accuracy: 74.148 %, Loss: 0.7634\n",
            "Epoch [56/100], Learning Rate: 0.000620395509268104\n",
            "Epoch [57/100], Train Accuracy: 74.364 %, Loss: 0.6932\n",
            "Epoch [57/100], Learning Rate: 0.000606399955103937\n",
            "Epoch [58/100], Train Accuracy: 74.43 %, Loss: 0.6705\n",
            "Epoch [58/100], Learning Rate: 0.0005923487409933315\n",
            "Epoch [59/100], Train Accuracy: 74.792 %, Loss: 0.6564\n",
            "Epoch [59/100], Learning Rate: 0.0005782557337881911\n",
            "Epoch [60/100], Train Accuracy: 74.738 %, Loss: 0.6780\n",
            "Epoch [60/100], Learning Rate: 0.0005641348415851578\n",
            "Epoch [61/100], Train Accuracy: 74.71 %, Loss: 0.7629\n",
            "Epoch [61/100], Learning Rate: 0.00055\n",
            "Epoch [62/100], Train Accuracy: 75.244 %, Loss: 0.6744\n",
            "Epoch [62/100], Learning Rate: 0.0005358651584148423\n",
            "Epoch [63/100], Train Accuracy: 75.014 %, Loss: 0.7156\n",
            "Epoch [63/100], Learning Rate: 0.000521744266211809\n",
            "Epoch [64/100], Train Accuracy: 75.49 %, Loss: 0.7796\n",
            "Epoch [64/100], Learning Rate: 0.0005076512590066686\n",
            "Epoch [65/100], Train Accuracy: 75.342 %, Loss: 0.6835\n",
            "Epoch [65/100], Learning Rate: 0.0004936000448960632\n",
            "Epoch [66/100], Train Accuracy: 75.5 %, Loss: 0.6249\n",
            "Epoch [66/100], Learning Rate: 0.0004796044907318961\n",
            "Epoch [67/100], Train Accuracy: 75.428 %, Loss: 0.6974\n",
            "Epoch [67/100], Learning Rate: 0.0004656784084364239\n",
            "Epoch [68/100], Train Accuracy: 76.074 %, Loss: 0.7151\n",
            "Epoch [68/100], Learning Rate: 0.00045183554137155597\n",
            "Epoch [69/100], Train Accuracy: 75.906 %, Loss: 0.6616\n",
            "Epoch [69/100], Learning Rate: 0.0004380895507758153\n",
            "Epoch [70/100], Train Accuracy: 76.044 %, Loss: 0.6549\n",
            "Epoch [70/100], Learning Rate: 0.00042445400228234684\n",
            "Epoch [71/100], Train Accuracy: 76.26 %, Loss: 0.6904\n",
            "Epoch [71/100], Learning Rate: 0.0004109423525312738\n",
            "Epoch [72/100], Train Accuracy: 76.262 %, Loss: 0.6738\n",
            "Epoch [72/100], Learning Rate: 0.0003975679358896189\n",
            "Epoch [73/100], Train Accuracy: 76.278 %, Loss: 0.6562\n",
            "Epoch [73/100], Learning Rate: 0.000384343951291895\n",
            "Epoch [74/100], Train Accuracy: 76.898 %, Loss: 0.7286\n",
            "Epoch [74/100], Learning Rate: 0.00037128344921434864\n",
            "Epoch [75/100], Train Accuracy: 76.734 %, Loss: 0.7946\n",
            "Epoch [75/100], Learning Rate: 0.0003583993187957173\n",
            "Epoch [76/100], Train Accuracy: 76.938 %, Loss: 0.5912\n",
            "Epoch [76/100], Learning Rate: 0.00034570427511720393\n",
            "Epoch [77/100], Train Accuracy: 77.38 %, Loss: 0.7532\n",
            "Epoch [77/100], Learning Rate: 0.0003332108466542281\n",
            "Epoch [78/100], Train Accuracy: 76.896 %, Loss: 0.6770\n",
            "Epoch [78/100], Learning Rate: 0.0003209313629123329\n",
            "Epoch [79/100], Train Accuracy: 77.444 %, Loss: 0.6192\n",
            "Epoch [79/100], Learning Rate: 0.0003088779422594514\n",
            "Epoch [80/100], Train Accuracy: 77.094 %, Loss: 0.6621\n",
            "Epoch [80/100], Learning Rate: 0.0002970624799665412\n",
            "Epoch [81/100], Train Accuracy: 77.256 %, Loss: 0.6271\n",
            "Epoch [81/100], Learning Rate: 0.00028549663646838715\n",
            "Epoch [82/100], Train Accuracy: 77.798 %, Loss: 0.6744\n",
            "Epoch [82/100], Learning Rate: 0.00027419182585616055\n",
            "Epoch [83/100], Train Accuracy: 77.852 %, Loss: 0.6905\n",
            "Epoch [83/100], Learning Rate: 0.0002631592046130896\n",
            "Epoch [84/100], Train Accuracy: 77.786 %, Loss: 0.6788\n",
            "Epoch [84/100], Learning Rate: 0.00025240966060435674\n",
            "Epoch [85/100], Train Accuracy: 78.468 %, Loss: 0.5814\n",
            "Epoch [85/100], Learning Rate: 0.00024195380233209008\n",
            "Epoch [86/100], Train Accuracy: 78.372 %, Loss: 0.6233\n",
            "Epoch [86/100], Learning Rate: 0.00023180194846605365\n",
            "Epoch [87/100], Train Accuracy: 78.46 %, Loss: 0.6502\n",
            "Epoch [87/100], Learning Rate: 0.0002219641176603649\n",
            "Epoch [88/100], Train Accuracy: 78.586 %, Loss: 0.6240\n",
            "Epoch [88/100], Learning Rate: 0.0002124500186662932\n",
            "Epoch [89/100], Train Accuracy: 79.06 %, Loss: 0.6139\n",
            "Epoch [89/100], Learning Rate: 0.00020326904075089488\n",
            "Epoch [90/100], Train Accuracy: 78.902 %, Loss: 0.6738\n",
            "Epoch [90/100], Learning Rate: 0.00019443024443093932\n",
            "Epoch [91/100], Train Accuracy: 78.81 %, Loss: 0.6487\n",
            "Epoch [91/100], Learning Rate: 0.00018594235253127368\n",
            "Epoch [92/100], Train Accuracy: 78.632 %, Loss: 0.6045\n",
            "Epoch [92/100], Learning Rate: 0.00017781374157644722\n",
            "Epoch [93/100], Train Accuracy: 78.898 %, Loss: 0.5726\n",
            "Epoch [93/100], Learning Rate: 0.00017005243352409332\n",
            "Epoch [94/100], Train Accuracy: 78.98 %, Loss: 0.6619\n",
            "Epoch [94/100], Learning Rate: 0.0001626660878482253\n",
            "Epoch [95/100], Train Accuracy: 79.302 %, Loss: 0.5489\n",
            "Epoch [95/100], Learning Rate: 0.0001556619939802614\n",
            "Epoch [96/100], Train Accuracy: 79.566 %, Loss: 0.5364\n",
            "Epoch [96/100], Learning Rate: 0.0001490470641152345\n",
            "Epoch [97/100], Train Accuracy: 79.662 %, Loss: 0.5851\n",
            "Epoch [97/100], Learning Rate: 0.0001428278263902913\n",
            "Epoch [98/100], Train Accuracy: 79.94 %, Loss: 0.4897\n",
            "Epoch [98/100], Learning Rate: 0.00013701041844220858\n",
            "Epoch [99/100], Train Accuracy: 79.95 %, Loss: 0.5271\n",
            "Epoch [99/100], Learning Rate: 0.0001316005813502869\n",
            "Epoch [100/100], Train Accuracy: 80.016 %, Loss: 0.5681\n",
            "Epoch [100/100], Learning Rate: 0.00012660365397059855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習前後のモデルの重みを比較する関数を作成\n",
        "def check_weight_change(model, model_init):\n",
        "    weights_changed = (model.conv1.state_dict()['weight'] != model_init.conv1.state_dict()['weight']).any()\n",
        "    if weights_changed:\n",
        "        return print('モデルの重みが変化しています')\n",
        "    else:\n",
        "        return print('モデルの重みは変化していません')\n",
        "# 学習前後でモデルの重みが変化していないかを確認\n",
        "check_weight_change(model, model_init)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0834045c-01cd-478b-9fd9-db63ff501500",
        "id": "8pEB0ikKj9_V"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "モデルの重みは変化していません\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの評価\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for images, labels in test_loader_cifar10:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度の確認\n",
        "    print('Test Accuracy: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca506120-ea33-433a-b89d-dc008527cacc",
        "id": "BBg7NcEQj9_W"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 72.52 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUメモリの解放\n",
        "del model, model_init\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "qX2Jy6Skj9_W"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prune_rate=0.9"
      ],
      "metadata": {
        "id": "jQuP9L3Bxccy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 刈り込み率を設定\n",
        "prune_rate = 0.9\n",
        "# 初期化手法を設定\n",
        "init = 'kaiming_uniform'\n",
        "\n",
        "# デバイスを設定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# モデルを定義\n",
        "model = supermaskresnet18_10().to(device)\n",
        "# 学習前のモデルの重みを保存\n",
        "model_init = copy.deepcopy(model)\n",
        "\n",
        "# 学習率を設定\n",
        "learning_rate = 0.001\n",
        "# 全体のepoch数を設定\n",
        "num_epochs = 100\n",
        "# warm-upするepoch数を設定\n",
        "warmup_epochs = 10\n",
        "\n",
        "# 損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# オプティマイザーを設定\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# スケジューラーを設定\n",
        "scheduler = CosineLRScheduler(optimizer, t_initial=num_epochs, lr_min=1e-4, warmup_t=warmup_epochs, warmup_lr_init=1e-4, warmup_prefix=True)\n",
        "\n",
        "# モデルの学習\n",
        "for epoch in range(num_epochs):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, (images, labels) in enumerate(train_loader_cifar10):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # 順伝播\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # 逆伝播\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # 全データ数\n",
        "        total += labels.size(0)\n",
        "        # 正解数\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度を計算\n",
        "    train_acc = 100 * correct / total\n",
        "    # 正解率精度と損失を確認\n",
        "    print(\"Epoch [{}/{}], Train Accuracy: {} %, Loss: {:.4f}\".format(epoch+1, num_epochs, 100 * correct / total, loss.item()))\n",
        "    # 1エポック終了後にスケジューラーを更新\n",
        "    scheduler.step(epoch)\n",
        "    # 学習率の確認\n",
        "    print(\"Epoch [{}/{}], Learning Rate: {}\".format(epoch+1, num_epochs, optimizer.param_groups[0]['lr']))\n",
        "\n",
        "# モデルを保存\n",
        "torch.save(model, '/content/drive/MyDrive/SupermaskResNet18_CIFAR10_100epochs_pr0.9_uniform_CLRS_restest.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dp0nMR7ixccz",
        "outputId": "e70b48ff-17f9-41e1-db8a-82659083987f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Accuracy: 35.506 %, Loss: 1.6719\n",
            "Epoch [1/100], Learning Rate: 0.0001\n",
            "Epoch [2/100], Train Accuracy: 45.318 %, Loss: 1.4340\n",
            "Epoch [2/100], Learning Rate: 0.00019\n",
            "Epoch [3/100], Train Accuracy: 48.392 %, Loss: 1.4159\n",
            "Epoch [3/100], Learning Rate: 0.00028\n",
            "Epoch [4/100], Train Accuracy: 50.986 %, Loss: 1.2204\n",
            "Epoch [4/100], Learning Rate: 0.00036999999999999994\n",
            "Epoch [5/100], Train Accuracy: 53.156 %, Loss: 1.2728\n",
            "Epoch [5/100], Learning Rate: 0.00045999999999999996\n",
            "Epoch [6/100], Train Accuracy: 55.538 %, Loss: 1.2876\n",
            "Epoch [6/100], Learning Rate: 0.00055\n",
            "Epoch [7/100], Train Accuracy: 57.672 %, Loss: 1.1049\n",
            "Epoch [7/100], Learning Rate: 0.0006399999999999999\n",
            "Epoch [8/100], Train Accuracy: 59.96 %, Loss: 1.1361\n",
            "Epoch [8/100], Learning Rate: 0.00073\n",
            "Epoch [9/100], Train Accuracy: 60.638 %, Loss: 1.1277\n",
            "Epoch [9/100], Learning Rate: 0.00082\n",
            "Epoch [10/100], Train Accuracy: 61.824 %, Loss: 1.0727\n",
            "Epoch [10/100], Learning Rate: 0.00091\n",
            "Epoch [11/100], Train Accuracy: 63.004 %, Loss: 0.9787\n",
            "Epoch [11/100], Learning Rate: 0.001\n",
            "Epoch [12/100], Train Accuracy: 64.17 %, Loss: 0.9890\n",
            "Epoch [12/100], Learning Rate: 0.0009997779521645793\n",
            "Epoch [13/100], Train Accuracy: 65.24 %, Loss: 0.9643\n",
            "Epoch [13/100], Learning Rate: 0.0009991120277927223\n",
            "Epoch [14/100], Train Accuracy: 66.228 %, Loss: 0.9096\n",
            "Epoch [14/100], Learning Rate: 0.000998002884071386\n",
            "Epoch [15/100], Train Accuracy: 67.122 %, Loss: 0.9597\n",
            "Epoch [15/100], Learning Rate: 0.000996451615591515\n",
            "Epoch [16/100], Train Accuracy: 68.316 %, Loss: 0.9683\n",
            "Epoch [16/100], Learning Rate: 0.000994459753267812\n",
            "Epoch [17/100], Train Accuracy: 68.614 %, Loss: 0.7891\n",
            "Epoch [17/100], Learning Rate: 0.00099202926282791\n",
            "Epoch [18/100], Train Accuracy: 69.432 %, Loss: 0.8269\n",
            "Epoch [18/100], Learning Rate: 0.0009891625428724363\n",
            "Epoch [19/100], Train Accuracy: 69.736 %, Loss: 0.8969\n",
            "Epoch [19/100], Learning Rate: 0.000985862422507884\n",
            "Epoch [20/100], Train Accuracy: 70.674 %, Loss: 0.8810\n",
            "Epoch [20/100], Learning Rate: 0.0009821321585546244\n",
            "Epoch [21/100], Train Accuracy: 71.052 %, Loss: 0.8065\n",
            "Epoch [21/100], Learning Rate: 0.0009779754323328191\n",
            "Epoch [22/100], Train Accuracy: 71.184 %, Loss: 0.7251\n",
            "Epoch [22/100], Learning Rate: 0.0009733963460294015\n",
            "Epoch [23/100], Train Accuracy: 71.434 %, Loss: 0.7559\n",
            "Epoch [23/100], Learning Rate: 0.0009683994186497131\n",
            "Epoch [24/100], Train Accuracy: 71.982 %, Loss: 0.9139\n",
            "Epoch [24/100], Learning Rate: 0.0009629895815577914\n",
            "Epoch [25/100], Train Accuracy: 72.078 %, Loss: 0.7780\n",
            "Epoch [25/100], Learning Rate: 0.0009571721736097088\n",
            "Epoch [26/100], Train Accuracy: 72.452 %, Loss: 0.7758\n",
            "Epoch [26/100], Learning Rate: 0.0009509529358847655\n",
            "Epoch [27/100], Train Accuracy: 72.7 %, Loss: 0.6945\n",
            "Epoch [27/100], Learning Rate: 0.0009443380060197386\n",
            "Epoch [28/100], Train Accuracy: 73.196 %, Loss: 0.7663\n",
            "Epoch [28/100], Learning Rate: 0.0009373339121517746\n",
            "Epoch [29/100], Train Accuracy: 73.256 %, Loss: 0.8233\n",
            "Epoch [29/100], Learning Rate: 0.0009299475664759069\n",
            "Epoch [30/100], Train Accuracy: 73.352 %, Loss: 0.8684\n",
            "Epoch [30/100], Learning Rate: 0.0009221862584235528\n",
            "Epoch [31/100], Train Accuracy: 73.668 %, Loss: 0.7215\n",
            "Epoch [31/100], Learning Rate: 0.0009140576474687263\n",
            "Epoch [32/100], Train Accuracy: 73.798 %, Loss: 0.7548\n",
            "Epoch [32/100], Learning Rate: 0.0009055697555690606\n",
            "Epoch [33/100], Train Accuracy: 74.244 %, Loss: 0.9310\n",
            "Epoch [33/100], Learning Rate: 0.0008967309592491052\n",
            "Epoch [34/100], Train Accuracy: 74.482 %, Loss: 0.7119\n",
            "Epoch [34/100], Learning Rate: 0.0008875499813337069\n",
            "Epoch [35/100], Train Accuracy: 74.424 %, Loss: 0.7644\n",
            "Epoch [35/100], Learning Rate: 0.0008780358823396353\n",
            "Epoch [36/100], Train Accuracy: 75.21 %, Loss: 0.6422\n",
            "Epoch [36/100], Learning Rate: 0.0008681980515339464\n",
            "Epoch [37/100], Train Accuracy: 75.388 %, Loss: 0.7187\n",
            "Epoch [37/100], Learning Rate: 0.0008580461976679099\n",
            "Epoch [38/100], Train Accuracy: 75.556 %, Loss: 0.6416\n",
            "Epoch [38/100], Learning Rate: 0.0008475903393956434\n",
            "Epoch [39/100], Train Accuracy: 75.69 %, Loss: 0.7010\n",
            "Epoch [39/100], Learning Rate: 0.0008368407953869105\n",
            "Epoch [40/100], Train Accuracy: 75.802 %, Loss: 0.7368\n",
            "Epoch [40/100], Learning Rate: 0.0008258081741438395\n",
            "Epoch [41/100], Train Accuracy: 76.078 %, Loss: 0.5956\n",
            "Epoch [41/100], Learning Rate: 0.0008145033635316131\n",
            "Epoch [42/100], Train Accuracy: 76.05 %, Loss: 0.6558\n",
            "Epoch [42/100], Learning Rate: 0.0008029375200334589\n",
            "Epoch [43/100], Train Accuracy: 76.462 %, Loss: 0.7665\n",
            "Epoch [43/100], Learning Rate: 0.0007911220577405485\n",
            "Epoch [44/100], Train Accuracy: 76.168 %, Loss: 0.7367\n",
            "Epoch [44/100], Learning Rate: 0.0007790686370876671\n",
            "Epoch [45/100], Train Accuracy: 76.532 %, Loss: 0.7613\n",
            "Epoch [45/100], Learning Rate: 0.0007667891533457719\n",
            "Epoch [46/100], Train Accuracy: 76.872 %, Loss: 0.6846\n",
            "Epoch [46/100], Learning Rate: 0.000754295724882796\n",
            "Epoch [47/100], Train Accuracy: 76.944 %, Loss: 0.6402\n",
            "Epoch [47/100], Learning Rate: 0.0007416006812042827\n",
            "Epoch [48/100], Train Accuracy: 77.596 %, Loss: 0.6525\n",
            "Epoch [48/100], Learning Rate: 0.0007287165507856513\n",
            "Epoch [49/100], Train Accuracy: 77.574 %, Loss: 0.6031\n",
            "Epoch [49/100], Learning Rate: 0.0007156560487081052\n",
            "Epoch [50/100], Train Accuracy: 77.832 %, Loss: 0.6149\n",
            "Epoch [50/100], Learning Rate: 0.0007024320641103813\n",
            "Epoch [51/100], Train Accuracy: 77.782 %, Loss: 0.6807\n",
            "Epoch [51/100], Learning Rate: 0.0006890576474687264\n",
            "Epoch [52/100], Train Accuracy: 78.028 %, Loss: 0.7171\n",
            "Epoch [52/100], Learning Rate: 0.0006755459977176532\n",
            "Epoch [53/100], Train Accuracy: 78.282 %, Loss: 0.6314\n",
            "Epoch [53/100], Learning Rate: 0.0006619104492241846\n",
            "Epoch [54/100], Train Accuracy: 78.336 %, Loss: 0.5391\n",
            "Epoch [54/100], Learning Rate: 0.0006481644586284443\n",
            "Epoch [55/100], Train Accuracy: 78.482 %, Loss: 0.5732\n",
            "Epoch [55/100], Learning Rate: 0.0006343215915635762\n",
            "Epoch [56/100], Train Accuracy: 78.898 %, Loss: 0.4743\n",
            "Epoch [56/100], Learning Rate: 0.000620395509268104\n",
            "Epoch [57/100], Train Accuracy: 78.648 %, Loss: 0.6843\n",
            "Epoch [57/100], Learning Rate: 0.000606399955103937\n",
            "Epoch [58/100], Train Accuracy: 78.586 %, Loss: 0.5661\n",
            "Epoch [58/100], Learning Rate: 0.0005923487409933315\n",
            "Epoch [59/100], Train Accuracy: 79.104 %, Loss: 0.5756\n",
            "Epoch [59/100], Learning Rate: 0.0005782557337881911\n",
            "Epoch [60/100], Train Accuracy: 79.5 %, Loss: 0.6135\n",
            "Epoch [60/100], Learning Rate: 0.0005641348415851578\n",
            "Epoch [61/100], Train Accuracy: 79.562 %, Loss: 0.5416\n",
            "Epoch [61/100], Learning Rate: 0.00055\n",
            "Epoch [62/100], Train Accuracy: 79.472 %, Loss: 0.7060\n",
            "Epoch [62/100], Learning Rate: 0.0005358651584148423\n",
            "Epoch [63/100], Train Accuracy: 79.662 %, Loss: 0.6529\n",
            "Epoch [63/100], Learning Rate: 0.000521744266211809\n",
            "Epoch [64/100], Train Accuracy: 79.82 %, Loss: 0.5087\n",
            "Epoch [64/100], Learning Rate: 0.0005076512590066686\n",
            "Epoch [65/100], Train Accuracy: 79.966 %, Loss: 0.5246\n",
            "Epoch [65/100], Learning Rate: 0.0004936000448960632\n",
            "Epoch [66/100], Train Accuracy: 80.476 %, Loss: 0.5475\n",
            "Epoch [66/100], Learning Rate: 0.0004796044907318961\n",
            "Epoch [67/100], Train Accuracy: 80.344 %, Loss: 0.6426\n",
            "Epoch [67/100], Learning Rate: 0.0004656784084364239\n",
            "Epoch [68/100], Train Accuracy: 80.498 %, Loss: 0.5898\n",
            "Epoch [68/100], Learning Rate: 0.00045183554137155597\n",
            "Epoch [69/100], Train Accuracy: 80.478 %, Loss: 0.4904\n",
            "Epoch [69/100], Learning Rate: 0.0004380895507758153\n",
            "Epoch [70/100], Train Accuracy: 80.94 %, Loss: 0.5850\n",
            "Epoch [70/100], Learning Rate: 0.00042445400228234684\n",
            "Epoch [71/100], Train Accuracy: 80.828 %, Loss: 0.5380\n",
            "Epoch [71/100], Learning Rate: 0.0004109423525312738\n",
            "Epoch [72/100], Train Accuracy: 81.058 %, Loss: 0.5838\n",
            "Epoch [72/100], Learning Rate: 0.0003975679358896189\n",
            "Epoch [73/100], Train Accuracy: 81.246 %, Loss: 0.4951\n",
            "Epoch [73/100], Learning Rate: 0.000384343951291895\n",
            "Epoch [74/100], Train Accuracy: 81.378 %, Loss: 0.4833\n",
            "Epoch [74/100], Learning Rate: 0.00037128344921434864\n",
            "Epoch [75/100], Train Accuracy: 81.43 %, Loss: 0.5418\n",
            "Epoch [75/100], Learning Rate: 0.0003583993187957173\n",
            "Epoch [76/100], Train Accuracy: 82.068 %, Loss: 0.5450\n",
            "Epoch [76/100], Learning Rate: 0.00034570427511720393\n",
            "Epoch [77/100], Train Accuracy: 81.382 %, Loss: 0.5100\n",
            "Epoch [77/100], Learning Rate: 0.0003332108466542281\n",
            "Epoch [78/100], Train Accuracy: 81.618 %, Loss: 0.4705\n",
            "Epoch [78/100], Learning Rate: 0.0003209313629123329\n",
            "Epoch [79/100], Train Accuracy: 82.384 %, Loss: 0.4694\n",
            "Epoch [79/100], Learning Rate: 0.0003088779422594514\n",
            "Epoch [80/100], Train Accuracy: 82.288 %, Loss: 0.5012\n",
            "Epoch [80/100], Learning Rate: 0.0002970624799665412\n",
            "Epoch [81/100], Train Accuracy: 82.314 %, Loss: 0.5841\n",
            "Epoch [81/100], Learning Rate: 0.00028549663646838715\n",
            "Epoch [82/100], Train Accuracy: 82.812 %, Loss: 0.5713\n",
            "Epoch [82/100], Learning Rate: 0.00027419182585616055\n",
            "Epoch [83/100], Train Accuracy: 82.728 %, Loss: 0.4774\n",
            "Epoch [83/100], Learning Rate: 0.0002631592046130896\n",
            "Epoch [84/100], Train Accuracy: 82.902 %, Loss: 0.4982\n",
            "Epoch [84/100], Learning Rate: 0.00025240966060435674\n",
            "Epoch [85/100], Train Accuracy: 83.098 %, Loss: 0.4011\n",
            "Epoch [85/100], Learning Rate: 0.00024195380233209008\n",
            "Epoch [86/100], Train Accuracy: 82.934 %, Loss: 0.6665\n",
            "Epoch [86/100], Learning Rate: 0.00023180194846605365\n",
            "Epoch [87/100], Train Accuracy: 83.272 %, Loss: 0.5131\n",
            "Epoch [87/100], Learning Rate: 0.0002219641176603649\n",
            "Epoch [88/100], Train Accuracy: 83.426 %, Loss: 0.4317\n",
            "Epoch [88/100], Learning Rate: 0.0002124500186662932\n",
            "Epoch [89/100], Train Accuracy: 83.536 %, Loss: 0.5038\n",
            "Epoch [89/100], Learning Rate: 0.00020326904075089488\n",
            "Epoch [90/100], Train Accuracy: 83.576 %, Loss: 0.4835\n",
            "Epoch [90/100], Learning Rate: 0.00019443024443093932\n",
            "Epoch [91/100], Train Accuracy: 84.066 %, Loss: 0.4378\n",
            "Epoch [91/100], Learning Rate: 0.00018594235253127368\n",
            "Epoch [92/100], Train Accuracy: 83.974 %, Loss: 0.3902\n",
            "Epoch [92/100], Learning Rate: 0.00017781374157644722\n",
            "Epoch [93/100], Train Accuracy: 84.114 %, Loss: 0.4353\n",
            "Epoch [93/100], Learning Rate: 0.00017005243352409332\n",
            "Epoch [94/100], Train Accuracy: 84.304 %, Loss: 0.5035\n",
            "Epoch [94/100], Learning Rate: 0.0001626660878482253\n",
            "Epoch [95/100], Train Accuracy: 84.446 %, Loss: 0.4484\n",
            "Epoch [95/100], Learning Rate: 0.0001556619939802614\n",
            "Epoch [96/100], Train Accuracy: 84.954 %, Loss: 0.4401\n",
            "Epoch [96/100], Learning Rate: 0.0001490470641152345\n",
            "Epoch [97/100], Train Accuracy: 84.73 %, Loss: 0.5102\n",
            "Epoch [97/100], Learning Rate: 0.0001428278263902913\n",
            "Epoch [98/100], Train Accuracy: 84.682 %, Loss: 0.3873\n",
            "Epoch [98/100], Learning Rate: 0.00013701041844220858\n",
            "Epoch [99/100], Train Accuracy: 85.202 %, Loss: 0.3538\n",
            "Epoch [99/100], Learning Rate: 0.0001316005813502869\n",
            "Epoch [100/100], Train Accuracy: 85.044 %, Loss: 0.3465\n",
            "Epoch [100/100], Learning Rate: 0.00012660365397059855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習前後のモデルの重みを比較する関数を作成\n",
        "def check_weight_change(model, model_init):\n",
        "    weights_changed = (model.conv1.state_dict()['weight'] != model_init.conv1.state_dict()['weight']).any()\n",
        "    if weights_changed:\n",
        "        return print('モデルの重みが変化しています')\n",
        "    else:\n",
        "        return print('モデルの重みは変化していません')\n",
        "# 学習前後でモデルの重みが変化していないかを確認\n",
        "check_weight_change(model, model_init)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nj2ts9H0xccz",
        "outputId": "60e3f5c8-d682-4dd3-f427-59f96f77f58c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "モデルの重みは変化していません\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの評価\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for images, labels in test_loader_cifar10:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度の確認\n",
        "    print('Test Accuracy: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPamaTpcxccz",
        "outputId": "1f397aff-606e-46f8-d76a-08a6400e01b0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 77.51 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUメモリの解放\n",
        "del model, model_init\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "_Vyaa379xccz"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prune_rate=0.7"
      ],
      "metadata": {
        "id": "pJyew3dez2Wc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 刈り込み率を設定\n",
        "prune_rate = 0.7\n",
        "# 初期化手法を設定\n",
        "init = 'kaiming_uniform'\n",
        "\n",
        "# デバイスを設定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# モデルを定義\n",
        "model = supermaskresnet18_10().to(device)\n",
        "# 学習前のモデルの重みを保存\n",
        "model_init = copy.deepcopy(model)\n",
        "\n",
        "# 学習率を設定\n",
        "learning_rate = 0.001\n",
        "# 全体のepoch数を設定\n",
        "num_epochs = 100\n",
        "# warm-upするepoch数を設定\n",
        "warmup_epochs = 10\n",
        "\n",
        "# 損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# オプティマイザーを設定\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# スケジューラーを設定\n",
        "scheduler = CosineLRScheduler(optimizer, t_initial=num_epochs, lr_min=1e-4, warmup_t=warmup_epochs, warmup_lr_init=1e-4, warmup_prefix=True)\n",
        "\n",
        "# モデルの学習\n",
        "for epoch in range(num_epochs):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, (images, labels) in enumerate(train_loader_cifar10):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # 順伝播\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # 逆伝播\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # 全データ数\n",
        "        total += labels.size(0)\n",
        "        # 正解数\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度を計算\n",
        "    train_acc = 100 * correct / total\n",
        "    # 正解率精度と損失を確認\n",
        "    print(\"Epoch [{}/{}], Train Accuracy: {} %, Loss: {:.4f}\".format(epoch+1, num_epochs, 100 * correct / total, loss.item()))\n",
        "    # 1エポック終了後にスケジューラーを更新\n",
        "    scheduler.step(epoch)\n",
        "    # 学習率の確認\n",
        "    print(\"Epoch [{}/{}], Learning Rate: {}\".format(epoch+1, num_epochs, optimizer.param_groups[0]['lr']))\n",
        "\n",
        "# モデルを保存\n",
        "torch.save(model, '/content/drive/MyDrive/SupermaskResNet18_CIFAR10_100epochs_pr0.7_uniform_CLRS_restest.pth')"
      ],
      "metadata": {
        "id": "lkPQYc-Uz2Wc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6905995b-cd57-4cca-9937-512f4bad002b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Accuracy: 34.728 %, Loss: 1.5875\n",
            "Epoch [1/100], Learning Rate: 0.0001\n",
            "Epoch [2/100], Train Accuracy: 44.7 %, Loss: 1.3852\n",
            "Epoch [2/100], Learning Rate: 0.00019\n",
            "Epoch [3/100], Train Accuracy: 48.354 %, Loss: 1.4456\n",
            "Epoch [3/100], Learning Rate: 0.00028\n",
            "Epoch [4/100], Train Accuracy: 52.154 %, Loss: 1.2339\n",
            "Epoch [4/100], Learning Rate: 0.00036999999999999994\n",
            "Epoch [5/100], Train Accuracy: 54.936 %, Loss: 1.3104\n",
            "Epoch [5/100], Learning Rate: 0.00045999999999999996\n",
            "Epoch [6/100], Train Accuracy: 57.596 %, Loss: 1.0353\n",
            "Epoch [6/100], Learning Rate: 0.00055\n",
            "Epoch [7/100], Train Accuracy: 59.792 %, Loss: 1.1274\n",
            "Epoch [7/100], Learning Rate: 0.0006399999999999999\n",
            "Epoch [8/100], Train Accuracy: 61.894 %, Loss: 1.0123\n",
            "Epoch [8/100], Learning Rate: 0.00073\n",
            "Epoch [9/100], Train Accuracy: 63.03 %, Loss: 0.9541\n",
            "Epoch [9/100], Learning Rate: 0.00082\n",
            "Epoch [10/100], Train Accuracy: 64.992 %, Loss: 0.9822\n",
            "Epoch [10/100], Learning Rate: 0.00091\n",
            "Epoch [11/100], Train Accuracy: 65.772 %, Loss: 0.8636\n",
            "Epoch [11/100], Learning Rate: 0.001\n",
            "Epoch [12/100], Train Accuracy: 66.846 %, Loss: 0.9591\n",
            "Epoch [12/100], Learning Rate: 0.0009997779521645793\n",
            "Epoch [13/100], Train Accuracy: 68.238 %, Loss: 0.8381\n",
            "Epoch [13/100], Learning Rate: 0.0009991120277927223\n",
            "Epoch [14/100], Train Accuracy: 69.426 %, Loss: 0.8602\n",
            "Epoch [14/100], Learning Rate: 0.000998002884071386\n",
            "Epoch [15/100], Train Accuracy: 70.472 %, Loss: 0.8627\n",
            "Epoch [15/100], Learning Rate: 0.000996451615591515\n",
            "Epoch [16/100], Train Accuracy: 71.116 %, Loss: 0.8713\n",
            "Epoch [16/100], Learning Rate: 0.000994459753267812\n",
            "Epoch [17/100], Train Accuracy: 71.876 %, Loss: 0.8156\n",
            "Epoch [17/100], Learning Rate: 0.00099202926282791\n",
            "Epoch [18/100], Train Accuracy: 72.804 %, Loss: 0.7205\n",
            "Epoch [18/100], Learning Rate: 0.0009891625428724363\n",
            "Epoch [19/100], Train Accuracy: 72.926 %, Loss: 0.7514\n",
            "Epoch [19/100], Learning Rate: 0.000985862422507884\n",
            "Epoch [20/100], Train Accuracy: 73.666 %, Loss: 0.8135\n",
            "Epoch [20/100], Learning Rate: 0.0009821321585546244\n",
            "Epoch [21/100], Train Accuracy: 73.588 %, Loss: 0.7741\n",
            "Epoch [21/100], Learning Rate: 0.0009779754323328191\n",
            "Epoch [22/100], Train Accuracy: 74.38 %, Loss: 0.8949\n",
            "Epoch [22/100], Learning Rate: 0.0009733963460294015\n",
            "Epoch [23/100], Train Accuracy: 74.712 %, Loss: 0.7340\n",
            "Epoch [23/100], Learning Rate: 0.0009683994186497131\n",
            "Epoch [24/100], Train Accuracy: 75.366 %, Loss: 0.6439\n",
            "Epoch [24/100], Learning Rate: 0.0009629895815577914\n",
            "Epoch [25/100], Train Accuracy: 75.414 %, Loss: 0.7301\n",
            "Epoch [25/100], Learning Rate: 0.0009571721736097088\n",
            "Epoch [26/100], Train Accuracy: 76.194 %, Loss: 0.7969\n",
            "Epoch [26/100], Learning Rate: 0.0009509529358847655\n",
            "Epoch [27/100], Train Accuracy: 75.846 %, Loss: 0.6566\n",
            "Epoch [27/100], Learning Rate: 0.0009443380060197386\n",
            "Epoch [28/100], Train Accuracy: 76.564 %, Loss: 0.6562\n",
            "Epoch [28/100], Learning Rate: 0.0009373339121517746\n",
            "Epoch [29/100], Train Accuracy: 76.828 %, Loss: 0.7099\n",
            "Epoch [29/100], Learning Rate: 0.0009299475664759069\n",
            "Epoch [30/100], Train Accuracy: 76.968 %, Loss: 0.7462\n",
            "Epoch [30/100], Learning Rate: 0.0009221862584235528\n",
            "Epoch [31/100], Train Accuracy: 77.324 %, Loss: 0.6494\n",
            "Epoch [31/100], Learning Rate: 0.0009140576474687263\n",
            "Epoch [32/100], Train Accuracy: 77.686 %, Loss: 0.5609\n",
            "Epoch [32/100], Learning Rate: 0.0009055697555690606\n",
            "Epoch [33/100], Train Accuracy: 77.64 %, Loss: 0.5462\n",
            "Epoch [33/100], Learning Rate: 0.0008967309592491052\n",
            "Epoch [34/100], Train Accuracy: 78.246 %, Loss: 0.6561\n",
            "Epoch [34/100], Learning Rate: 0.0008875499813337069\n",
            "Epoch [35/100], Train Accuracy: 78.32 %, Loss: 0.6286\n",
            "Epoch [35/100], Learning Rate: 0.0008780358823396353\n",
            "Epoch [36/100], Train Accuracy: 78.434 %, Loss: 0.6482\n",
            "Epoch [36/100], Learning Rate: 0.0008681980515339464\n",
            "Epoch [37/100], Train Accuracy: 78.64 %, Loss: 0.5356\n",
            "Epoch [37/100], Learning Rate: 0.0008580461976679099\n",
            "Epoch [38/100], Train Accuracy: 78.994 %, Loss: 0.5234\n",
            "Epoch [38/100], Learning Rate: 0.0008475903393956434\n",
            "Epoch [39/100], Train Accuracy: 79.434 %, Loss: 0.7298\n",
            "Epoch [39/100], Learning Rate: 0.0008368407953869105\n",
            "Epoch [40/100], Train Accuracy: 79.534 %, Loss: 0.6866\n",
            "Epoch [40/100], Learning Rate: 0.0008258081741438395\n",
            "Epoch [41/100], Train Accuracy: 79.468 %, Loss: 0.5614\n",
            "Epoch [41/100], Learning Rate: 0.0008145033635316131\n",
            "Epoch [42/100], Train Accuracy: 80.144 %, Loss: 0.4875\n",
            "Epoch [42/100], Learning Rate: 0.0008029375200334589\n",
            "Epoch [43/100], Train Accuracy: 80.276 %, Loss: 0.6198\n",
            "Epoch [43/100], Learning Rate: 0.0007911220577405485\n",
            "Epoch [44/100], Train Accuracy: 80.422 %, Loss: 0.4776\n",
            "Epoch [44/100], Learning Rate: 0.0007790686370876671\n",
            "Epoch [45/100], Train Accuracy: 80.742 %, Loss: 0.5750\n",
            "Epoch [45/100], Learning Rate: 0.0007667891533457719\n",
            "Epoch [46/100], Train Accuracy: 80.928 %, Loss: 0.6081\n",
            "Epoch [46/100], Learning Rate: 0.000754295724882796\n",
            "Epoch [47/100], Train Accuracy: 80.974 %, Loss: 0.5531\n",
            "Epoch [47/100], Learning Rate: 0.0007416006812042827\n",
            "Epoch [48/100], Train Accuracy: 81.38 %, Loss: 0.5857\n",
            "Epoch [48/100], Learning Rate: 0.0007287165507856513\n",
            "Epoch [49/100], Train Accuracy: 81.544 %, Loss: 0.5818\n",
            "Epoch [49/100], Learning Rate: 0.0007156560487081052\n",
            "Epoch [50/100], Train Accuracy: 81.516 %, Loss: 0.5516\n",
            "Epoch [50/100], Learning Rate: 0.0007024320641103813\n",
            "Epoch [51/100], Train Accuracy: 81.382 %, Loss: 0.5704\n",
            "Epoch [51/100], Learning Rate: 0.0006890576474687264\n",
            "Epoch [52/100], Train Accuracy: 81.892 %, Loss: 0.5549\n",
            "Epoch [52/100], Learning Rate: 0.0006755459977176532\n",
            "Epoch [53/100], Train Accuracy: 82.39 %, Loss: 0.4330\n",
            "Epoch [53/100], Learning Rate: 0.0006619104492241846\n",
            "Epoch [54/100], Train Accuracy: 82.316 %, Loss: 0.4374\n",
            "Epoch [54/100], Learning Rate: 0.0006481644586284443\n",
            "Epoch [55/100], Train Accuracy: 82.382 %, Loss: 0.4260\n",
            "Epoch [55/100], Learning Rate: 0.0006343215915635762\n",
            "Epoch [56/100], Train Accuracy: 82.522 %, Loss: 0.4479\n",
            "Epoch [56/100], Learning Rate: 0.000620395509268104\n",
            "Epoch [57/100], Train Accuracy: 82.952 %, Loss: 0.4634\n",
            "Epoch [57/100], Learning Rate: 0.000606399955103937\n",
            "Epoch [58/100], Train Accuracy: 82.98 %, Loss: 0.4621\n",
            "Epoch [58/100], Learning Rate: 0.0005923487409933315\n",
            "Epoch [59/100], Train Accuracy: 83.268 %, Loss: 0.4679\n",
            "Epoch [59/100], Learning Rate: 0.0005782557337881911\n",
            "Epoch [60/100], Train Accuracy: 83.512 %, Loss: 0.4738\n",
            "Epoch [60/100], Learning Rate: 0.0005641348415851578\n",
            "Epoch [61/100], Train Accuracy: 83.676 %, Loss: 0.5031\n",
            "Epoch [61/100], Learning Rate: 0.00055\n",
            "Epoch [62/100], Train Accuracy: 84.094 %, Loss: 0.4131\n",
            "Epoch [62/100], Learning Rate: 0.0005358651584148423\n",
            "Epoch [63/100], Train Accuracy: 84.098 %, Loss: 0.4402\n",
            "Epoch [63/100], Learning Rate: 0.000521744266211809\n",
            "Epoch [64/100], Train Accuracy: 84.12 %, Loss: 0.4287\n",
            "Epoch [64/100], Learning Rate: 0.0005076512590066686\n",
            "Epoch [65/100], Train Accuracy: 84.364 %, Loss: 0.4138\n",
            "Epoch [65/100], Learning Rate: 0.0004936000448960632\n",
            "Epoch [66/100], Train Accuracy: 84.886 %, Loss: 0.5019\n",
            "Epoch [66/100], Learning Rate: 0.0004796044907318961\n",
            "Epoch [67/100], Train Accuracy: 84.742 %, Loss: 0.4652\n",
            "Epoch [67/100], Learning Rate: 0.0004656784084364239\n",
            "Epoch [68/100], Train Accuracy: 85.382 %, Loss: 0.4422\n",
            "Epoch [68/100], Learning Rate: 0.00045183554137155597\n",
            "Epoch [69/100], Train Accuracy: 85.374 %, Loss: 0.4495\n",
            "Epoch [69/100], Learning Rate: 0.0004380895507758153\n",
            "Epoch [70/100], Train Accuracy: 85.334 %, Loss: 0.4156\n",
            "Epoch [70/100], Learning Rate: 0.00042445400228234684\n",
            "Epoch [71/100], Train Accuracy: 85.626 %, Loss: 0.3945\n",
            "Epoch [71/100], Learning Rate: 0.0004109423525312738\n",
            "Epoch [72/100], Train Accuracy: 85.63 %, Loss: 0.3683\n",
            "Epoch [72/100], Learning Rate: 0.0003975679358896189\n",
            "Epoch [73/100], Train Accuracy: 85.788 %, Loss: 0.3872\n",
            "Epoch [73/100], Learning Rate: 0.000384343951291895\n",
            "Epoch [74/100], Train Accuracy: 85.95 %, Loss: 0.3806\n",
            "Epoch [74/100], Learning Rate: 0.00037128344921434864\n",
            "Epoch [75/100], Train Accuracy: 86.216 %, Loss: 0.3948\n",
            "Epoch [75/100], Learning Rate: 0.0003583993187957173\n",
            "Epoch [76/100], Train Accuracy: 86.114 %, Loss: 0.4061\n",
            "Epoch [76/100], Learning Rate: 0.00034570427511720393\n",
            "Epoch [77/100], Train Accuracy: 86.462 %, Loss: 0.3270\n",
            "Epoch [77/100], Learning Rate: 0.0003332108466542281\n",
            "Epoch [78/100], Train Accuracy: 87.144 %, Loss: 0.4327\n",
            "Epoch [78/100], Learning Rate: 0.0003209313629123329\n",
            "Epoch [79/100], Train Accuracy: 86.892 %, Loss: 0.4150\n",
            "Epoch [79/100], Learning Rate: 0.0003088779422594514\n",
            "Epoch [80/100], Train Accuracy: 87.182 %, Loss: 0.4118\n",
            "Epoch [80/100], Learning Rate: 0.0002970624799665412\n",
            "Epoch [81/100], Train Accuracy: 87.282 %, Loss: 0.3498\n",
            "Epoch [81/100], Learning Rate: 0.00028549663646838715\n",
            "Epoch [82/100], Train Accuracy: 87.398 %, Loss: 0.3496\n",
            "Epoch [82/100], Learning Rate: 0.00027419182585616055\n",
            "Epoch [83/100], Train Accuracy: 88.012 %, Loss: 0.3632\n",
            "Epoch [83/100], Learning Rate: 0.0002631592046130896\n",
            "Epoch [84/100], Train Accuracy: 88.128 %, Loss: 0.2893\n",
            "Epoch [84/100], Learning Rate: 0.00025240966060435674\n",
            "Epoch [85/100], Train Accuracy: 88.224 %, Loss: 0.3230\n",
            "Epoch [85/100], Learning Rate: 0.00024195380233209008\n",
            "Epoch [86/100], Train Accuracy: 88.282 %, Loss: 0.2669\n",
            "Epoch [86/100], Learning Rate: 0.00023180194846605365\n",
            "Epoch [87/100], Train Accuracy: 88.48 %, Loss: 0.2611\n",
            "Epoch [87/100], Learning Rate: 0.0002219641176603649\n",
            "Epoch [88/100], Train Accuracy: 88.8 %, Loss: 0.2398\n",
            "Epoch [88/100], Learning Rate: 0.0002124500186662932\n",
            "Epoch [89/100], Train Accuracy: 88.78 %, Loss: 0.3837\n",
            "Epoch [89/100], Learning Rate: 0.00020326904075089488\n",
            "Epoch [90/100], Train Accuracy: 88.948 %, Loss: 0.2540\n",
            "Epoch [90/100], Learning Rate: 0.00019443024443093932\n",
            "Epoch [91/100], Train Accuracy: 89.25 %, Loss: 0.3071\n",
            "Epoch [91/100], Learning Rate: 0.00018594235253127368\n",
            "Epoch [92/100], Train Accuracy: 89.448 %, Loss: 0.3303\n",
            "Epoch [92/100], Learning Rate: 0.00017781374157644722\n",
            "Epoch [93/100], Train Accuracy: 89.83 %, Loss: 0.3840\n",
            "Epoch [93/100], Learning Rate: 0.00017005243352409332\n",
            "Epoch [94/100], Train Accuracy: 89.91 %, Loss: 0.3767\n",
            "Epoch [94/100], Learning Rate: 0.0001626660878482253\n",
            "Epoch [95/100], Train Accuracy: 90.132 %, Loss: 0.3195\n",
            "Epoch [95/100], Learning Rate: 0.0001556619939802614\n",
            "Epoch [96/100], Train Accuracy: 90.052 %, Loss: 0.2338\n",
            "Epoch [96/100], Learning Rate: 0.0001490470641152345\n",
            "Epoch [97/100], Train Accuracy: 90.374 %, Loss: 0.2735\n",
            "Epoch [97/100], Learning Rate: 0.0001428278263902913\n",
            "Epoch [98/100], Train Accuracy: 90.462 %, Loss: 0.3153\n",
            "Epoch [98/100], Learning Rate: 0.00013701041844220858\n",
            "Epoch [99/100], Train Accuracy: 90.546 %, Loss: 0.2473\n",
            "Epoch [99/100], Learning Rate: 0.0001316005813502869\n",
            "Epoch [100/100], Train Accuracy: 90.72 %, Loss: 0.3292\n",
            "Epoch [100/100], Learning Rate: 0.00012660365397059855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習前後のモデルの重みを比較する関数を作成\n",
        "def check_weight_change(model, model_init):\n",
        "    weights_changed = (model.conv1.state_dict()['weight'] != model_init.conv1.state_dict()['weight']).any()\n",
        "    if weights_changed:\n",
        "        return print('モデルの重みが変化しています')\n",
        "    else:\n",
        "        return print('モデルの重みは変化していません')\n",
        "# 学習前後でモデルの重みが変化していないかを確認\n",
        "check_weight_change(model, model_init)"
      ],
      "metadata": {
        "id": "A9VrLeGlqwq7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14a0786e-37f0-47fc-c77b-6397728a4162"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "モデルの重みは変化していません\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの評価\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for images, labels in test_loader_cifar10:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度の確認\n",
        "    print('Test Accuracy: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "id": "1-yK4fD1z2Wd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99c4793d-d0bd-4324-9160-125466c3535d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 80.29 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUメモリの解放\n",
        "del model, model_init\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "J-woiaozz2Wd"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prune_rate=0.5"
      ],
      "metadata": {
        "id": "_KprKABkwBxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 刈り込み率を設定\n",
        "prune_rate = 0.5\n",
        "# 初期化手法を設定\n",
        "init = 'kaiming_uniform'\n",
        "\n",
        "# デバイスを設定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# モデルを定義\n",
        "model = supermaskresnet18_10().to(device)\n",
        "# 学習前のモデルの重みを保存\n",
        "model_init = copy.deepcopy(model)\n",
        "\n",
        "# 学習率を設定\n",
        "learning_rate = 0.001\n",
        "# 全体のepoch数を設定\n",
        "num_epochs = 100\n",
        "# warm-upするepoch数を設定\n",
        "warmup_epochs = 10\n",
        "\n",
        "# 損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# オプティマイザーを設定\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# スケジューラーを設定\n",
        "scheduler = CosineLRScheduler(optimizer, t_initial=num_epochs, lr_min=1e-4, warmup_t=warmup_epochs, warmup_lr_init=1e-4, warmup_prefix=True)\n",
        "\n",
        "# モデルの学習\n",
        "for epoch in range(num_epochs):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, (images, labels) in enumerate(train_loader_cifar10):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # 順伝播\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # 逆伝播\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # 全データ数\n",
        "        total += labels.size(0)\n",
        "        # 正解数\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度を計算\n",
        "    train_acc = 100 * correct / total\n",
        "    # 正解率精度と損失を確認\n",
        "    print(\"Epoch [{}/{}], Train Accuracy: {} %, Loss: {:.4f}\".format(epoch+1, num_epochs, 100 * correct / total, loss.item()))\n",
        "    # 1エポック終了後にスケジューラーを更新\n",
        "    scheduler.step(epoch)\n",
        "    # 学習率の確認\n",
        "    print(\"Epoch [{}/{}], Learning Rate: {}\".format(epoch+1, num_epochs, optimizer.param_groups[0]['lr']))\n",
        "\n",
        "# モデルを保存\n",
        "torch.save(model, '/content/drive/MyDrive/SupermaskResNet18_CIFAR10_100epochs_pr0.5_uniform_CLRS_restest.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-4T4JOowKjW",
        "outputId": "d8463ebb-807b-4095-9cf8-435a4edb027f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Accuracy: 33.062 %, Loss: 1.7033\n",
            "Epoch [1/100], Learning Rate: 0.0001\n",
            "Epoch [2/100], Train Accuracy: 43.42 %, Loss: 1.4690\n",
            "Epoch [2/100], Learning Rate: 0.00019\n",
            "Epoch [3/100], Train Accuracy: 47.358 %, Loss: 1.3610\n",
            "Epoch [3/100], Learning Rate: 0.00028\n",
            "Epoch [4/100], Train Accuracy: 51.13 %, Loss: 1.2903\n",
            "Epoch [4/100], Learning Rate: 0.00036999999999999994\n",
            "Epoch [5/100], Train Accuracy: 54.436 %, Loss: 1.2489\n",
            "Epoch [5/100], Learning Rate: 0.00045999999999999996\n",
            "Epoch [6/100], Train Accuracy: 57.162 %, Loss: 1.0601\n",
            "Epoch [6/100], Learning Rate: 0.00055\n",
            "Epoch [7/100], Train Accuracy: 59.382 %, Loss: 1.0830\n",
            "Epoch [7/100], Learning Rate: 0.0006399999999999999\n",
            "Epoch [8/100], Train Accuracy: 61.356 %, Loss: 1.1013\n",
            "Epoch [8/100], Learning Rate: 0.00073\n",
            "Epoch [9/100], Train Accuracy: 63.408 %, Loss: 0.9560\n",
            "Epoch [9/100], Learning Rate: 0.00082\n",
            "Epoch [10/100], Train Accuracy: 64.582 %, Loss: 0.9698\n",
            "Epoch [10/100], Learning Rate: 0.00091\n",
            "Epoch [11/100], Train Accuracy: 65.878 %, Loss: 0.9164\n",
            "Epoch [11/100], Learning Rate: 0.001\n",
            "Epoch [12/100], Train Accuracy: 66.538 %, Loss: 1.0242\n",
            "Epoch [12/100], Learning Rate: 0.0009997779521645793\n",
            "Epoch [13/100], Train Accuracy: 67.87 %, Loss: 0.8834\n",
            "Epoch [13/100], Learning Rate: 0.0009991120277927223\n",
            "Epoch [14/100], Train Accuracy: 69.156 %, Loss: 0.9013\n",
            "Epoch [14/100], Learning Rate: 0.000998002884071386\n",
            "Epoch [15/100], Train Accuracy: 69.868 %, Loss: 0.7844\n",
            "Epoch [15/100], Learning Rate: 0.000996451615591515\n",
            "Epoch [16/100], Train Accuracy: 70.562 %, Loss: 0.8055\n",
            "Epoch [16/100], Learning Rate: 0.000994459753267812\n",
            "Epoch [17/100], Train Accuracy: 71.416 %, Loss: 0.8575\n",
            "Epoch [17/100], Learning Rate: 0.00099202926282791\n",
            "Epoch [18/100], Train Accuracy: 71.918 %, Loss: 0.7753\n",
            "Epoch [18/100], Learning Rate: 0.0009891625428724363\n",
            "Epoch [19/100], Train Accuracy: 72.748 %, Loss: 0.7061\n",
            "Epoch [19/100], Learning Rate: 0.000985862422507884\n",
            "Epoch [20/100], Train Accuracy: 73.036 %, Loss: 0.7202\n",
            "Epoch [20/100], Learning Rate: 0.0009821321585546244\n",
            "Epoch [21/100], Train Accuracy: 73.458 %, Loss: 0.8021\n",
            "Epoch [21/100], Learning Rate: 0.0009779754323328191\n",
            "Epoch [22/100], Train Accuracy: 73.84 %, Loss: 0.7589\n",
            "Epoch [22/100], Learning Rate: 0.0009733963460294015\n",
            "Epoch [23/100], Train Accuracy: 74.322 %, Loss: 0.6791\n",
            "Epoch [23/100], Learning Rate: 0.0009683994186497131\n",
            "Epoch [24/100], Train Accuracy: 74.696 %, Loss: 0.7401\n",
            "Epoch [24/100], Learning Rate: 0.0009629895815577914\n",
            "Epoch [25/100], Train Accuracy: 74.896 %, Loss: 0.7208\n",
            "Epoch [25/100], Learning Rate: 0.0009571721736097088\n",
            "Epoch [26/100], Train Accuracy: 75.23 %, Loss: 0.7797\n",
            "Epoch [26/100], Learning Rate: 0.0009509529358847655\n",
            "Epoch [27/100], Train Accuracy: 75.52 %, Loss: 0.6925\n",
            "Epoch [27/100], Learning Rate: 0.0009443380060197386\n",
            "Epoch [28/100], Train Accuracy: 75.684 %, Loss: 0.6654\n",
            "Epoch [28/100], Learning Rate: 0.0009373339121517746\n",
            "Epoch [29/100], Train Accuracy: 75.736 %, Loss: 0.6759\n",
            "Epoch [29/100], Learning Rate: 0.0009299475664759069\n",
            "Epoch [30/100], Train Accuracy: 76.222 %, Loss: 0.6481\n",
            "Epoch [30/100], Learning Rate: 0.0009221862584235528\n",
            "Epoch [31/100], Train Accuracy: 76.496 %, Loss: 0.6128\n",
            "Epoch [31/100], Learning Rate: 0.0009140576474687263\n",
            "Epoch [32/100], Train Accuracy: 76.66 %, Loss: 0.6637\n",
            "Epoch [32/100], Learning Rate: 0.0009055697555690606\n",
            "Epoch [33/100], Train Accuracy: 76.71 %, Loss: 0.6450\n",
            "Epoch [33/100], Learning Rate: 0.0008967309592491052\n",
            "Epoch [34/100], Train Accuracy: 76.884 %, Loss: 0.5897\n",
            "Epoch [34/100], Learning Rate: 0.0008875499813337069\n",
            "Epoch [35/100], Train Accuracy: 77.2 %, Loss: 0.6023\n",
            "Epoch [35/100], Learning Rate: 0.0008780358823396353\n",
            "Epoch [36/100], Train Accuracy: 77.834 %, Loss: 0.6502\n",
            "Epoch [36/100], Learning Rate: 0.0008681980515339464\n",
            "Epoch [37/100], Train Accuracy: 77.8 %, Loss: 0.6986\n",
            "Epoch [37/100], Learning Rate: 0.0008580461976679099\n",
            "Epoch [38/100], Train Accuracy: 77.762 %, Loss: 0.6892\n",
            "Epoch [38/100], Learning Rate: 0.0008475903393956434\n",
            "Epoch [39/100], Train Accuracy: 77.924 %, Loss: 0.6181\n",
            "Epoch [39/100], Learning Rate: 0.0008368407953869105\n",
            "Epoch [40/100], Train Accuracy: 78.26 %, Loss: 0.6969\n",
            "Epoch [40/100], Learning Rate: 0.0008258081741438395\n",
            "Epoch [41/100], Train Accuracy: 78.79 %, Loss: 0.5883\n",
            "Epoch [41/100], Learning Rate: 0.0008145033635316131\n",
            "Epoch [42/100], Train Accuracy: 78.778 %, Loss: 0.7190\n",
            "Epoch [42/100], Learning Rate: 0.0008029375200334589\n",
            "Epoch [43/100], Train Accuracy: 78.688 %, Loss: 0.6311\n",
            "Epoch [43/100], Learning Rate: 0.0007911220577405485\n",
            "Epoch [44/100], Train Accuracy: 79.124 %, Loss: 0.6040\n",
            "Epoch [44/100], Learning Rate: 0.0007790686370876671\n",
            "Epoch [45/100], Train Accuracy: 79.49 %, Loss: 0.5833\n",
            "Epoch [45/100], Learning Rate: 0.0007667891533457719\n",
            "Epoch [46/100], Train Accuracy: 79.234 %, Loss: 0.5520\n",
            "Epoch [46/100], Learning Rate: 0.000754295724882796\n",
            "Epoch [47/100], Train Accuracy: 79.876 %, Loss: 0.6770\n",
            "Epoch [47/100], Learning Rate: 0.0007416006812042827\n",
            "Epoch [48/100], Train Accuracy: 80.098 %, Loss: 0.6057\n",
            "Epoch [48/100], Learning Rate: 0.0007287165507856513\n",
            "Epoch [49/100], Train Accuracy: 80.072 %, Loss: 0.5636\n",
            "Epoch [49/100], Learning Rate: 0.0007156560487081052\n",
            "Epoch [50/100], Train Accuracy: 80.59 %, Loss: 0.6279\n",
            "Epoch [50/100], Learning Rate: 0.0007024320641103813\n",
            "Epoch [51/100], Train Accuracy: 80.48 %, Loss: 0.5478\n",
            "Epoch [51/100], Learning Rate: 0.0006890576474687264\n",
            "Epoch [52/100], Train Accuracy: 80.856 %, Loss: 0.6554\n",
            "Epoch [52/100], Learning Rate: 0.0006755459977176532\n",
            "Epoch [53/100], Train Accuracy: 81.156 %, Loss: 0.5798\n",
            "Epoch [53/100], Learning Rate: 0.0006619104492241846\n",
            "Epoch [54/100], Train Accuracy: 81.294 %, Loss: 0.5134\n",
            "Epoch [54/100], Learning Rate: 0.0006481644586284443\n",
            "Epoch [55/100], Train Accuracy: 81.612 %, Loss: 0.6180\n",
            "Epoch [55/100], Learning Rate: 0.0006343215915635762\n",
            "Epoch [56/100], Train Accuracy: 81.708 %, Loss: 0.6137\n",
            "Epoch [56/100], Learning Rate: 0.000620395509268104\n",
            "Epoch [57/100], Train Accuracy: 81.876 %, Loss: 0.5104\n",
            "Epoch [57/100], Learning Rate: 0.000606399955103937\n",
            "Epoch [58/100], Train Accuracy: 82.148 %, Loss: 0.5024\n",
            "Epoch [58/100], Learning Rate: 0.0005923487409933315\n",
            "Epoch [59/100], Train Accuracy: 82.302 %, Loss: 0.5926\n",
            "Epoch [59/100], Learning Rate: 0.0005782557337881911\n",
            "Epoch [60/100], Train Accuracy: 82.532 %, Loss: 0.4761\n",
            "Epoch [60/100], Learning Rate: 0.0005641348415851578\n",
            "Epoch [61/100], Train Accuracy: 82.462 %, Loss: 0.4788\n",
            "Epoch [61/100], Learning Rate: 0.00055\n",
            "Epoch [62/100], Train Accuracy: 82.964 %, Loss: 0.5112\n",
            "Epoch [62/100], Learning Rate: 0.0005358651584148423\n",
            "Epoch [63/100], Train Accuracy: 83.002 %, Loss: 0.5380\n",
            "Epoch [63/100], Learning Rate: 0.000521744266211809\n",
            "Epoch [64/100], Train Accuracy: 83.524 %, Loss: 0.4780\n",
            "Epoch [64/100], Learning Rate: 0.0005076512590066686\n",
            "Epoch [65/100], Train Accuracy: 83.492 %, Loss: 0.5096\n",
            "Epoch [65/100], Learning Rate: 0.0004936000448960632\n",
            "Epoch [66/100], Train Accuracy: 83.84 %, Loss: 0.4950\n",
            "Epoch [66/100], Learning Rate: 0.0004796044907318961\n",
            "Epoch [67/100], Train Accuracy: 83.846 %, Loss: 0.4929\n",
            "Epoch [67/100], Learning Rate: 0.0004656784084364239\n",
            "Epoch [68/100], Train Accuracy: 84.178 %, Loss: 0.4665\n",
            "Epoch [68/100], Learning Rate: 0.00045183554137155597\n",
            "Epoch [69/100], Train Accuracy: 84.346 %, Loss: 0.5758\n",
            "Epoch [69/100], Learning Rate: 0.0004380895507758153\n",
            "Epoch [70/100], Train Accuracy: 84.334 %, Loss: 0.3865\n",
            "Epoch [70/100], Learning Rate: 0.00042445400228234684\n",
            "Epoch [71/100], Train Accuracy: 84.286 %, Loss: 0.5103\n",
            "Epoch [71/100], Learning Rate: 0.0004109423525312738\n",
            "Epoch [72/100], Train Accuracy: 84.64 %, Loss: 0.4661\n",
            "Epoch [72/100], Learning Rate: 0.0003975679358896189\n",
            "Epoch [73/100], Train Accuracy: 85.194 %, Loss: 0.4524\n",
            "Epoch [73/100], Learning Rate: 0.000384343951291895\n",
            "Epoch [74/100], Train Accuracy: 85.16 %, Loss: 0.3825\n",
            "Epoch [74/100], Learning Rate: 0.00037128344921434864\n",
            "Epoch [75/100], Train Accuracy: 85.398 %, Loss: 0.4989\n",
            "Epoch [75/100], Learning Rate: 0.0003583993187957173\n",
            "Epoch [76/100], Train Accuracy: 85.64 %, Loss: 0.4474\n",
            "Epoch [76/100], Learning Rate: 0.00034570427511720393\n",
            "Epoch [77/100], Train Accuracy: 85.768 %, Loss: 0.4835\n",
            "Epoch [77/100], Learning Rate: 0.0003332108466542281\n",
            "Epoch [78/100], Train Accuracy: 86.406 %, Loss: 0.4374\n",
            "Epoch [78/100], Learning Rate: 0.0003209313629123329\n",
            "Epoch [79/100], Train Accuracy: 86.274 %, Loss: 0.4351\n",
            "Epoch [79/100], Learning Rate: 0.0003088779422594514\n",
            "Epoch [80/100], Train Accuracy: 86.442 %, Loss: 0.4466\n",
            "Epoch [80/100], Learning Rate: 0.0002970624799665412\n",
            "Epoch [81/100], Train Accuracy: 86.812 %, Loss: 0.4273\n",
            "Epoch [81/100], Learning Rate: 0.00028549663646838715\n",
            "Epoch [82/100], Train Accuracy: 87.076 %, Loss: 0.3751\n",
            "Epoch [82/100], Learning Rate: 0.00027419182585616055\n",
            "Epoch [83/100], Train Accuracy: 87.154 %, Loss: 0.4022\n",
            "Epoch [83/100], Learning Rate: 0.0002631592046130896\n",
            "Epoch [84/100], Train Accuracy: 87.182 %, Loss: 0.3640\n",
            "Epoch [84/100], Learning Rate: 0.00025240966060435674\n",
            "Epoch [85/100], Train Accuracy: 87.3 %, Loss: 0.3955\n",
            "Epoch [85/100], Learning Rate: 0.00024195380233209008\n",
            "Epoch [86/100], Train Accuracy: 87.428 %, Loss: 0.2986\n",
            "Epoch [86/100], Learning Rate: 0.00023180194846605365\n",
            "Epoch [87/100], Train Accuracy: 87.926 %, Loss: 0.3475\n",
            "Epoch [87/100], Learning Rate: 0.0002219641176603649\n",
            "Epoch [88/100], Train Accuracy: 87.974 %, Loss: 0.3946\n",
            "Epoch [88/100], Learning Rate: 0.0002124500186662932\n",
            "Epoch [89/100], Train Accuracy: 88.212 %, Loss: 0.3734\n",
            "Epoch [89/100], Learning Rate: 0.00020326904075089488\n",
            "Epoch [90/100], Train Accuracy: 88.474 %, Loss: 0.3099\n",
            "Epoch [90/100], Learning Rate: 0.00019443024443093932\n",
            "Epoch [91/100], Train Accuracy: 88.734 %, Loss: 0.3392\n",
            "Epoch [91/100], Learning Rate: 0.00018594235253127368\n",
            "Epoch [92/100], Train Accuracy: 89.164 %, Loss: 0.3367\n",
            "Epoch [92/100], Learning Rate: 0.00017781374157644722\n",
            "Epoch [93/100], Train Accuracy: 89.208 %, Loss: 0.3824\n",
            "Epoch [93/100], Learning Rate: 0.00017005243352409332\n",
            "Epoch [94/100], Train Accuracy: 89.25 %, Loss: 0.4225\n",
            "Epoch [94/100], Learning Rate: 0.0001626660878482253\n",
            "Epoch [95/100], Train Accuracy: 89.524 %, Loss: 0.3248\n",
            "Epoch [95/100], Learning Rate: 0.0001556619939802614\n",
            "Epoch [96/100], Train Accuracy: 89.328 %, Loss: 0.2464\n",
            "Epoch [96/100], Learning Rate: 0.0001490470641152345\n",
            "Epoch [97/100], Train Accuracy: 89.42 %, Loss: 0.3650\n",
            "Epoch [97/100], Learning Rate: 0.0001428278263902913\n",
            "Epoch [98/100], Train Accuracy: 89.488 %, Loss: 0.3885\n",
            "Epoch [98/100], Learning Rate: 0.00013701041844220858\n",
            "Epoch [99/100], Train Accuracy: 89.718 %, Loss: 0.3017\n",
            "Epoch [99/100], Learning Rate: 0.0001316005813502869\n",
            "Epoch [100/100], Train Accuracy: 89.912 %, Loss: 0.3207\n",
            "Epoch [100/100], Learning Rate: 0.00012660365397059855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習前後のモデルの重みを比較する関数を作成\n",
        "def check_weight_change(model, model_init):\n",
        "    weights_changed = (model.conv1.state_dict()['weight'] != model_init.conv1.state_dict()['weight']).any()\n",
        "    if weights_changed:\n",
        "        return print('モデルの重みが変化しています')\n",
        "    else:\n",
        "        return print('モデルの重みは変化していません')\n",
        "# 学習前後でモデルの重みが変化していないかを確認\n",
        "check_weight_change(model, model_init)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aegn7l8uwBxt",
        "outputId": "fc37326b-f21b-40b5-8541-bf233ac38304"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "モデルの重みは変化していません\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの評価\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for images, labels in test_loader_cifar10:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度の確認\n",
        "    print('Test Accuracy: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5lKzHvOwBxt",
        "outputId": "552a9227-7070-443a-cf6d-6e607066acc2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 79.9 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUメモリの解放\n",
        "del model, model_init\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "G_o3eFdXwBxt"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 学習と評価（100 epochs, init=kaiming_normal）"
      ],
      "metadata": {
        "id": "2atmPsCD5ThV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prune_rate=0.999"
      ],
      "metadata": {
        "id": "FVN0ulM5TFEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 刈り込み率を設定\n",
        "prune_rate = 0.999\n",
        "# 初期化手法を設定\n",
        "init = 'kaiming_normal'\n",
        "\n",
        "# デバイスを設定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# モデルを定義\n",
        "model = supermaskresnet18_10().to(device)\n",
        "# 学習前のモデルの重みを保存\n",
        "model_init = copy.deepcopy(model)\n",
        "\n",
        "# 学習率を設定\n",
        "learning_rate = 0.001\n",
        "# 全体のepoch数を設定\n",
        "num_epochs = 100\n",
        "# warm-upするepoch数を設定\n",
        "warmup_epochs = 10\n",
        "\n",
        "# 損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# オプティマイザーを設定\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# スケジューラーを設定\n",
        "scheduler = CosineLRScheduler(optimizer, t_initial=num_epochs, lr_min=1e-4, warmup_t=warmup_epochs, warmup_lr_init=1e-4, warmup_prefix=True)\n",
        "\n",
        "# モデルの学習\n",
        "for epoch in range(num_epochs):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, (images, labels) in enumerate(train_loader_cifar10):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # 順伝播\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # 逆伝播\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # 全データ数\n",
        "        total += labels.size(0)\n",
        "        # 正解数\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度を計算\n",
        "    train_acc = 100 * correct / total\n",
        "    # 正解率精度と損失を確認\n",
        "    print(\"Epoch [{}/{}], Train Accuracy: {} %, Loss: {:.4f}\".format(epoch+1, num_epochs, 100 * correct / total, loss.item()))\n",
        "    # 1エポック終了後にスケジューラーを更新\n",
        "    scheduler.step(epoch)\n",
        "    # 学習率の確認\n",
        "    print(\"Epoch [{}/{}], Learning Rate: {}\".format(epoch+1, num_epochs, optimizer.param_groups[0]['lr']))\n",
        "\n",
        "# モデルを保存\n",
        "torch.save(model, '/content/drive/MyDrive/SupermaskResNet18_CIFAR10_100epochs_pr0.999_normal_CLRS_restest.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fc76774-94fe-4f53-b235-a3a8dac4e6fd",
        "id": "IAZ58gS7TFEu"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Accuracy: 18.632 %, Loss: 2.0620\n",
            "Epoch [1/100], Learning Rate: 0.0001\n",
            "Epoch [2/100], Train Accuracy: 24.9 %, Loss: 2.0218\n",
            "Epoch [2/100], Learning Rate: 0.00019\n",
            "Epoch [3/100], Train Accuracy: 26.988 %, Loss: 1.9368\n",
            "Epoch [3/100], Learning Rate: 0.00028\n",
            "Epoch [4/100], Train Accuracy: 28.332 %, Loss: 2.0893\n",
            "Epoch [4/100], Learning Rate: 0.00036999999999999994\n",
            "Epoch [5/100], Train Accuracy: 26.486 %, Loss: 1.9775\n",
            "Epoch [5/100], Learning Rate: 0.00045999999999999996\n",
            "Epoch [6/100], Train Accuracy: 26.206 %, Loss: 2.1574\n",
            "Epoch [6/100], Learning Rate: 0.00055\n",
            "Epoch [7/100], Train Accuracy: 25.004 %, Loss: 1.9946\n",
            "Epoch [7/100], Learning Rate: 0.0006399999999999999\n",
            "Epoch [8/100], Train Accuracy: 26.336 %, Loss: 1.9649\n",
            "Epoch [8/100], Learning Rate: 0.00073\n",
            "Epoch [9/100], Train Accuracy: 26.258 %, Loss: 1.9484\n",
            "Epoch [9/100], Learning Rate: 0.00082\n",
            "Epoch [10/100], Train Accuracy: 27.818 %, Loss: 1.9542\n",
            "Epoch [10/100], Learning Rate: 0.00091\n",
            "Epoch [11/100], Train Accuracy: 28.508 %, Loss: 1.9826\n",
            "Epoch [11/100], Learning Rate: 0.001\n",
            "Epoch [12/100], Train Accuracy: 29.448 %, Loss: 1.8794\n",
            "Epoch [12/100], Learning Rate: 0.0009997779521645793\n",
            "Epoch [13/100], Train Accuracy: 30.096 %, Loss: 1.9477\n",
            "Epoch [13/100], Learning Rate: 0.0009991120277927223\n",
            "Epoch [14/100], Train Accuracy: 29.69 %, Loss: 1.9508\n",
            "Epoch [14/100], Learning Rate: 0.000998002884071386\n",
            "Epoch [15/100], Train Accuracy: 30.39 %, Loss: 1.8458\n",
            "Epoch [15/100], Learning Rate: 0.000996451615591515\n",
            "Epoch [16/100], Train Accuracy: 30.998 %, Loss: 1.9103\n",
            "Epoch [16/100], Learning Rate: 0.000994459753267812\n",
            "Epoch [17/100], Train Accuracy: 31.174 %, Loss: 1.8909\n",
            "Epoch [17/100], Learning Rate: 0.00099202926282791\n",
            "Epoch [18/100], Train Accuracy: 31.894 %, Loss: 1.9550\n",
            "Epoch [18/100], Learning Rate: 0.0009891625428724363\n",
            "Epoch [19/100], Train Accuracy: 32.182 %, Loss: 1.8170\n",
            "Epoch [19/100], Learning Rate: 0.000985862422507884\n",
            "Epoch [20/100], Train Accuracy: 32.204 %, Loss: 1.8751\n",
            "Epoch [20/100], Learning Rate: 0.0009821321585546244\n",
            "Epoch [21/100], Train Accuracy: 32.638 %, Loss: 1.8498\n",
            "Epoch [21/100], Learning Rate: 0.0009779754323328191\n",
            "Epoch [22/100], Train Accuracy: 32.436 %, Loss: 1.9183\n",
            "Epoch [22/100], Learning Rate: 0.0009733963460294015\n",
            "Epoch [23/100], Train Accuracy: 32.16 %, Loss: 1.9348\n",
            "Epoch [23/100], Learning Rate: 0.0009683994186497131\n",
            "Epoch [24/100], Train Accuracy: 32.64 %, Loss: 1.8149\n",
            "Epoch [24/100], Learning Rate: 0.0009629895815577914\n",
            "Epoch [25/100], Train Accuracy: 32.244 %, Loss: 1.8851\n",
            "Epoch [25/100], Learning Rate: 0.0009571721736097088\n",
            "Epoch [26/100], Train Accuracy: 32.498 %, Loss: 1.8303\n",
            "Epoch [26/100], Learning Rate: 0.0009509529358847655\n",
            "Epoch [27/100], Train Accuracy: 32.83 %, Loss: 1.8707\n",
            "Epoch [27/100], Learning Rate: 0.0009443380060197386\n",
            "Epoch [28/100], Train Accuracy: 32.8 %, Loss: 1.7108\n",
            "Epoch [28/100], Learning Rate: 0.0009373339121517746\n",
            "Epoch [29/100], Train Accuracy: 33.164 %, Loss: 1.9800\n",
            "Epoch [29/100], Learning Rate: 0.0009299475664759069\n",
            "Epoch [30/100], Train Accuracy: 32.922 %, Loss: 1.8352\n",
            "Epoch [30/100], Learning Rate: 0.0009221862584235528\n",
            "Epoch [31/100], Train Accuracy: 32.854 %, Loss: 1.8132\n",
            "Epoch [31/100], Learning Rate: 0.0009140576474687263\n",
            "Epoch [32/100], Train Accuracy: 32.74 %, Loss: 1.8182\n",
            "Epoch [32/100], Learning Rate: 0.0009055697555690606\n",
            "Epoch [33/100], Train Accuracy: 33.504 %, Loss: 1.7822\n",
            "Epoch [33/100], Learning Rate: 0.0008967309592491052\n",
            "Epoch [34/100], Train Accuracy: 32.984 %, Loss: 1.7670\n",
            "Epoch [34/100], Learning Rate: 0.0008875499813337069\n",
            "Epoch [35/100], Train Accuracy: 33.434 %, Loss: 2.0299\n",
            "Epoch [35/100], Learning Rate: 0.0008780358823396353\n",
            "Epoch [36/100], Train Accuracy: 33.108 %, Loss: 1.8002\n",
            "Epoch [36/100], Learning Rate: 0.0008681980515339464\n",
            "Epoch [37/100], Train Accuracy: 32.82 %, Loss: 1.7691\n",
            "Epoch [37/100], Learning Rate: 0.0008580461976679099\n",
            "Epoch [38/100], Train Accuracy: 32.936 %, Loss: 1.8815\n",
            "Epoch [38/100], Learning Rate: 0.0008475903393956434\n",
            "Epoch [39/100], Train Accuracy: 32.982 %, Loss: 1.7668\n",
            "Epoch [39/100], Learning Rate: 0.0008368407953869105\n",
            "Epoch [40/100], Train Accuracy: 33.136 %, Loss: 1.8080\n",
            "Epoch [40/100], Learning Rate: 0.0008258081741438395\n",
            "Epoch [41/100], Train Accuracy: 33.39 %, Loss: 1.7972\n",
            "Epoch [41/100], Learning Rate: 0.0008145033635316131\n",
            "Epoch [42/100], Train Accuracy: 33.308 %, Loss: 1.7837\n",
            "Epoch [42/100], Learning Rate: 0.0008029375200334589\n",
            "Epoch [43/100], Train Accuracy: 33.384 %, Loss: 1.7877\n",
            "Epoch [43/100], Learning Rate: 0.0007911220577405485\n",
            "Epoch [44/100], Train Accuracy: 33.11 %, Loss: 1.8339\n",
            "Epoch [44/100], Learning Rate: 0.0007790686370876671\n",
            "Epoch [45/100], Train Accuracy: 33.46 %, Loss: 1.7525\n",
            "Epoch [45/100], Learning Rate: 0.0007667891533457719\n",
            "Epoch [46/100], Train Accuracy: 33.114 %, Loss: 1.8062\n",
            "Epoch [46/100], Learning Rate: 0.000754295724882796\n",
            "Epoch [47/100], Train Accuracy: 33.732 %, Loss: 1.9293\n",
            "Epoch [47/100], Learning Rate: 0.0007416006812042827\n",
            "Epoch [48/100], Train Accuracy: 33.742 %, Loss: 1.7625\n",
            "Epoch [48/100], Learning Rate: 0.0007287165507856513\n",
            "Epoch [49/100], Train Accuracy: 34.26 %, Loss: 1.8338\n",
            "Epoch [49/100], Learning Rate: 0.0007156560487081052\n",
            "Epoch [50/100], Train Accuracy: 33.486 %, Loss: 1.7658\n",
            "Epoch [50/100], Learning Rate: 0.0007024320641103813\n",
            "Epoch [51/100], Train Accuracy: 34.284 %, Loss: 1.6890\n",
            "Epoch [51/100], Learning Rate: 0.0006890576474687264\n",
            "Epoch [52/100], Train Accuracy: 33.746 %, Loss: 1.8393\n",
            "Epoch [52/100], Learning Rate: 0.0006755459977176532\n",
            "Epoch [53/100], Train Accuracy: 33.986 %, Loss: 1.7623\n",
            "Epoch [53/100], Learning Rate: 0.0006619104492241846\n",
            "Epoch [54/100], Train Accuracy: 34.01 %, Loss: 1.7631\n",
            "Epoch [54/100], Learning Rate: 0.0006481644586284443\n",
            "Epoch [55/100], Train Accuracy: 34.372 %, Loss: 1.7613\n",
            "Epoch [55/100], Learning Rate: 0.0006343215915635762\n",
            "Epoch [56/100], Train Accuracy: 34.012 %, Loss: 1.7846\n",
            "Epoch [56/100], Learning Rate: 0.000620395509268104\n",
            "Epoch [57/100], Train Accuracy: 33.776 %, Loss: 1.8082\n",
            "Epoch [57/100], Learning Rate: 0.000606399955103937\n",
            "Epoch [58/100], Train Accuracy: 33.778 %, Loss: 1.7791\n",
            "Epoch [58/100], Learning Rate: 0.0005923487409933315\n",
            "Epoch [59/100], Train Accuracy: 33.788 %, Loss: 1.6986\n",
            "Epoch [59/100], Learning Rate: 0.0005782557337881911\n",
            "Epoch [60/100], Train Accuracy: 33.672 %, Loss: 1.8216\n",
            "Epoch [60/100], Learning Rate: 0.0005641348415851578\n",
            "Epoch [61/100], Train Accuracy: 33.826 %, Loss: 1.8300\n",
            "Epoch [61/100], Learning Rate: 0.00055\n",
            "Epoch [62/100], Train Accuracy: 34.064 %, Loss: 1.7768\n",
            "Epoch [62/100], Learning Rate: 0.0005358651584148423\n",
            "Epoch [63/100], Train Accuracy: 34.304 %, Loss: 1.7467\n",
            "Epoch [63/100], Learning Rate: 0.000521744266211809\n",
            "Epoch [64/100], Train Accuracy: 34.2 %, Loss: 1.9445\n",
            "Epoch [64/100], Learning Rate: 0.0005076512590066686\n",
            "Epoch [65/100], Train Accuracy: 33.938 %, Loss: 1.7982\n",
            "Epoch [65/100], Learning Rate: 0.0004936000448960632\n",
            "Epoch [66/100], Train Accuracy: 33.998 %, Loss: 1.7614\n",
            "Epoch [66/100], Learning Rate: 0.0004796044907318961\n",
            "Epoch [67/100], Train Accuracy: 33.932 %, Loss: 1.8094\n",
            "Epoch [67/100], Learning Rate: 0.0004656784084364239\n",
            "Epoch [68/100], Train Accuracy: 34.042 %, Loss: 1.7075\n",
            "Epoch [68/100], Learning Rate: 0.00045183554137155597\n",
            "Epoch [69/100], Train Accuracy: 33.804 %, Loss: 2.0505\n",
            "Epoch [69/100], Learning Rate: 0.0004380895507758153\n",
            "Epoch [70/100], Train Accuracy: 34.398 %, Loss: 1.7653\n",
            "Epoch [70/100], Learning Rate: 0.00042445400228234684\n",
            "Epoch [71/100], Train Accuracy: 34.202 %, Loss: 1.7710\n",
            "Epoch [71/100], Learning Rate: 0.0004109423525312738\n",
            "Epoch [72/100], Train Accuracy: 34.144 %, Loss: 1.8258\n",
            "Epoch [72/100], Learning Rate: 0.0003975679358896189\n",
            "Epoch [73/100], Train Accuracy: 34.76 %, Loss: 1.7515\n",
            "Epoch [73/100], Learning Rate: 0.000384343951291895\n",
            "Epoch [74/100], Train Accuracy: 34.416 %, Loss: 1.8625\n",
            "Epoch [74/100], Learning Rate: 0.00037128344921434864\n",
            "Epoch [75/100], Train Accuracy: 34.382 %, Loss: 1.8388\n",
            "Epoch [75/100], Learning Rate: 0.0003583993187957173\n",
            "Epoch [76/100], Train Accuracy: 34.2 %, Loss: 1.7914\n",
            "Epoch [76/100], Learning Rate: 0.00034570427511720393\n",
            "Epoch [77/100], Train Accuracy: 34.606 %, Loss: 1.8792\n",
            "Epoch [77/100], Learning Rate: 0.0003332108466542281\n",
            "Epoch [78/100], Train Accuracy: 34.322 %, Loss: 1.7402\n",
            "Epoch [78/100], Learning Rate: 0.0003209313629123329\n",
            "Epoch [79/100], Train Accuracy: 34.514 %, Loss: 1.8158\n",
            "Epoch [79/100], Learning Rate: 0.0003088779422594514\n",
            "Epoch [80/100], Train Accuracy: 34.638 %, Loss: 1.7625\n",
            "Epoch [80/100], Learning Rate: 0.0002970624799665412\n",
            "Epoch [81/100], Train Accuracy: 34.564 %, Loss: 1.7778\n",
            "Epoch [81/100], Learning Rate: 0.00028549663646838715\n",
            "Epoch [82/100], Train Accuracy: 34.334 %, Loss: 1.7788\n",
            "Epoch [82/100], Learning Rate: 0.00027419182585616055\n",
            "Epoch [83/100], Train Accuracy: 34.522 %, Loss: 1.7204\n",
            "Epoch [83/100], Learning Rate: 0.0002631592046130896\n",
            "Epoch [84/100], Train Accuracy: 34.226 %, Loss: 1.7942\n",
            "Epoch [84/100], Learning Rate: 0.00025240966060435674\n",
            "Epoch [85/100], Train Accuracy: 34.39 %, Loss: 1.7704\n",
            "Epoch [85/100], Learning Rate: 0.00024195380233209008\n",
            "Epoch [86/100], Train Accuracy: 34.826 %, Loss: 1.9361\n",
            "Epoch [86/100], Learning Rate: 0.00023180194846605365\n",
            "Epoch [87/100], Train Accuracy: 34.164 %, Loss: 1.7175\n",
            "Epoch [87/100], Learning Rate: 0.0002219641176603649\n",
            "Epoch [88/100], Train Accuracy: 34.632 %, Loss: 1.7879\n",
            "Epoch [88/100], Learning Rate: 0.0002124500186662932\n",
            "Epoch [89/100], Train Accuracy: 34.438 %, Loss: 2.0690\n",
            "Epoch [89/100], Learning Rate: 0.00020326904075089488\n",
            "Epoch [90/100], Train Accuracy: 34.58 %, Loss: 1.7910\n",
            "Epoch [90/100], Learning Rate: 0.00019443024443093932\n",
            "Epoch [91/100], Train Accuracy: 34.328 %, Loss: 1.7270\n",
            "Epoch [91/100], Learning Rate: 0.00018594235253127368\n",
            "Epoch [92/100], Train Accuracy: 34.54 %, Loss: 1.7099\n",
            "Epoch [92/100], Learning Rate: 0.00017781374157644722\n",
            "Epoch [93/100], Train Accuracy: 34.336 %, Loss: 1.7987\n",
            "Epoch [93/100], Learning Rate: 0.00017005243352409332\n",
            "Epoch [94/100], Train Accuracy: 34.554 %, Loss: 1.7472\n",
            "Epoch [94/100], Learning Rate: 0.0001626660878482253\n",
            "Epoch [95/100], Train Accuracy: 34.89 %, Loss: 1.8429\n",
            "Epoch [95/100], Learning Rate: 0.0001556619939802614\n",
            "Epoch [96/100], Train Accuracy: 34.86 %, Loss: 1.6985\n",
            "Epoch [96/100], Learning Rate: 0.0001490470641152345\n",
            "Epoch [97/100], Train Accuracy: 34.522 %, Loss: 1.6897\n",
            "Epoch [97/100], Learning Rate: 0.0001428278263902913\n",
            "Epoch [98/100], Train Accuracy: 34.86 %, Loss: 1.7149\n",
            "Epoch [98/100], Learning Rate: 0.00013701041844220858\n",
            "Epoch [99/100], Train Accuracy: 35.002 %, Loss: 1.6768\n",
            "Epoch [99/100], Learning Rate: 0.0001316005813502869\n",
            "Epoch [100/100], Train Accuracy: 34.718 %, Loss: 1.7628\n",
            "Epoch [100/100], Learning Rate: 0.00012660365397059855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習前後のモデルの重みを比較する関数を作成\n",
        "def check_weight_change(model, model_init):\n",
        "    weights_changed = (model.conv1.state_dict()['weight'] != model_init.conv1.state_dict()['weight']).any()\n",
        "    if weights_changed:\n",
        "        return print('モデルの重みが変化しています')\n",
        "    else:\n",
        "        return print('モデルの重みは変化していません')\n",
        "# 学習前後でモデルの重みが変化していないかを確認\n",
        "check_weight_change(model, model_init)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e45e6371-075d-408e-c2bd-7536efdf6152",
        "id": "eeSaJGVWTFEv"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "モデルの重みは変化していません\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの評価\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for images, labels in test_loader_cifar10:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度の確認\n",
        "    print('Test Accuracy: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcd80a7b-e5ba-4fc7-ba6c-9324f3b583bb",
        "id": "IzCiczOTTFEv"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 32.27 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUメモリの解放\n",
        "del model, model_init\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "4maNhw9ITFEv"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prune_rate=0.995"
      ],
      "metadata": {
        "id": "wc65N7AtS-xa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 刈り込み率を設定\n",
        "prune_rate = 0.995\n",
        "# 初期化手法を設定\n",
        "init = 'kaiming_normal'\n",
        "\n",
        "# デバイスを設定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# モデルを定義\n",
        "model = supermaskresnet18_10().to(device)\n",
        "# 学習前のモデルの重みを保存\n",
        "model_init = copy.deepcopy(model)\n",
        "\n",
        "# 学習率を設定\n",
        "learning_rate = 0.001\n",
        "# 全体のepoch数を設定\n",
        "num_epochs = 100\n",
        "# warm-upするepoch数を設定\n",
        "warmup_epochs = 10\n",
        "\n",
        "# 損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# オプティマイザーを設定\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# スケジューラーを設定\n",
        "scheduler = CosineLRScheduler(optimizer, t_initial=num_epochs, lr_min=1e-4, warmup_t=warmup_epochs, warmup_lr_init=1e-4, warmup_prefix=True)\n",
        "\n",
        "# モデルの学習\n",
        "for epoch in range(num_epochs):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, (images, labels) in enumerate(train_loader_cifar10):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # 順伝播\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # 逆伝播\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # 全データ数\n",
        "        total += labels.size(0)\n",
        "        # 正解数\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度を計算\n",
        "    train_acc = 100 * correct / total\n",
        "    # 正解率精度と損失を確認\n",
        "    print(\"Epoch [{}/{}], Train Accuracy: {} %, Loss: {:.4f}\".format(epoch+1, num_epochs, 100 * correct / total, loss.item()))\n",
        "    # 1エポック終了後にスケジューラーを更新\n",
        "    scheduler.step(epoch)\n",
        "    # 学習率の確認\n",
        "    print(\"Epoch [{}/{}], Learning Rate: {}\".format(epoch+1, num_epochs, optimizer.param_groups[0]['lr']))\n",
        "\n",
        "# モデルを保存\n",
        "torch.save(model, '/content/drive/MyDrive/SupermaskResNet18_CIFAR10_100epochs_pr0.995_normal_CLRS_restest.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43d034be-200f-43f4-e4c5-2d2eb2b86861",
        "id": "_z0RytJPS-xb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Accuracy: 24.23 %, Loss: 1.8979\n",
            "Epoch [1/100], Learning Rate: 0.0001\n",
            "Epoch [2/100], Train Accuracy: 31.434 %, Loss: 1.7993\n",
            "Epoch [2/100], Learning Rate: 0.00019\n",
            "Epoch [3/100], Train Accuracy: 33.608 %, Loss: 1.7704\n",
            "Epoch [3/100], Learning Rate: 0.00028\n",
            "Epoch [4/100], Train Accuracy: 34.036 %, Loss: 1.8683\n",
            "Epoch [4/100], Learning Rate: 0.00036999999999999994\n",
            "Epoch [5/100], Train Accuracy: 36.302 %, Loss: 1.6548\n",
            "Epoch [5/100], Learning Rate: 0.00045999999999999996\n",
            "Epoch [6/100], Train Accuracy: 37.766 %, Loss: 1.6149\n",
            "Epoch [6/100], Learning Rate: 0.00055\n",
            "Epoch [7/100], Train Accuracy: 38.42 %, Loss: 1.6976\n",
            "Epoch [7/100], Learning Rate: 0.0006399999999999999\n",
            "Epoch [8/100], Train Accuracy: 38.774 %, Loss: 1.6143\n",
            "Epoch [8/100], Learning Rate: 0.00073\n",
            "Epoch [9/100], Train Accuracy: 39.32 %, Loss: 1.6270\n",
            "Epoch [9/100], Learning Rate: 0.00082\n",
            "Epoch [10/100], Train Accuracy: 40.106 %, Loss: 1.6412\n",
            "Epoch [10/100], Learning Rate: 0.00091\n",
            "Epoch [11/100], Train Accuracy: 40.522 %, Loss: 1.6779\n",
            "Epoch [11/100], Learning Rate: 0.001\n",
            "Epoch [12/100], Train Accuracy: 41.34 %, Loss: 1.7515\n",
            "Epoch [12/100], Learning Rate: 0.0009997779521645793\n",
            "Epoch [13/100], Train Accuracy: 41.372 %, Loss: 1.5928\n",
            "Epoch [13/100], Learning Rate: 0.0009991120277927223\n",
            "Epoch [14/100], Train Accuracy: 42.338 %, Loss: 1.4646\n",
            "Epoch [14/100], Learning Rate: 0.000998002884071386\n",
            "Epoch [15/100], Train Accuracy: 42.584 %, Loss: 1.7210\n",
            "Epoch [15/100], Learning Rate: 0.000996451615591515\n",
            "Epoch [16/100], Train Accuracy: 43.004 %, Loss: 1.6506\n",
            "Epoch [16/100], Learning Rate: 0.000994459753267812\n",
            "Epoch [17/100], Train Accuracy: 43.504 %, Loss: 1.4096\n",
            "Epoch [17/100], Learning Rate: 0.00099202926282791\n",
            "Epoch [18/100], Train Accuracy: 44.204 %, Loss: 1.4315\n",
            "Epoch [18/100], Learning Rate: 0.0009891625428724363\n",
            "Epoch [19/100], Train Accuracy: 43.976 %, Loss: 1.4609\n",
            "Epoch [19/100], Learning Rate: 0.000985862422507884\n",
            "Epoch [20/100], Train Accuracy: 44.982 %, Loss: 1.5550\n",
            "Epoch [20/100], Learning Rate: 0.0009821321585546244\n",
            "Epoch [21/100], Train Accuracy: 45.372 %, Loss: 1.4563\n",
            "Epoch [21/100], Learning Rate: 0.0009779754323328191\n",
            "Epoch [22/100], Train Accuracy: 45.648 %, Loss: 1.5331\n",
            "Epoch [22/100], Learning Rate: 0.0009733963460294015\n",
            "Epoch [23/100], Train Accuracy: 45.726 %, Loss: 1.5456\n",
            "Epoch [23/100], Learning Rate: 0.0009683994186497131\n",
            "Epoch [24/100], Train Accuracy: 45.632 %, Loss: 1.4676\n",
            "Epoch [24/100], Learning Rate: 0.0009629895815577914\n",
            "Epoch [25/100], Train Accuracy: 45.91 %, Loss: 1.5036\n",
            "Epoch [25/100], Learning Rate: 0.0009571721736097088\n",
            "Epoch [26/100], Train Accuracy: 46.424 %, Loss: 1.4637\n",
            "Epoch [26/100], Learning Rate: 0.0009509529358847655\n",
            "Epoch [27/100], Train Accuracy: 46.298 %, Loss: 1.4776\n",
            "Epoch [27/100], Learning Rate: 0.0009443380060197386\n",
            "Epoch [28/100], Train Accuracy: 46.408 %, Loss: 1.5350\n",
            "Epoch [28/100], Learning Rate: 0.0009373339121517746\n",
            "Epoch [29/100], Train Accuracy: 46.714 %, Loss: 1.3381\n",
            "Epoch [29/100], Learning Rate: 0.0009299475664759069\n",
            "Epoch [30/100], Train Accuracy: 46.992 %, Loss: 1.3921\n",
            "Epoch [30/100], Learning Rate: 0.0009221862584235528\n",
            "Epoch [31/100], Train Accuracy: 47.498 %, Loss: 1.4511\n",
            "Epoch [31/100], Learning Rate: 0.0009140576474687263\n",
            "Epoch [32/100], Train Accuracy: 46.838 %, Loss: 1.4432\n",
            "Epoch [32/100], Learning Rate: 0.0009055697555690606\n",
            "Epoch [33/100], Train Accuracy: 47.094 %, Loss: 1.5381\n",
            "Epoch [33/100], Learning Rate: 0.0008967309592491052\n",
            "Epoch [34/100], Train Accuracy: 47.424 %, Loss: 1.3366\n",
            "Epoch [34/100], Learning Rate: 0.0008875499813337069\n",
            "Epoch [35/100], Train Accuracy: 47.802 %, Loss: 1.4665\n",
            "Epoch [35/100], Learning Rate: 0.0008780358823396353\n",
            "Epoch [36/100], Train Accuracy: 48.032 %, Loss: 1.5945\n",
            "Epoch [36/100], Learning Rate: 0.0008681980515339464\n",
            "Epoch [37/100], Train Accuracy: 47.718 %, Loss: 1.3590\n",
            "Epoch [37/100], Learning Rate: 0.0008580461976679099\n",
            "Epoch [38/100], Train Accuracy: 47.944 %, Loss: 1.4545\n",
            "Epoch [38/100], Learning Rate: 0.0008475903393956434\n",
            "Epoch [39/100], Train Accuracy: 48.65 %, Loss: 1.4175\n",
            "Epoch [39/100], Learning Rate: 0.0008368407953869105\n",
            "Epoch [40/100], Train Accuracy: 48.384 %, Loss: 1.3786\n",
            "Epoch [40/100], Learning Rate: 0.0008258081741438395\n",
            "Epoch [41/100], Train Accuracy: 48.946 %, Loss: 1.4485\n",
            "Epoch [41/100], Learning Rate: 0.0008145033635316131\n",
            "Epoch [42/100], Train Accuracy: 48.842 %, Loss: 1.3238\n",
            "Epoch [42/100], Learning Rate: 0.0008029375200334589\n",
            "Epoch [43/100], Train Accuracy: 49.108 %, Loss: 1.3514\n",
            "Epoch [43/100], Learning Rate: 0.0007911220577405485\n",
            "Epoch [44/100], Train Accuracy: 48.888 %, Loss: 1.4104\n",
            "Epoch [44/100], Learning Rate: 0.0007790686370876671\n",
            "Epoch [45/100], Train Accuracy: 49.434 %, Loss: 1.4839\n",
            "Epoch [45/100], Learning Rate: 0.0007667891533457719\n",
            "Epoch [46/100], Train Accuracy: 49.216 %, Loss: 1.3501\n",
            "Epoch [46/100], Learning Rate: 0.000754295724882796\n",
            "Epoch [47/100], Train Accuracy: 49.684 %, Loss: 1.3535\n",
            "Epoch [47/100], Learning Rate: 0.0007416006812042827\n",
            "Epoch [48/100], Train Accuracy: 49.468 %, Loss: 1.2863\n",
            "Epoch [48/100], Learning Rate: 0.0007287165507856513\n",
            "Epoch [49/100], Train Accuracy: 49.508 %, Loss: 1.4633\n",
            "Epoch [49/100], Learning Rate: 0.0007156560487081052\n",
            "Epoch [50/100], Train Accuracy: 49.206 %, Loss: 1.3913\n",
            "Epoch [50/100], Learning Rate: 0.0007024320641103813\n",
            "Epoch [51/100], Train Accuracy: 49.784 %, Loss: 1.2512\n",
            "Epoch [51/100], Learning Rate: 0.0006890576474687264\n",
            "Epoch [52/100], Train Accuracy: 49.67 %, Loss: 1.5486\n",
            "Epoch [52/100], Learning Rate: 0.0006755459977176532\n",
            "Epoch [53/100], Train Accuracy: 50.068 %, Loss: 1.3185\n",
            "Epoch [53/100], Learning Rate: 0.0006619104492241846\n",
            "Epoch [54/100], Train Accuracy: 49.766 %, Loss: 1.4793\n",
            "Epoch [54/100], Learning Rate: 0.0006481644586284443\n",
            "Epoch [55/100], Train Accuracy: 50.144 %, Loss: 1.4416\n",
            "Epoch [55/100], Learning Rate: 0.0006343215915635762\n",
            "Epoch [56/100], Train Accuracy: 50.456 %, Loss: 1.5521\n",
            "Epoch [56/100], Learning Rate: 0.000620395509268104\n",
            "Epoch [57/100], Train Accuracy: 49.878 %, Loss: 1.4846\n",
            "Epoch [57/100], Learning Rate: 0.000606399955103937\n",
            "Epoch [58/100], Train Accuracy: 49.95 %, Loss: 1.3176\n",
            "Epoch [58/100], Learning Rate: 0.0005923487409933315\n",
            "Epoch [59/100], Train Accuracy: 50.332 %, Loss: 1.4111\n",
            "Epoch [59/100], Learning Rate: 0.0005782557337881911\n",
            "Epoch [60/100], Train Accuracy: 49.96 %, Loss: 1.4430\n",
            "Epoch [60/100], Learning Rate: 0.0005641348415851578\n",
            "Epoch [61/100], Train Accuracy: 50.358 %, Loss: 1.3554\n",
            "Epoch [61/100], Learning Rate: 0.00055\n",
            "Epoch [62/100], Train Accuracy: 50.216 %, Loss: 1.3525\n",
            "Epoch [62/100], Learning Rate: 0.0005358651584148423\n",
            "Epoch [63/100], Train Accuracy: 51.064 %, Loss: 1.3768\n",
            "Epoch [63/100], Learning Rate: 0.000521744266211809\n",
            "Epoch [64/100], Train Accuracy: 50.526 %, Loss: 1.3809\n",
            "Epoch [64/100], Learning Rate: 0.0005076512590066686\n",
            "Epoch [65/100], Train Accuracy: 50.314 %, Loss: 1.3362\n",
            "Epoch [65/100], Learning Rate: 0.0004936000448960632\n",
            "Epoch [66/100], Train Accuracy: 51.172 %, Loss: 1.3468\n",
            "Epoch [66/100], Learning Rate: 0.0004796044907318961\n",
            "Epoch [67/100], Train Accuracy: 50.74 %, Loss: 1.3353\n",
            "Epoch [67/100], Learning Rate: 0.0004656784084364239\n",
            "Epoch [68/100], Train Accuracy: 50.954 %, Loss: 1.4057\n",
            "Epoch [68/100], Learning Rate: 0.00045183554137155597\n",
            "Epoch [69/100], Train Accuracy: 51.018 %, Loss: 1.4465\n",
            "Epoch [69/100], Learning Rate: 0.0004380895507758153\n",
            "Epoch [70/100], Train Accuracy: 51.232 %, Loss: 1.3511\n",
            "Epoch [70/100], Learning Rate: 0.00042445400228234684\n",
            "Epoch [71/100], Train Accuracy: 51.012 %, Loss: 1.3829\n",
            "Epoch [71/100], Learning Rate: 0.0004109423525312738\n",
            "Epoch [72/100], Train Accuracy: 51.156 %, Loss: 1.4978\n",
            "Epoch [72/100], Learning Rate: 0.0003975679358896189\n",
            "Epoch [73/100], Train Accuracy: 51.348 %, Loss: 1.2594\n",
            "Epoch [73/100], Learning Rate: 0.000384343951291895\n",
            "Epoch [74/100], Train Accuracy: 51.32 %, Loss: 1.4000\n",
            "Epoch [74/100], Learning Rate: 0.00037128344921434864\n",
            "Epoch [75/100], Train Accuracy: 51.758 %, Loss: 1.4800\n",
            "Epoch [75/100], Learning Rate: 0.0003583993187957173\n",
            "Epoch [76/100], Train Accuracy: 51.308 %, Loss: 1.3548\n",
            "Epoch [76/100], Learning Rate: 0.00034570427511720393\n",
            "Epoch [77/100], Train Accuracy: 51.784 %, Loss: 1.4327\n",
            "Epoch [77/100], Learning Rate: 0.0003332108466542281\n",
            "Epoch [78/100], Train Accuracy: 51.598 %, Loss: 1.3071\n",
            "Epoch [78/100], Learning Rate: 0.0003209313629123329\n",
            "Epoch [79/100], Train Accuracy: 52.192 %, Loss: 1.2779\n",
            "Epoch [79/100], Learning Rate: 0.0003088779422594514\n",
            "Epoch [80/100], Train Accuracy: 51.922 %, Loss: 1.3349\n",
            "Epoch [80/100], Learning Rate: 0.0002970624799665412\n",
            "Epoch [81/100], Train Accuracy: 52.048 %, Loss: 1.3229\n",
            "Epoch [81/100], Learning Rate: 0.00028549663646838715\n",
            "Epoch [82/100], Train Accuracy: 52.078 %, Loss: 1.2642\n",
            "Epoch [82/100], Learning Rate: 0.00027419182585616055\n",
            "Epoch [83/100], Train Accuracy: 52.544 %, Loss: 1.4142\n",
            "Epoch [83/100], Learning Rate: 0.0002631592046130896\n",
            "Epoch [84/100], Train Accuracy: 52.204 %, Loss: 1.2037\n",
            "Epoch [84/100], Learning Rate: 0.00025240966060435674\n",
            "Epoch [85/100], Train Accuracy: 52.198 %, Loss: 1.3904\n",
            "Epoch [85/100], Learning Rate: 0.00024195380233209008\n",
            "Epoch [86/100], Train Accuracy: 51.9 %, Loss: 1.4367\n",
            "Epoch [86/100], Learning Rate: 0.00023180194846605365\n",
            "Epoch [87/100], Train Accuracy: 52.34 %, Loss: 1.3830\n",
            "Epoch [87/100], Learning Rate: 0.0002219641176603649\n",
            "Epoch [88/100], Train Accuracy: 52.464 %, Loss: 1.3040\n",
            "Epoch [88/100], Learning Rate: 0.0002124500186662932\n",
            "Epoch [89/100], Train Accuracy: 52.586 %, Loss: 1.2878\n",
            "Epoch [89/100], Learning Rate: 0.00020326904075089488\n",
            "Epoch [90/100], Train Accuracy: 52.444 %, Loss: 1.3786\n",
            "Epoch [90/100], Learning Rate: 0.00019443024443093932\n",
            "Epoch [91/100], Train Accuracy: 52.25 %, Loss: 1.2855\n",
            "Epoch [91/100], Learning Rate: 0.00018594235253127368\n",
            "Epoch [92/100], Train Accuracy: 52.324 %, Loss: 1.4352\n",
            "Epoch [92/100], Learning Rate: 0.00017781374157644722\n",
            "Epoch [93/100], Train Accuracy: 52.698 %, Loss: 1.3014\n",
            "Epoch [93/100], Learning Rate: 0.00017005243352409332\n",
            "Epoch [94/100], Train Accuracy: 52.274 %, Loss: 1.3767\n",
            "Epoch [94/100], Learning Rate: 0.0001626660878482253\n",
            "Epoch [95/100], Train Accuracy: 52.488 %, Loss: 1.3276\n",
            "Epoch [95/100], Learning Rate: 0.0001556619939802614\n",
            "Epoch [96/100], Train Accuracy: 52.572 %, Loss: 1.3516\n",
            "Epoch [96/100], Learning Rate: 0.0001490470641152345\n",
            "Epoch [97/100], Train Accuracy: 52.878 %, Loss: 1.2503\n",
            "Epoch [97/100], Learning Rate: 0.0001428278263902913\n",
            "Epoch [98/100], Train Accuracy: 52.618 %, Loss: 1.2434\n",
            "Epoch [98/100], Learning Rate: 0.00013701041844220858\n",
            "Epoch [99/100], Train Accuracy: 53.364 %, Loss: 1.3569\n",
            "Epoch [99/100], Learning Rate: 0.0001316005813502869\n",
            "Epoch [100/100], Train Accuracy: 53.338 %, Loss: 1.2829\n",
            "Epoch [100/100], Learning Rate: 0.00012660365397059855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習前後のモデルの重みを比較する関数を作成\n",
        "def check_weight_change(model, model_init):\n",
        "    weights_changed = (model.conv1.state_dict()['weight'] != model_init.conv1.state_dict()['weight']).any()\n",
        "    if weights_changed:\n",
        "        return print('モデルの重みが変化しています')\n",
        "    else:\n",
        "        return print('モデルの重みは変化していません')\n",
        "# 学習前後でモデルの重みが変化していないかを確認\n",
        "check_weight_change(model, model_init)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edbf4656-0aac-4b59-d040-e2a82bb88eee",
        "id": "-tVbfo91S-xb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "モデルの重みは変化していません\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの評価\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for images, labels in test_loader_cifar10:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度の確認\n",
        "    print('Test Accuracy: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b256ca9d-8857-4929-d7f3-46d653a330a9",
        "id": "K_KCq6QpS-xb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 45.68 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUメモリの解放\n",
        "del model, model_init\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "jWE2J8kES-xb"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prune_rate=0.99"
      ],
      "metadata": {
        "id": "89pXntXAk7YN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 刈り込み率を設定\n",
        "prune_rate = 0.99\n",
        "# 初期化手法を設定\n",
        "init = 'kaiming_normal'\n",
        "\n",
        "# デバイスを設定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# モデルを定義\n",
        "model = supermaskresnet18_10().to(device)\n",
        "# 学習前のモデルの重みを保存\n",
        "model_init = copy.deepcopy(model)\n",
        "\n",
        "# 学習率を設定\n",
        "learning_rate = 0.001\n",
        "# 全体のepoch数を設定\n",
        "num_epochs = 100\n",
        "# warm-upするepoch数を設定\n",
        "warmup_epochs = 10\n",
        "\n",
        "# 損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# オプティマイザーを設定\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# スケジューラーを設定\n",
        "scheduler = CosineLRScheduler(optimizer, t_initial=num_epochs, lr_min=1e-4, warmup_t=warmup_epochs, warmup_lr_init=1e-4, warmup_prefix=True)\n",
        "\n",
        "# モデルの学習\n",
        "for epoch in range(num_epochs):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, (images, labels) in enumerate(train_loader_cifar10):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # 順伝播\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # 逆伝播\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # 全データ数\n",
        "        total += labels.size(0)\n",
        "        # 正解数\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度を計算\n",
        "    train_acc = 100 * correct / total\n",
        "    # 正解率精度と損失を確認\n",
        "    print(\"Epoch [{}/{}], Train Accuracy: {} %, Loss: {:.4f}\".format(epoch+1, num_epochs, 100 * correct / total, loss.item()))\n",
        "    # 1エポック終了後にスケジューラーを更新\n",
        "    scheduler.step(epoch)\n",
        "    # 学習率の確認\n",
        "    print(\"Epoch [{}/{}], Learning Rate: {}\".format(epoch+1, num_epochs, optimizer.param_groups[0]['lr']))\n",
        "\n",
        "# モデルを保存\n",
        "torch.save(model, '/content/drive/MyDrive/SupermaskResNet18_CIFAR10_100epochs_pr0.99_normal_CLRS_restest.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abab9024-2cdc-4091-bcae-dd37b644669e",
        "id": "d5fZU7Fuk7Yc"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Accuracy: 28.488 %, Loss: 1.7739\n",
            "Epoch [1/100], Learning Rate: 0.0001\n",
            "Epoch [2/100], Train Accuracy: 35.826 %, Loss: 1.7363\n",
            "Epoch [2/100], Learning Rate: 0.00019\n",
            "Epoch [3/100], Train Accuracy: 37.812 %, Loss: 1.5748\n",
            "Epoch [3/100], Learning Rate: 0.00028\n",
            "Epoch [4/100], Train Accuracy: 39.322 %, Loss: 1.7079\n",
            "Epoch [4/100], Learning Rate: 0.00036999999999999994\n",
            "Epoch [5/100], Train Accuracy: 40.548 %, Loss: 1.5482\n",
            "Epoch [5/100], Learning Rate: 0.00045999999999999996\n",
            "Epoch [6/100], Train Accuracy: 41.68 %, Loss: 1.5948\n",
            "Epoch [6/100], Learning Rate: 0.00055\n",
            "Epoch [7/100], Train Accuracy: 42.786 %, Loss: 1.6034\n",
            "Epoch [7/100], Learning Rate: 0.0006399999999999999\n",
            "Epoch [8/100], Train Accuracy: 43.138 %, Loss: 1.4458\n",
            "Epoch [8/100], Learning Rate: 0.00073\n",
            "Epoch [9/100], Train Accuracy: 44.1 %, Loss: 1.5398\n",
            "Epoch [9/100], Learning Rate: 0.00082\n",
            "Epoch [10/100], Train Accuracy: 45.064 %, Loss: 1.4486\n",
            "Epoch [10/100], Learning Rate: 0.00091\n",
            "Epoch [11/100], Train Accuracy: 46.482 %, Loss: 1.5367\n",
            "Epoch [11/100], Learning Rate: 0.001\n",
            "Epoch [12/100], Train Accuracy: 46.2 %, Loss: 1.5192\n",
            "Epoch [12/100], Learning Rate: 0.0009997779521645793\n",
            "Epoch [13/100], Train Accuracy: 47.716 %, Loss: 1.4570\n",
            "Epoch [13/100], Learning Rate: 0.0009991120277927223\n",
            "Epoch [14/100], Train Accuracy: 48.11 %, Loss: 1.3265\n",
            "Epoch [14/100], Learning Rate: 0.000998002884071386\n",
            "Epoch [15/100], Train Accuracy: 48.772 %, Loss: 1.3499\n",
            "Epoch [15/100], Learning Rate: 0.000996451615591515\n",
            "Epoch [16/100], Train Accuracy: 49.094 %, Loss: 1.4600\n",
            "Epoch [16/100], Learning Rate: 0.000994459753267812\n",
            "Epoch [17/100], Train Accuracy: 49.664 %, Loss: 1.4265\n",
            "Epoch [17/100], Learning Rate: 0.00099202926282791\n",
            "Epoch [18/100], Train Accuracy: 49.53 %, Loss: 1.4194\n",
            "Epoch [18/100], Learning Rate: 0.0009891625428724363\n",
            "Epoch [19/100], Train Accuracy: 50.09 %, Loss: 1.3861\n",
            "Epoch [19/100], Learning Rate: 0.000985862422507884\n",
            "Epoch [20/100], Train Accuracy: 50.816 %, Loss: 1.3138\n",
            "Epoch [20/100], Learning Rate: 0.0009821321585546244\n",
            "Epoch [21/100], Train Accuracy: 51.166 %, Loss: 1.4354\n",
            "Epoch [21/100], Learning Rate: 0.0009779754323328191\n",
            "Epoch [22/100], Train Accuracy: 50.92 %, Loss: 1.2255\n",
            "Epoch [22/100], Learning Rate: 0.0009733963460294015\n",
            "Epoch [23/100], Train Accuracy: 51.64 %, Loss: 1.4235\n",
            "Epoch [23/100], Learning Rate: 0.0009683994186497131\n",
            "Epoch [24/100], Train Accuracy: 51.784 %, Loss: 1.3402\n",
            "Epoch [24/100], Learning Rate: 0.0009629895815577914\n",
            "Epoch [25/100], Train Accuracy: 52.31 %, Loss: 1.3555\n",
            "Epoch [25/100], Learning Rate: 0.0009571721736097088\n",
            "Epoch [26/100], Train Accuracy: 52.09 %, Loss: 1.3674\n",
            "Epoch [26/100], Learning Rate: 0.0009509529358847655\n",
            "Epoch [27/100], Train Accuracy: 52.43 %, Loss: 1.2277\n",
            "Epoch [27/100], Learning Rate: 0.0009443380060197386\n",
            "Epoch [28/100], Train Accuracy: 52.098 %, Loss: 1.3126\n",
            "Epoch [28/100], Learning Rate: 0.0009373339121517746\n",
            "Epoch [29/100], Train Accuracy: 52.214 %, Loss: 1.2862\n",
            "Epoch [29/100], Learning Rate: 0.0009299475664759069\n",
            "Epoch [30/100], Train Accuracy: 52.818 %, Loss: 1.1988\n",
            "Epoch [30/100], Learning Rate: 0.0009221862584235528\n",
            "Epoch [31/100], Train Accuracy: 52.664 %, Loss: 1.2523\n",
            "Epoch [31/100], Learning Rate: 0.0009140576474687263\n",
            "Epoch [32/100], Train Accuracy: 52.77 %, Loss: 1.3338\n",
            "Epoch [32/100], Learning Rate: 0.0009055697555690606\n",
            "Epoch [33/100], Train Accuracy: 53.43 %, Loss: 1.3217\n",
            "Epoch [33/100], Learning Rate: 0.0008967309592491052\n",
            "Epoch [34/100], Train Accuracy: 53.624 %, Loss: 1.3641\n",
            "Epoch [34/100], Learning Rate: 0.0008875499813337069\n",
            "Epoch [35/100], Train Accuracy: 53.394 %, Loss: 1.1723\n",
            "Epoch [35/100], Learning Rate: 0.0008780358823396353\n",
            "Epoch [36/100], Train Accuracy: 54.278 %, Loss: 1.3161\n",
            "Epoch [36/100], Learning Rate: 0.0008681980515339464\n",
            "Epoch [37/100], Train Accuracy: 54.162 %, Loss: 1.2293\n",
            "Epoch [37/100], Learning Rate: 0.0008580461976679099\n",
            "Epoch [38/100], Train Accuracy: 53.932 %, Loss: 1.1831\n",
            "Epoch [38/100], Learning Rate: 0.0008475903393956434\n",
            "Epoch [39/100], Train Accuracy: 54.186 %, Loss: 1.2338\n",
            "Epoch [39/100], Learning Rate: 0.0008368407953869105\n",
            "Epoch [40/100], Train Accuracy: 54.526 %, Loss: 1.2263\n",
            "Epoch [40/100], Learning Rate: 0.0008258081741438395\n",
            "Epoch [41/100], Train Accuracy: 55.23 %, Loss: 1.2353\n",
            "Epoch [41/100], Learning Rate: 0.0008145033635316131\n",
            "Epoch [42/100], Train Accuracy: 54.832 %, Loss: 1.2664\n",
            "Epoch [42/100], Learning Rate: 0.0008029375200334589\n",
            "Epoch [43/100], Train Accuracy: 54.442 %, Loss: 1.1694\n",
            "Epoch [43/100], Learning Rate: 0.0007911220577405485\n",
            "Epoch [44/100], Train Accuracy: 55.174 %, Loss: 1.2802\n",
            "Epoch [44/100], Learning Rate: 0.0007790686370876671\n",
            "Epoch [45/100], Train Accuracy: 55.374 %, Loss: 1.3508\n",
            "Epoch [45/100], Learning Rate: 0.0007667891533457719\n",
            "Epoch [46/100], Train Accuracy: 55.416 %, Loss: 1.3233\n",
            "Epoch [46/100], Learning Rate: 0.000754295724882796\n",
            "Epoch [47/100], Train Accuracy: 55.644 %, Loss: 1.2536\n",
            "Epoch [47/100], Learning Rate: 0.0007416006812042827\n",
            "Epoch [48/100], Train Accuracy: 55.63 %, Loss: 1.2631\n",
            "Epoch [48/100], Learning Rate: 0.0007287165507856513\n",
            "Epoch [49/100], Train Accuracy: 56.398 %, Loss: 1.2073\n",
            "Epoch [49/100], Learning Rate: 0.0007156560487081052\n",
            "Epoch [50/100], Train Accuracy: 55.688 %, Loss: 1.2045\n",
            "Epoch [50/100], Learning Rate: 0.0007024320641103813\n",
            "Epoch [51/100], Train Accuracy: 56.462 %, Loss: 1.3064\n",
            "Epoch [51/100], Learning Rate: 0.0006890576474687264\n",
            "Epoch [52/100], Train Accuracy: 56.024 %, Loss: 1.2742\n",
            "Epoch [52/100], Learning Rate: 0.0006755459977176532\n",
            "Epoch [53/100], Train Accuracy: 56.376 %, Loss: 1.2693\n",
            "Epoch [53/100], Learning Rate: 0.0006619104492241846\n",
            "Epoch [54/100], Train Accuracy: 56.418 %, Loss: 1.2456\n",
            "Epoch [54/100], Learning Rate: 0.0006481644586284443\n",
            "Epoch [55/100], Train Accuracy: 56.506 %, Loss: 1.1982\n",
            "Epoch [55/100], Learning Rate: 0.0006343215915635762\n",
            "Epoch [56/100], Train Accuracy: 56.482 %, Loss: 1.2008\n",
            "Epoch [56/100], Learning Rate: 0.000620395509268104\n",
            "Epoch [57/100], Train Accuracy: 57.048 %, Loss: 1.3464\n",
            "Epoch [57/100], Learning Rate: 0.000606399955103937\n",
            "Epoch [58/100], Train Accuracy: 56.518 %, Loss: 1.1599\n",
            "Epoch [58/100], Learning Rate: 0.0005923487409933315\n",
            "Epoch [59/100], Train Accuracy: 56.854 %, Loss: 1.0835\n",
            "Epoch [59/100], Learning Rate: 0.0005782557337881911\n",
            "Epoch [60/100], Train Accuracy: 56.816 %, Loss: 1.1869\n",
            "Epoch [60/100], Learning Rate: 0.0005641348415851578\n",
            "Epoch [61/100], Train Accuracy: 57.208 %, Loss: 1.2484\n",
            "Epoch [61/100], Learning Rate: 0.00055\n",
            "Epoch [62/100], Train Accuracy: 57.016 %, Loss: 1.2647\n",
            "Epoch [62/100], Learning Rate: 0.0005358651584148423\n",
            "Epoch [63/100], Train Accuracy: 57.154 %, Loss: 1.1058\n",
            "Epoch [63/100], Learning Rate: 0.000521744266211809\n",
            "Epoch [64/100], Train Accuracy: 57.644 %, Loss: 1.1579\n",
            "Epoch [64/100], Learning Rate: 0.0005076512590066686\n",
            "Epoch [65/100], Train Accuracy: 57.306 %, Loss: 1.2324\n",
            "Epoch [65/100], Learning Rate: 0.0004936000448960632\n",
            "Epoch [66/100], Train Accuracy: 57.106 %, Loss: 1.1175\n",
            "Epoch [66/100], Learning Rate: 0.0004796044907318961\n",
            "Epoch [67/100], Train Accuracy: 57.994 %, Loss: 1.2123\n",
            "Epoch [67/100], Learning Rate: 0.0004656784084364239\n",
            "Epoch [68/100], Train Accuracy: 57.578 %, Loss: 1.1524\n",
            "Epoch [68/100], Learning Rate: 0.00045183554137155597\n",
            "Epoch [69/100], Train Accuracy: 57.614 %, Loss: 1.0996\n",
            "Epoch [69/100], Learning Rate: 0.0004380895507758153\n",
            "Epoch [70/100], Train Accuracy: 57.736 %, Loss: 1.2519\n",
            "Epoch [70/100], Learning Rate: 0.00042445400228234684\n",
            "Epoch [71/100], Train Accuracy: 57.882 %, Loss: 1.2477\n",
            "Epoch [71/100], Learning Rate: 0.0004109423525312738\n",
            "Epoch [72/100], Train Accuracy: 57.754 %, Loss: 1.2384\n",
            "Epoch [72/100], Learning Rate: 0.0003975679358896189\n",
            "Epoch [73/100], Train Accuracy: 57.792 %, Loss: 1.1792\n",
            "Epoch [73/100], Learning Rate: 0.000384343951291895\n",
            "Epoch [74/100], Train Accuracy: 58.29 %, Loss: 1.1269\n",
            "Epoch [74/100], Learning Rate: 0.00037128344921434864\n",
            "Epoch [75/100], Train Accuracy: 57.986 %, Loss: 1.0348\n",
            "Epoch [75/100], Learning Rate: 0.0003583993187957173\n",
            "Epoch [76/100], Train Accuracy: 58.042 %, Loss: 1.0877\n",
            "Epoch [76/100], Learning Rate: 0.00034570427511720393\n",
            "Epoch [77/100], Train Accuracy: 58.404 %, Loss: 1.0994\n",
            "Epoch [77/100], Learning Rate: 0.0003332108466542281\n",
            "Epoch [78/100], Train Accuracy: 58.262 %, Loss: 1.2278\n",
            "Epoch [78/100], Learning Rate: 0.0003209313629123329\n",
            "Epoch [79/100], Train Accuracy: 58.5 %, Loss: 1.1025\n",
            "Epoch [79/100], Learning Rate: 0.0003088779422594514\n",
            "Epoch [80/100], Train Accuracy: 58.656 %, Loss: 1.1197\n",
            "Epoch [80/100], Learning Rate: 0.0002970624799665412\n",
            "Epoch [81/100], Train Accuracy: 58.96 %, Loss: 1.0851\n",
            "Epoch [81/100], Learning Rate: 0.00028549663646838715\n",
            "Epoch [82/100], Train Accuracy: 58.714 %, Loss: 1.2925\n",
            "Epoch [82/100], Learning Rate: 0.00027419182585616055\n",
            "Epoch [83/100], Train Accuracy: 59.004 %, Loss: 1.0791\n",
            "Epoch [83/100], Learning Rate: 0.0002631592046130896\n",
            "Epoch [84/100], Train Accuracy: 59.296 %, Loss: 1.2426\n",
            "Epoch [84/100], Learning Rate: 0.00025240966060435674\n",
            "Epoch [85/100], Train Accuracy: 58.602 %, Loss: 1.1632\n",
            "Epoch [85/100], Learning Rate: 0.00024195380233209008\n",
            "Epoch [86/100], Train Accuracy: 58.986 %, Loss: 1.0449\n",
            "Epoch [86/100], Learning Rate: 0.00023180194846605365\n",
            "Epoch [87/100], Train Accuracy: 59.106 %, Loss: 1.0818\n",
            "Epoch [87/100], Learning Rate: 0.0002219641176603649\n",
            "Epoch [88/100], Train Accuracy: 58.934 %, Loss: 1.2404\n",
            "Epoch [88/100], Learning Rate: 0.0002124500186662932\n",
            "Epoch [89/100], Train Accuracy: 58.876 %, Loss: 1.1211\n",
            "Epoch [89/100], Learning Rate: 0.00020326904075089488\n",
            "Epoch [90/100], Train Accuracy: 59.34 %, Loss: 1.0672\n",
            "Epoch [90/100], Learning Rate: 0.00019443024443093932\n",
            "Epoch [91/100], Train Accuracy: 59.376 %, Loss: 1.0184\n",
            "Epoch [91/100], Learning Rate: 0.00018594235253127368\n",
            "Epoch [92/100], Train Accuracy: 59.498 %, Loss: 1.1279\n",
            "Epoch [92/100], Learning Rate: 0.00017781374157644722\n",
            "Epoch [93/100], Train Accuracy: 59.022 %, Loss: 1.2699\n",
            "Epoch [93/100], Learning Rate: 0.00017005243352409332\n",
            "Epoch [94/100], Train Accuracy: 59.862 %, Loss: 1.0556\n",
            "Epoch [94/100], Learning Rate: 0.0001626660878482253\n",
            "Epoch [95/100], Train Accuracy: 59.902 %, Loss: 1.0875\n",
            "Epoch [95/100], Learning Rate: 0.0001556619939802614\n",
            "Epoch [96/100], Train Accuracy: 59.806 %, Loss: 1.0073\n",
            "Epoch [96/100], Learning Rate: 0.0001490470641152345\n",
            "Epoch [97/100], Train Accuracy: 60.11 %, Loss: 1.0496\n",
            "Epoch [97/100], Learning Rate: 0.0001428278263902913\n",
            "Epoch [98/100], Train Accuracy: 59.966 %, Loss: 1.1218\n",
            "Epoch [98/100], Learning Rate: 0.00013701041844220858\n",
            "Epoch [99/100], Train Accuracy: 60.024 %, Loss: 1.0222\n",
            "Epoch [99/100], Learning Rate: 0.0001316005813502869\n",
            "Epoch [100/100], Train Accuracy: 59.858 %, Loss: 1.0961\n",
            "Epoch [100/100], Learning Rate: 0.00012660365397059855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習前後のモデルの重みを比較する関数を作成\n",
        "def check_weight_change(model, model_init):\n",
        "    weights_changed = (model.conv1.state_dict()['weight'] != model_init.conv1.state_dict()['weight']).any()\n",
        "    if weights_changed:\n",
        "        return print('モデルの重みが変化しています')\n",
        "    else:\n",
        "        return print('モデルの重みは変化していません')\n",
        "# 学習前後でモデルの重みが変化していないかを確認\n",
        "check_weight_change(model, model_init)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed0f4dcd-7e85-4001-d9d6-c2b5b1621203",
        "id": "Nexlldthk7Yc"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "モデルの重みは変化していません\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの評価\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for images, labels in test_loader_cifar10:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度の確認\n",
        "    print('Test Accuracy: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e631191-2b77-45a2-aab1-58c40d596271",
        "id": "oecc50eNk7Yc"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 37.2 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUメモリの解放\n",
        "del model, model_init\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "MAWsGMRzk7Yc"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prune_rate=0.95"
      ],
      "metadata": {
        "id": "wdS4vAI5kfg_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 刈り込み率を設定\n",
        "prune_rate = 0.95\n",
        "# 初期化手法を設定\n",
        "init = 'kaiming_normal'\n",
        "\n",
        "# デバイスを設定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# モデルを定義\n",
        "model = supermaskresnet18_10().to(device)\n",
        "# 学習前のモデルの重みを保存\n",
        "model_init = copy.deepcopy(model)\n",
        "\n",
        "# 学習率を設定\n",
        "learning_rate = 0.001\n",
        "# 全体のepoch数を設定\n",
        "num_epochs = 100\n",
        "# warm-upするepoch数を設定\n",
        "warmup_epochs = 10\n",
        "\n",
        "# 損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# オプティマイザーを設定\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# スケジューラーを設定\n",
        "scheduler = CosineLRScheduler(optimizer, t_initial=num_epochs, lr_min=1e-4, warmup_t=warmup_epochs, warmup_lr_init=1e-4, warmup_prefix=True)\n",
        "\n",
        "# モデルの学習\n",
        "for epoch in range(num_epochs):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, (images, labels) in enumerate(train_loader_cifar10):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # 順伝播\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # 逆伝播\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # 全データ数\n",
        "        total += labels.size(0)\n",
        "        # 正解数\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度を計算\n",
        "    train_acc = 100 * correct / total\n",
        "    # 正解率精度と損失を確認\n",
        "    print(\"Epoch [{}/{}], Train Accuracy: {} %, Loss: {:.4f}\".format(epoch+1, num_epochs, 100 * correct / total, loss.item()))\n",
        "    # 1エポック終了後にスケジューラーを更新\n",
        "    scheduler.step(epoch)\n",
        "    # 学習率の確認\n",
        "    print(\"Epoch [{}/{}], Learning Rate: {}\".format(epoch+1, num_epochs, optimizer.param_groups[0]['lr']))\n",
        "\n",
        "# モデルを保存\n",
        "torch.save(model, '/content/drive/MyDrive/SupermaskResNet18_CIFAR10_100epochs_pr0.95_normal_CLRS_restest.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzrilPHpm0Vm",
        "outputId": "665b00eb-3e40-4867-e529-b02922305cca"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Accuracy: 34.074 %, Loss: 1.5564\n",
            "Epoch [1/100], Learning Rate: 0.0001\n",
            "Epoch [2/100], Train Accuracy: 43.662 %, Loss: 1.4325\n",
            "Epoch [2/100], Learning Rate: 0.00019\n",
            "Epoch [3/100], Train Accuracy: 45.006 %, Loss: 1.4670\n",
            "Epoch [3/100], Learning Rate: 0.00028\n",
            "Epoch [4/100], Train Accuracy: 47.106 %, Loss: 1.4178\n",
            "Epoch [4/100], Learning Rate: 0.00036999999999999994\n",
            "Epoch [5/100], Train Accuracy: 49.21 %, Loss: 1.3701\n",
            "Epoch [5/100], Learning Rate: 0.00045999999999999996\n",
            "Epoch [6/100], Train Accuracy: 50.6 %, Loss: 1.2977\n",
            "Epoch [6/100], Learning Rate: 0.00055\n",
            "Epoch [7/100], Train Accuracy: 52.202 %, Loss: 1.3430\n",
            "Epoch [7/100], Learning Rate: 0.0006399999999999999\n",
            "Epoch [8/100], Train Accuracy: 53.928 %, Loss: 1.2079\n",
            "Epoch [8/100], Learning Rate: 0.00073\n",
            "Epoch [9/100], Train Accuracy: 54.706 %, Loss: 1.1356\n",
            "Epoch [9/100], Learning Rate: 0.00082\n",
            "Epoch [10/100], Train Accuracy: 55.984 %, Loss: 1.2509\n",
            "Epoch [10/100], Learning Rate: 0.00091\n",
            "Epoch [11/100], Train Accuracy: 56.612 %, Loss: 1.1322\n",
            "Epoch [11/100], Learning Rate: 0.001\n",
            "Epoch [12/100], Train Accuracy: 57.922 %, Loss: 1.1012\n",
            "Epoch [12/100], Learning Rate: 0.0009997779521645793\n",
            "Epoch [13/100], Train Accuracy: 58.572 %, Loss: 1.2265\n",
            "Epoch [13/100], Learning Rate: 0.0009991120277927223\n",
            "Epoch [14/100], Train Accuracy: 59.652 %, Loss: 1.0488\n",
            "Epoch [14/100], Learning Rate: 0.000998002884071386\n",
            "Epoch [15/100], Train Accuracy: 60.788 %, Loss: 1.0186\n",
            "Epoch [15/100], Learning Rate: 0.000996451615591515\n",
            "Epoch [16/100], Train Accuracy: 61.5 %, Loss: 1.0339\n",
            "Epoch [16/100], Learning Rate: 0.000994459753267812\n",
            "Epoch [17/100], Train Accuracy: 61.578 %, Loss: 1.0775\n",
            "Epoch [17/100], Learning Rate: 0.00099202926282791\n",
            "Epoch [18/100], Train Accuracy: 62.536 %, Loss: 1.0017\n",
            "Epoch [18/100], Learning Rate: 0.0009891625428724363\n",
            "Epoch [19/100], Train Accuracy: 62.998 %, Loss: 0.9781\n",
            "Epoch [19/100], Learning Rate: 0.000985862422507884\n",
            "Epoch [20/100], Train Accuracy: 63.562 %, Loss: 1.1095\n",
            "Epoch [20/100], Learning Rate: 0.0009821321585546244\n",
            "Epoch [21/100], Train Accuracy: 63.844 %, Loss: 1.0611\n",
            "Epoch [21/100], Learning Rate: 0.0009779754323328191\n",
            "Epoch [22/100], Train Accuracy: 64.228 %, Loss: 1.1665\n",
            "Epoch [22/100], Learning Rate: 0.0009733963460294015\n",
            "Epoch [23/100], Train Accuracy: 64.694 %, Loss: 1.0278\n",
            "Epoch [23/100], Learning Rate: 0.0009683994186497131\n",
            "Epoch [24/100], Train Accuracy: 65.12 %, Loss: 1.1602\n",
            "Epoch [24/100], Learning Rate: 0.0009629895815577914\n",
            "Epoch [25/100], Train Accuracy: 65.132 %, Loss: 0.9840\n",
            "Epoch [25/100], Learning Rate: 0.0009571721736097088\n",
            "Epoch [26/100], Train Accuracy: 65.834 %, Loss: 0.9014\n",
            "Epoch [26/100], Learning Rate: 0.0009509529358847655\n",
            "Epoch [27/100], Train Accuracy: 66.018 %, Loss: 0.9394\n",
            "Epoch [27/100], Learning Rate: 0.0009443380060197386\n",
            "Epoch [28/100], Train Accuracy: 66.79 %, Loss: 0.9564\n",
            "Epoch [28/100], Learning Rate: 0.0009373339121517746\n",
            "Epoch [29/100], Train Accuracy: 66.39 %, Loss: 0.9431\n",
            "Epoch [29/100], Learning Rate: 0.0009299475664759069\n",
            "Epoch [30/100], Train Accuracy: 66.952 %, Loss: 0.9329\n",
            "Epoch [30/100], Learning Rate: 0.0009221862584235528\n",
            "Epoch [31/100], Train Accuracy: 67.61 %, Loss: 0.9318\n",
            "Epoch [31/100], Learning Rate: 0.0009140576474687263\n",
            "Epoch [32/100], Train Accuracy: 67.622 %, Loss: 0.9822\n",
            "Epoch [32/100], Learning Rate: 0.0009055697555690606\n",
            "Epoch [33/100], Train Accuracy: 67.356 %, Loss: 0.8812\n",
            "Epoch [33/100], Learning Rate: 0.0008967309592491052\n",
            "Epoch [34/100], Train Accuracy: 68.2 %, Loss: 0.9250\n",
            "Epoch [34/100], Learning Rate: 0.0008875499813337069\n",
            "Epoch [35/100], Train Accuracy: 68.34 %, Loss: 0.9939\n",
            "Epoch [35/100], Learning Rate: 0.0008780358823396353\n",
            "Epoch [36/100], Train Accuracy: 68.314 %, Loss: 0.8737\n",
            "Epoch [36/100], Learning Rate: 0.0008681980515339464\n",
            "Epoch [37/100], Train Accuracy: 68.448 %, Loss: 0.9767\n",
            "Epoch [37/100], Learning Rate: 0.0008580461976679099\n",
            "Epoch [38/100], Train Accuracy: 68.504 %, Loss: 0.9272\n",
            "Epoch [38/100], Learning Rate: 0.0008475903393956434\n",
            "Epoch [39/100], Train Accuracy: 68.934 %, Loss: 0.8798\n",
            "Epoch [39/100], Learning Rate: 0.0008368407953869105\n",
            "Epoch [40/100], Train Accuracy: 69.324 %, Loss: 0.8014\n",
            "Epoch [40/100], Learning Rate: 0.0008258081741438395\n",
            "Epoch [41/100], Train Accuracy: 69.21 %, Loss: 0.8748\n",
            "Epoch [41/100], Learning Rate: 0.0008145033635316131\n",
            "Epoch [42/100], Train Accuracy: 69.614 %, Loss: 0.8667\n",
            "Epoch [42/100], Learning Rate: 0.0008029375200334589\n",
            "Epoch [43/100], Train Accuracy: 70.164 %, Loss: 0.7403\n",
            "Epoch [43/100], Learning Rate: 0.0007911220577405485\n",
            "Epoch [44/100], Train Accuracy: 69.976 %, Loss: 0.9227\n",
            "Epoch [44/100], Learning Rate: 0.0007790686370876671\n",
            "Epoch [45/100], Train Accuracy: 69.862 %, Loss: 0.7734\n",
            "Epoch [45/100], Learning Rate: 0.0007667891533457719\n",
            "Epoch [46/100], Train Accuracy: 70.224 %, Loss: 0.8736\n",
            "Epoch [46/100], Learning Rate: 0.000754295724882796\n",
            "Epoch [47/100], Train Accuracy: 70.498 %, Loss: 0.8488\n",
            "Epoch [47/100], Learning Rate: 0.0007416006812042827\n",
            "Epoch [48/100], Train Accuracy: 70.782 %, Loss: 0.8229\n",
            "Epoch [48/100], Learning Rate: 0.0007287165507856513\n",
            "Epoch [49/100], Train Accuracy: 70.824 %, Loss: 0.8565\n",
            "Epoch [49/100], Learning Rate: 0.0007156560487081052\n",
            "Epoch [50/100], Train Accuracy: 70.936 %, Loss: 0.7830\n",
            "Epoch [50/100], Learning Rate: 0.0007024320641103813\n",
            "Epoch [51/100], Train Accuracy: 71.458 %, Loss: 0.9520\n",
            "Epoch [51/100], Learning Rate: 0.0006890576474687264\n",
            "Epoch [52/100], Train Accuracy: 71.516 %, Loss: 0.9624\n",
            "Epoch [52/100], Learning Rate: 0.0006755459977176532\n",
            "Epoch [53/100], Train Accuracy: 71.422 %, Loss: 0.8258\n",
            "Epoch [53/100], Learning Rate: 0.0006619104492241846\n",
            "Epoch [54/100], Train Accuracy: 71.234 %, Loss: 0.8101\n",
            "Epoch [54/100], Learning Rate: 0.0006481644586284443\n",
            "Epoch [55/100], Train Accuracy: 71.222 %, Loss: 0.8262\n",
            "Epoch [55/100], Learning Rate: 0.0006343215915635762\n",
            "Epoch [56/100], Train Accuracy: 71.85 %, Loss: 0.8441\n",
            "Epoch [56/100], Learning Rate: 0.000620395509268104\n",
            "Epoch [57/100], Train Accuracy: 72.322 %, Loss: 0.7906\n",
            "Epoch [57/100], Learning Rate: 0.000606399955103937\n",
            "Epoch [58/100], Train Accuracy: 71.898 %, Loss: 0.8491\n",
            "Epoch [58/100], Learning Rate: 0.0005923487409933315\n",
            "Epoch [59/100], Train Accuracy: 71.994 %, Loss: 0.8773\n",
            "Epoch [59/100], Learning Rate: 0.0005782557337881911\n",
            "Epoch [60/100], Train Accuracy: 72.114 %, Loss: 0.8677\n",
            "Epoch [60/100], Learning Rate: 0.0005641348415851578\n",
            "Epoch [61/100], Train Accuracy: 72.538 %, Loss: 0.8755\n",
            "Epoch [61/100], Learning Rate: 0.00055\n",
            "Epoch [62/100], Train Accuracy: 72.618 %, Loss: 0.7155\n",
            "Epoch [62/100], Learning Rate: 0.0005358651584148423\n",
            "Epoch [63/100], Train Accuracy: 72.406 %, Loss: 0.9116\n",
            "Epoch [63/100], Learning Rate: 0.000521744266211809\n",
            "Epoch [64/100], Train Accuracy: 72.602 %, Loss: 0.9136\n",
            "Epoch [64/100], Learning Rate: 0.0005076512590066686\n",
            "Epoch [65/100], Train Accuracy: 73.21 %, Loss: 0.7614\n",
            "Epoch [65/100], Learning Rate: 0.0004936000448960632\n",
            "Epoch [66/100], Train Accuracy: 72.918 %, Loss: 0.8604\n",
            "Epoch [66/100], Learning Rate: 0.0004796044907318961\n",
            "Epoch [67/100], Train Accuracy: 73.236 %, Loss: 0.7513\n",
            "Epoch [67/100], Learning Rate: 0.0004656784084364239\n",
            "Epoch [68/100], Train Accuracy: 73.262 %, Loss: 0.7492\n",
            "Epoch [68/100], Learning Rate: 0.00045183554137155597\n",
            "Epoch [69/100], Train Accuracy: 73.248 %, Loss: 0.6999\n",
            "Epoch [69/100], Learning Rate: 0.0004380895507758153\n",
            "Epoch [70/100], Train Accuracy: 73.744 %, Loss: 0.8581\n",
            "Epoch [70/100], Learning Rate: 0.00042445400228234684\n",
            "Epoch [71/100], Train Accuracy: 73.952 %, Loss: 0.8691\n",
            "Epoch [71/100], Learning Rate: 0.0004109423525312738\n",
            "Epoch [72/100], Train Accuracy: 73.886 %, Loss: 0.7859\n",
            "Epoch [72/100], Learning Rate: 0.0003975679358896189\n",
            "Epoch [73/100], Train Accuracy: 74.066 %, Loss: 0.6896\n",
            "Epoch [73/100], Learning Rate: 0.000384343951291895\n",
            "Epoch [74/100], Train Accuracy: 74.21 %, Loss: 0.7094\n",
            "Epoch [74/100], Learning Rate: 0.00037128344921434864\n",
            "Epoch [75/100], Train Accuracy: 73.856 %, Loss: 0.6602\n",
            "Epoch [75/100], Learning Rate: 0.0003583993187957173\n",
            "Epoch [76/100], Train Accuracy: 74.41 %, Loss: 0.6963\n",
            "Epoch [76/100], Learning Rate: 0.00034570427511720393\n",
            "Epoch [77/100], Train Accuracy: 74.492 %, Loss: 0.7716\n",
            "Epoch [77/100], Learning Rate: 0.0003332108466542281\n",
            "Epoch [78/100], Train Accuracy: 74.754 %, Loss: 0.6719\n",
            "Epoch [78/100], Learning Rate: 0.0003209313629123329\n",
            "Epoch [79/100], Train Accuracy: 74.744 %, Loss: 0.6496\n",
            "Epoch [79/100], Learning Rate: 0.0003088779422594514\n",
            "Epoch [80/100], Train Accuracy: 74.874 %, Loss: 0.8380\n",
            "Epoch [80/100], Learning Rate: 0.0002970624799665412\n",
            "Epoch [81/100], Train Accuracy: 75.176 %, Loss: 0.7735\n",
            "Epoch [81/100], Learning Rate: 0.00028549663646838715\n",
            "Epoch [82/100], Train Accuracy: 75.418 %, Loss: 0.7365\n",
            "Epoch [82/100], Learning Rate: 0.00027419182585616055\n",
            "Epoch [83/100], Train Accuracy: 75.55 %, Loss: 0.6551\n",
            "Epoch [83/100], Learning Rate: 0.0002631592046130896\n",
            "Epoch [84/100], Train Accuracy: 75.374 %, Loss: 0.6457\n",
            "Epoch [84/100], Learning Rate: 0.00025240966060435674\n",
            "Epoch [85/100], Train Accuracy: 75.346 %, Loss: 0.7034\n",
            "Epoch [85/100], Learning Rate: 0.00024195380233209008\n",
            "Epoch [86/100], Train Accuracy: 75.364 %, Loss: 0.7399\n",
            "Epoch [86/100], Learning Rate: 0.00023180194846605365\n",
            "Epoch [87/100], Train Accuracy: 75.834 %, Loss: 0.5994\n",
            "Epoch [87/100], Learning Rate: 0.0002219641176603649\n",
            "Epoch [88/100], Train Accuracy: 75.408 %, Loss: 0.6773\n",
            "Epoch [88/100], Learning Rate: 0.0002124500186662932\n",
            "Epoch [89/100], Train Accuracy: 76.174 %, Loss: 0.6353\n",
            "Epoch [89/100], Learning Rate: 0.00020326904075089488\n",
            "Epoch [90/100], Train Accuracy: 75.974 %, Loss: 0.6635\n",
            "Epoch [90/100], Learning Rate: 0.00019443024443093932\n",
            "Epoch [91/100], Train Accuracy: 75.694 %, Loss: 0.6844\n",
            "Epoch [91/100], Learning Rate: 0.00018594235253127368\n",
            "Epoch [92/100], Train Accuracy: 76.042 %, Loss: 0.6102\n",
            "Epoch [92/100], Learning Rate: 0.00017781374157644722\n",
            "Epoch [93/100], Train Accuracy: 76.59 %, Loss: 0.6475\n",
            "Epoch [93/100], Learning Rate: 0.00017005243352409332\n",
            "Epoch [94/100], Train Accuracy: 76.06 %, Loss: 0.6760\n",
            "Epoch [94/100], Learning Rate: 0.0001626660878482253\n",
            "Epoch [95/100], Train Accuracy: 76.284 %, Loss: 0.6450\n",
            "Epoch [95/100], Learning Rate: 0.0001556619939802614\n",
            "Epoch [96/100], Train Accuracy: 76.37 %, Loss: 0.7305\n",
            "Epoch [96/100], Learning Rate: 0.0001490470641152345\n",
            "Epoch [97/100], Train Accuracy: 76.694 %, Loss: 0.6116\n",
            "Epoch [97/100], Learning Rate: 0.0001428278263902913\n",
            "Epoch [98/100], Train Accuracy: 76.93 %, Loss: 0.7203\n",
            "Epoch [98/100], Learning Rate: 0.00013701041844220858\n",
            "Epoch [99/100], Train Accuracy: 76.898 %, Loss: 0.6102\n",
            "Epoch [99/100], Learning Rate: 0.0001316005813502869\n",
            "Epoch [100/100], Train Accuracy: 76.74 %, Loss: 0.6118\n",
            "Epoch [100/100], Learning Rate: 0.00012660365397059855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習前後のモデルの重みを比較する関数を作成\n",
        "def check_weight_change(model, model_init):\n",
        "    weights_changed = (model.conv1.state_dict()['weight'] != model_init.conv1.state_dict()['weight']).any()\n",
        "    if weights_changed:\n",
        "        return print('モデルの重みが変化しています')\n",
        "    else:\n",
        "        return print('モデルの重みは変化していません')\n",
        "# 学習前後でモデルの重みが変化していないかを確認\n",
        "check_weight_change(model, model_init)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a85281a3-7dd3-4bcb-c049-1c8a0a125ad7",
        "id": "ozGDJO1_kfhP"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "モデルの重みは変化していません\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの評価\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for images, labels in test_loader_cifar10:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度の確認\n",
        "    print('Test Accuracy: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c166153-24ae-4b92-8326-d97f10bfd614",
        "id": "hLwkDiNNkfhP"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 68.35 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUメモリの解放\n",
        "del model, model_init\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "YaxLNbEzkfhP"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prune_rate=0.9"
      ],
      "metadata": {
        "id": "VFsN4qoY1qFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 刈り込み率を設定\n",
        "prune_rate = 0.9\n",
        "# 初期化手法を設定\n",
        "init = 'kaiming_normal'\n",
        "\n",
        "# デバイスを設定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# モデルを定義\n",
        "model = supermaskresnet18_10().to(device)\n",
        "# 学習前のモデルの重みを保存\n",
        "model_init = copy.deepcopy(model)\n",
        "\n",
        "# 学習率を設定\n",
        "learning_rate = 0.001\n",
        "# 全体のepoch数を設定\n",
        "num_epochs = 100\n",
        "# warm-upするepoch数を設定\n",
        "warmup_epochs = 10\n",
        "\n",
        "# 損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# オプティマイザーを設定\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# スケジューラーを設定\n",
        "scheduler = CosineLRScheduler(optimizer, t_initial=num_epochs, lr_min=1e-4, warmup_t=warmup_epochs, warmup_lr_init=1e-4, warmup_prefix=True)\n",
        "\n",
        "# モデルの学習\n",
        "for epoch in range(num_epochs):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, (images, labels) in enumerate(train_loader_cifar10):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # 順伝播\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # 逆伝播\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # 全データ数\n",
        "        total += labels.size(0)\n",
        "        # 正解数\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度を計算\n",
        "    train_acc = 100 * correct / total\n",
        "    # 正解率精度と損失を確認\n",
        "    print(\"Epoch [{}/{}], Train Accuracy: {} %, Loss: {:.4f}\".format(epoch+1, num_epochs, 100 * correct / total, loss.item()))\n",
        "    # 1エポック終了後にスケジューラーを更新\n",
        "    scheduler.step(epoch)\n",
        "    # 学習率の確認\n",
        "    print(\"Epoch [{}/{}], Learning Rate: {}\".format(epoch+1, num_epochs, optimizer.param_groups[0]['lr']))\n",
        "\n",
        "# モデルを保存\n",
        "torch.save(model, '/content/drive/MyDrive/SupermaskResNet18_CIFAR10_100epochs_pr0.9_normal_CLRS_restest.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GyyL2l41qFy",
        "outputId": "638b0d2a-c95c-4346-b2ee-a0163b6a0e28"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Accuracy: 34.54 %, Loss: 1.5927\n",
            "Epoch [1/100], Learning Rate: 0.0001\n",
            "Epoch [2/100], Train Accuracy: 44.474 %, Loss: 1.5099\n",
            "Epoch [2/100], Learning Rate: 0.00019\n",
            "Epoch [3/100], Train Accuracy: 47.128 %, Loss: 1.3527\n",
            "Epoch [3/100], Learning Rate: 0.00028\n",
            "Epoch [4/100], Train Accuracy: 50.592 %, Loss: 1.2141\n",
            "Epoch [4/100], Learning Rate: 0.00036999999999999994\n",
            "Epoch [5/100], Train Accuracy: 52.04 %, Loss: 1.2118\n",
            "Epoch [5/100], Learning Rate: 0.00045999999999999996\n",
            "Epoch [6/100], Train Accuracy: 54.638 %, Loss: 1.2181\n",
            "Epoch [6/100], Learning Rate: 0.00055\n",
            "Epoch [7/100], Train Accuracy: 56.258 %, Loss: 1.3008\n",
            "Epoch [7/100], Learning Rate: 0.0006399999999999999\n",
            "Epoch [8/100], Train Accuracy: 57.782 %, Loss: 1.0823\n",
            "Epoch [8/100], Learning Rate: 0.00073\n",
            "Epoch [9/100], Train Accuracy: 59.142 %, Loss: 1.1718\n",
            "Epoch [9/100], Learning Rate: 0.00082\n",
            "Epoch [10/100], Train Accuracy: 60.766 %, Loss: 0.9915\n",
            "Epoch [10/100], Learning Rate: 0.00091\n",
            "Epoch [11/100], Train Accuracy: 61.606 %, Loss: 1.0523\n",
            "Epoch [11/100], Learning Rate: 0.001\n",
            "Epoch [12/100], Train Accuracy: 62.52 %, Loss: 1.0034\n",
            "Epoch [12/100], Learning Rate: 0.0009997779521645793\n",
            "Epoch [13/100], Train Accuracy: 63.696 %, Loss: 1.0674\n",
            "Epoch [13/100], Learning Rate: 0.0009991120277927223\n",
            "Epoch [14/100], Train Accuracy: 65.048 %, Loss: 0.9569\n",
            "Epoch [14/100], Learning Rate: 0.000998002884071386\n",
            "Epoch [15/100], Train Accuracy: 65.864 %, Loss: 1.0166\n",
            "Epoch [15/100], Learning Rate: 0.000996451615591515\n",
            "Epoch [16/100], Train Accuracy: 66.606 %, Loss: 0.9949\n",
            "Epoch [16/100], Learning Rate: 0.000994459753267812\n",
            "Epoch [17/100], Train Accuracy: 67.146 %, Loss: 0.9734\n",
            "Epoch [17/100], Learning Rate: 0.00099202926282791\n",
            "Epoch [18/100], Train Accuracy: 68.058 %, Loss: 0.8712\n",
            "Epoch [18/100], Learning Rate: 0.0009891625428724363\n",
            "Epoch [19/100], Train Accuracy: 68.848 %, Loss: 0.9302\n",
            "Epoch [19/100], Learning Rate: 0.000985862422507884\n",
            "Epoch [20/100], Train Accuracy: 68.978 %, Loss: 0.8935\n",
            "Epoch [20/100], Learning Rate: 0.0009821321585546244\n",
            "Epoch [21/100], Train Accuracy: 69.202 %, Loss: 0.9191\n",
            "Epoch [21/100], Learning Rate: 0.0009779754323328191\n",
            "Epoch [22/100], Train Accuracy: 69.318 %, Loss: 0.8691\n",
            "Epoch [22/100], Learning Rate: 0.0009733963460294015\n",
            "Epoch [23/100], Train Accuracy: 70.086 %, Loss: 0.8894\n",
            "Epoch [23/100], Learning Rate: 0.0009683994186497131\n",
            "Epoch [24/100], Train Accuracy: 70.572 %, Loss: 0.7348\n",
            "Epoch [24/100], Learning Rate: 0.0009629895815577914\n",
            "Epoch [25/100], Train Accuracy: 70.802 %, Loss: 0.8120\n",
            "Epoch [25/100], Learning Rate: 0.0009571721736097088\n",
            "Epoch [26/100], Train Accuracy: 70.91 %, Loss: 0.9088\n",
            "Epoch [26/100], Learning Rate: 0.0009509529358847655\n",
            "Epoch [27/100], Train Accuracy: 71.374 %, Loss: 0.7825\n",
            "Epoch [27/100], Learning Rate: 0.0009443380060197386\n",
            "Epoch [28/100], Train Accuracy: 71.886 %, Loss: 0.8457\n",
            "Epoch [28/100], Learning Rate: 0.0009373339121517746\n",
            "Epoch [29/100], Train Accuracy: 71.64 %, Loss: 0.9066\n",
            "Epoch [29/100], Learning Rate: 0.0009299475664759069\n",
            "Epoch [30/100], Train Accuracy: 71.79 %, Loss: 0.8451\n",
            "Epoch [30/100], Learning Rate: 0.0009221862584235528\n",
            "Epoch [31/100], Train Accuracy: 72.726 %, Loss: 0.7688\n",
            "Epoch [31/100], Learning Rate: 0.0009140576474687263\n",
            "Epoch [32/100], Train Accuracy: 72.906 %, Loss: 0.7081\n",
            "Epoch [32/100], Learning Rate: 0.0009055697555690606\n",
            "Epoch [33/100], Train Accuracy: 72.92 %, Loss: 0.7551\n",
            "Epoch [33/100], Learning Rate: 0.0008967309592491052\n",
            "Epoch [34/100], Train Accuracy: 72.83 %, Loss: 0.7153\n",
            "Epoch [34/100], Learning Rate: 0.0008875499813337069\n",
            "Epoch [35/100], Train Accuracy: 73.45 %, Loss: 0.7793\n",
            "Epoch [35/100], Learning Rate: 0.0008780358823396353\n",
            "Epoch [36/100], Train Accuracy: 73.358 %, Loss: 0.7890\n",
            "Epoch [36/100], Learning Rate: 0.0008681980515339464\n",
            "Epoch [37/100], Train Accuracy: 73.478 %, Loss: 0.6789\n",
            "Epoch [37/100], Learning Rate: 0.0008580461976679099\n",
            "Epoch [38/100], Train Accuracy: 74.066 %, Loss: 0.6529\n",
            "Epoch [38/100], Learning Rate: 0.0008475903393956434\n",
            "Epoch [39/100], Train Accuracy: 74.002 %, Loss: 0.6949\n",
            "Epoch [39/100], Learning Rate: 0.0008368407953869105\n",
            "Epoch [40/100], Train Accuracy: 74.396 %, Loss: 0.7496\n",
            "Epoch [40/100], Learning Rate: 0.0008258081741438395\n",
            "Epoch [41/100], Train Accuracy: 74.956 %, Loss: 0.7465\n",
            "Epoch [41/100], Learning Rate: 0.0008145033635316131\n",
            "Epoch [42/100], Train Accuracy: 74.732 %, Loss: 0.7187\n",
            "Epoch [42/100], Learning Rate: 0.0008029375200334589\n",
            "Epoch [43/100], Train Accuracy: 75.056 %, Loss: 0.6468\n",
            "Epoch [43/100], Learning Rate: 0.0007911220577405485\n",
            "Epoch [44/100], Train Accuracy: 75.156 %, Loss: 0.7293\n",
            "Epoch [44/100], Learning Rate: 0.0007790686370876671\n",
            "Epoch [45/100], Train Accuracy: 75.274 %, Loss: 0.7256\n",
            "Epoch [45/100], Learning Rate: 0.0007667891533457719\n",
            "Epoch [46/100], Train Accuracy: 75.182 %, Loss: 0.8084\n",
            "Epoch [46/100], Learning Rate: 0.000754295724882796\n",
            "Epoch [47/100], Train Accuracy: 75.756 %, Loss: 0.6939\n",
            "Epoch [47/100], Learning Rate: 0.0007416006812042827\n",
            "Epoch [48/100], Train Accuracy: 75.71 %, Loss: 0.6334\n",
            "Epoch [48/100], Learning Rate: 0.0007287165507856513\n",
            "Epoch [49/100], Train Accuracy: 76.118 %, Loss: 0.7009\n",
            "Epoch [49/100], Learning Rate: 0.0007156560487081052\n",
            "Epoch [50/100], Train Accuracy: 76.326 %, Loss: 0.6904\n",
            "Epoch [50/100], Learning Rate: 0.0007024320641103813\n",
            "Epoch [51/100], Train Accuracy: 76.314 %, Loss: 0.6789\n",
            "Epoch [51/100], Learning Rate: 0.0006890576474687264\n",
            "Epoch [52/100], Train Accuracy: 76.366 %, Loss: 0.5456\n",
            "Epoch [52/100], Learning Rate: 0.0006755459977176532\n",
            "Epoch [53/100], Train Accuracy: 76.634 %, Loss: 0.7196\n",
            "Epoch [53/100], Learning Rate: 0.0006619104492241846\n",
            "Epoch [54/100], Train Accuracy: 76.386 %, Loss: 0.6240\n",
            "Epoch [54/100], Learning Rate: 0.0006481644586284443\n",
            "Epoch [55/100], Train Accuracy: 76.914 %, Loss: 0.8349\n",
            "Epoch [55/100], Learning Rate: 0.0006343215915635762\n",
            "Epoch [56/100], Train Accuracy: 77.128 %, Loss: 0.7897\n",
            "Epoch [56/100], Learning Rate: 0.000620395509268104\n",
            "Epoch [57/100], Train Accuracy: 77.088 %, Loss: 0.5726\n",
            "Epoch [57/100], Learning Rate: 0.000606399955103937\n",
            "Epoch [58/100], Train Accuracy: 77.312 %, Loss: 0.7117\n",
            "Epoch [58/100], Learning Rate: 0.0005923487409933315\n",
            "Epoch [59/100], Train Accuracy: 77.452 %, Loss: 0.6215\n",
            "Epoch [59/100], Learning Rate: 0.0005782557337881911\n",
            "Epoch [60/100], Train Accuracy: 77.704 %, Loss: 0.5815\n",
            "Epoch [60/100], Learning Rate: 0.0005641348415851578\n",
            "Epoch [61/100], Train Accuracy: 77.73 %, Loss: 0.7780\n",
            "Epoch [61/100], Learning Rate: 0.00055\n",
            "Epoch [62/100], Train Accuracy: 77.544 %, Loss: 0.5958\n",
            "Epoch [62/100], Learning Rate: 0.0005358651584148423\n",
            "Epoch [63/100], Train Accuracy: 78.012 %, Loss: 0.7082\n",
            "Epoch [63/100], Learning Rate: 0.000521744266211809\n",
            "Epoch [64/100], Train Accuracy: 78.11 %, Loss: 0.6019\n",
            "Epoch [64/100], Learning Rate: 0.0005076512590066686\n",
            "Epoch [65/100], Train Accuracy: 78.458 %, Loss: 0.7075\n",
            "Epoch [65/100], Learning Rate: 0.0004936000448960632\n",
            "Epoch [66/100], Train Accuracy: 78.464 %, Loss: 0.5969\n",
            "Epoch [66/100], Learning Rate: 0.0004796044907318961\n",
            "Epoch [67/100], Train Accuracy: 78.82 %, Loss: 0.6374\n",
            "Epoch [67/100], Learning Rate: 0.0004656784084364239\n",
            "Epoch [68/100], Train Accuracy: 78.632 %, Loss: 0.6143\n",
            "Epoch [68/100], Learning Rate: 0.00045183554137155597\n",
            "Epoch [69/100], Train Accuracy: 78.584 %, Loss: 0.6460\n",
            "Epoch [69/100], Learning Rate: 0.0004380895507758153\n",
            "Epoch [70/100], Train Accuracy: 79.05 %, Loss: 0.5276\n",
            "Epoch [70/100], Learning Rate: 0.00042445400228234684\n",
            "Epoch [71/100], Train Accuracy: 78.686 %, Loss: 0.5859\n",
            "Epoch [71/100], Learning Rate: 0.0004109423525312738\n",
            "Epoch [72/100], Train Accuracy: 79.3 %, Loss: 0.6604\n",
            "Epoch [72/100], Learning Rate: 0.0003975679358896189\n",
            "Epoch [73/100], Train Accuracy: 79.55 %, Loss: 0.5994\n",
            "Epoch [73/100], Learning Rate: 0.000384343951291895\n",
            "Epoch [74/100], Train Accuracy: 79.76 %, Loss: 0.5202\n",
            "Epoch [74/100], Learning Rate: 0.00037128344921434864\n",
            "Epoch [75/100], Train Accuracy: 79.666 %, Loss: 0.5685\n",
            "Epoch [75/100], Learning Rate: 0.0003583993187957173\n",
            "Epoch [76/100], Train Accuracy: 79.766 %, Loss: 0.5515\n",
            "Epoch [76/100], Learning Rate: 0.00034570427511720393\n",
            "Epoch [77/100], Train Accuracy: 80.002 %, Loss: 0.5021\n",
            "Epoch [77/100], Learning Rate: 0.0003332108466542281\n",
            "Epoch [78/100], Train Accuracy: 80.28 %, Loss: 0.5824\n",
            "Epoch [78/100], Learning Rate: 0.0003209313629123329\n",
            "Epoch [79/100], Train Accuracy: 80.25 %, Loss: 0.6007\n",
            "Epoch [79/100], Learning Rate: 0.0003088779422594514\n",
            "Epoch [80/100], Train Accuracy: 80.428 %, Loss: 0.4392\n",
            "Epoch [80/100], Learning Rate: 0.0002970624799665412\n",
            "Epoch [81/100], Train Accuracy: 80.596 %, Loss: 0.5905\n",
            "Epoch [81/100], Learning Rate: 0.00028549663646838715\n",
            "Epoch [82/100], Train Accuracy: 81.16 %, Loss: 0.5076\n",
            "Epoch [82/100], Learning Rate: 0.00027419182585616055\n",
            "Epoch [83/100], Train Accuracy: 80.724 %, Loss: 0.5375\n",
            "Epoch [83/100], Learning Rate: 0.0002631592046130896\n",
            "Epoch [84/100], Train Accuracy: 81.25 %, Loss: 0.5535\n",
            "Epoch [84/100], Learning Rate: 0.00025240966060435674\n",
            "Epoch [85/100], Train Accuracy: 81.08 %, Loss: 0.4844\n",
            "Epoch [85/100], Learning Rate: 0.00024195380233209008\n",
            "Epoch [86/100], Train Accuracy: 81.274 %, Loss: 0.5379\n",
            "Epoch [86/100], Learning Rate: 0.00023180194846605365\n",
            "Epoch [87/100], Train Accuracy: 81.22 %, Loss: 0.5088\n",
            "Epoch [87/100], Learning Rate: 0.0002219641176603649\n",
            "Epoch [88/100], Train Accuracy: 81.538 %, Loss: 0.4401\n",
            "Epoch [88/100], Learning Rate: 0.0002124500186662932\n",
            "Epoch [89/100], Train Accuracy: 81.518 %, Loss: 0.4955\n",
            "Epoch [89/100], Learning Rate: 0.00020326904075089488\n",
            "Epoch [90/100], Train Accuracy: 81.722 %, Loss: 0.5515\n",
            "Epoch [90/100], Learning Rate: 0.00019443024443093932\n",
            "Epoch [91/100], Train Accuracy: 81.672 %, Loss: 0.5129\n",
            "Epoch [91/100], Learning Rate: 0.00018594235253127368\n",
            "Epoch [92/100], Train Accuracy: 82.334 %, Loss: 0.5025\n",
            "Epoch [92/100], Learning Rate: 0.00017781374157644722\n",
            "Epoch [93/100], Train Accuracy: 82.266 %, Loss: 0.6158\n",
            "Epoch [93/100], Learning Rate: 0.00017005243352409332\n",
            "Epoch [94/100], Train Accuracy: 82.466 %, Loss: 0.4549\n",
            "Epoch [94/100], Learning Rate: 0.0001626660878482253\n",
            "Epoch [95/100], Train Accuracy: 82.588 %, Loss: 0.4832\n",
            "Epoch [95/100], Learning Rate: 0.0001556619939802614\n",
            "Epoch [96/100], Train Accuracy: 82.55 %, Loss: 0.4814\n",
            "Epoch [96/100], Learning Rate: 0.0001490470641152345\n",
            "Epoch [97/100], Train Accuracy: 82.812 %, Loss: 0.5426\n",
            "Epoch [97/100], Learning Rate: 0.0001428278263902913\n",
            "Epoch [98/100], Train Accuracy: 82.722 %, Loss: 0.4643\n",
            "Epoch [98/100], Learning Rate: 0.00013701041844220858\n",
            "Epoch [99/100], Train Accuracy: 83.106 %, Loss: 0.4821\n",
            "Epoch [99/100], Learning Rate: 0.0001316005813502869\n",
            "Epoch [100/100], Train Accuracy: 82.708 %, Loss: 0.4719\n",
            "Epoch [100/100], Learning Rate: 0.00012660365397059855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習前後のモデルの重みを比較する関数を作成\n",
        "def check_weight_change(model, model_init):\n",
        "    weights_changed = (model.conv1.state_dict()['weight'] != model_init.conv1.state_dict()['weight']).any()\n",
        "    if weights_changed:\n",
        "        return print('モデルの重みが変化しています')\n",
        "    else:\n",
        "        return print('モデルの重みは変化していません')\n",
        "# 学習前後でモデルの重みが変化していないかを確認\n",
        "check_weight_change(model, model_init)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6grIJheW1qFy",
        "outputId": "f0db2a2e-8cb6-4b8f-cfd8-7e8724fccd18"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "モデルの重みは変化していません\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの評価\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for images, labels in test_loader_cifar10:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度の確認\n",
        "    print('Test Accuracy: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sa4dqSl1qFy",
        "outputId": "4df4c106-c857-4c3f-b1b8-1844ff401cb2"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 76.45 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUメモリの解放\n",
        "del model, model_init\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Duqe46Pu1qFz"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prune_rate=0.7"
      ],
      "metadata": {
        "id": "B6yICeOH1qFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 刈り込み率を設定\n",
        "prune_rate = 0.7\n",
        "# 初期化手法を設定\n",
        "init = 'kaiming_normal'\n",
        "\n",
        "# デバイスを設定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# モデルを定義\n",
        "model = supermaskresnet18_10().to(device)\n",
        "# 学習前のモデルの重みを保存\n",
        "model_init = copy.deepcopy(model)\n",
        "\n",
        "# 学習率を設定\n",
        "learning_rate = 0.001\n",
        "# 全体のepoch数を設定\n",
        "num_epochs = 100\n",
        "# warm-upするepoch数を設定\n",
        "warmup_epochs = 10\n",
        "\n",
        "# 損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# オプティマイザーを設定\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# スケジューラーを設定\n",
        "scheduler = CosineLRScheduler(optimizer, t_initial=num_epochs, lr_min=1e-4, warmup_t=warmup_epochs, warmup_lr_init=1e-4, warmup_prefix=True)\n",
        "\n",
        "# モデルの学習\n",
        "for epoch in range(num_epochs):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, (images, labels) in enumerate(train_loader_cifar10):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # 順伝播\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # 逆伝播\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # 全データ数\n",
        "        total += labels.size(0)\n",
        "        # 正解数\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度を計算\n",
        "    train_acc = 100 * correct / total\n",
        "    # 正解率精度と損失を確認\n",
        "    print(\"Epoch [{}/{}], Train Accuracy: {} %, Loss: {:.4f}\".format(epoch+1, num_epochs, 100 * correct / total, loss.item()))\n",
        "    # 1エポック終了後にスケジューラーを更新\n",
        "    scheduler.step(epoch)\n",
        "    # 学習率の確認\n",
        "    print(\"Epoch [{}/{}], Learning Rate: {}\".format(epoch+1, num_epochs, optimizer.param_groups[0]['lr']))\n",
        "\n",
        "# モデルを保存\n",
        "torch.save(model, '/content/drive/MyDrive/SupermaskResNet18_CIFAR10_100epochs_pr0.7_normal_CLRS_restest.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0683e21e-f4d7-46fd-e150-71307e29d4b2",
        "id": "TG-DEqrH1qF0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Accuracy: 33.366 %, Loss: 1.6649\n",
            "Epoch [1/100], Learning Rate: 0.0001\n",
            "Epoch [2/100], Train Accuracy: 44.176 %, Loss: 1.4245\n",
            "Epoch [2/100], Learning Rate: 0.00019\n",
            "Epoch [3/100], Train Accuracy: 47.486 %, Loss: 1.4883\n",
            "Epoch [3/100], Learning Rate: 0.00028\n",
            "Epoch [4/100], Train Accuracy: 51.01 %, Loss: 1.3216\n",
            "Epoch [4/100], Learning Rate: 0.00036999999999999994\n",
            "Epoch [5/100], Train Accuracy: 54.212 %, Loss: 1.1973\n",
            "Epoch [5/100], Learning Rate: 0.00045999999999999996\n",
            "Epoch [6/100], Train Accuracy: 56.416 %, Loss: 1.1063\n",
            "Epoch [6/100], Learning Rate: 0.00055\n",
            "Epoch [7/100], Train Accuracy: 58.208 %, Loss: 1.1021\n",
            "Epoch [7/100], Learning Rate: 0.0006399999999999999\n",
            "Epoch [8/100], Train Accuracy: 60.802 %, Loss: 1.0709\n",
            "Epoch [8/100], Learning Rate: 0.00073\n",
            "Epoch [9/100], Train Accuracy: 62.218 %, Loss: 0.9591\n",
            "Epoch [9/100], Learning Rate: 0.00082\n",
            "Epoch [10/100], Train Accuracy: 63.818 %, Loss: 0.9471\n",
            "Epoch [10/100], Learning Rate: 0.00091\n",
            "Epoch [11/100], Train Accuracy: 65.094 %, Loss: 0.8369\n",
            "Epoch [11/100], Learning Rate: 0.001\n",
            "Epoch [12/100], Train Accuracy: 66.266 %, Loss: 0.9229\n",
            "Epoch [12/100], Learning Rate: 0.0009997779521645793\n",
            "Epoch [13/100], Train Accuracy: 66.972 %, Loss: 0.9088\n",
            "Epoch [13/100], Learning Rate: 0.0009991120277927223\n",
            "Epoch [14/100], Train Accuracy: 68.32 %, Loss: 0.8403\n",
            "Epoch [14/100], Learning Rate: 0.000998002884071386\n",
            "Epoch [15/100], Train Accuracy: 68.974 %, Loss: 0.8496\n",
            "Epoch [15/100], Learning Rate: 0.000996451615591515\n",
            "Epoch [16/100], Train Accuracy: 70.082 %, Loss: 0.8995\n",
            "Epoch [16/100], Learning Rate: 0.000994459753267812\n",
            "Epoch [17/100], Train Accuracy: 70.55 %, Loss: 0.7549\n",
            "Epoch [17/100], Learning Rate: 0.00099202926282791\n",
            "Epoch [18/100], Train Accuracy: 71.378 %, Loss: 0.8596\n",
            "Epoch [18/100], Learning Rate: 0.0009891625428724363\n",
            "Epoch [19/100], Train Accuracy: 72.08 %, Loss: 0.7958\n",
            "Epoch [19/100], Learning Rate: 0.000985862422507884\n",
            "Epoch [20/100], Train Accuracy: 72.438 %, Loss: 0.7475\n",
            "Epoch [20/100], Learning Rate: 0.0009821321585546244\n",
            "Epoch [21/100], Train Accuracy: 72.79 %, Loss: 0.7584\n",
            "Epoch [21/100], Learning Rate: 0.0009779754323328191\n",
            "Epoch [22/100], Train Accuracy: 72.928 %, Loss: 0.7279\n",
            "Epoch [22/100], Learning Rate: 0.0009733963460294015\n",
            "Epoch [23/100], Train Accuracy: 73.542 %, Loss: 0.7354\n",
            "Epoch [23/100], Learning Rate: 0.0009683994186497131\n",
            "Epoch [24/100], Train Accuracy: 73.9 %, Loss: 0.7232\n",
            "Epoch [24/100], Learning Rate: 0.0009629895815577914\n",
            "Epoch [25/100], Train Accuracy: 74.654 %, Loss: 0.6992\n",
            "Epoch [25/100], Learning Rate: 0.0009571721736097088\n",
            "Epoch [26/100], Train Accuracy: 74.536 %, Loss: 0.8096\n",
            "Epoch [26/100], Learning Rate: 0.0009509529358847655\n",
            "Epoch [27/100], Train Accuracy: 75.022 %, Loss: 0.7185\n",
            "Epoch [27/100], Learning Rate: 0.0009443380060197386\n",
            "Epoch [28/100], Train Accuracy: 75.374 %, Loss: 0.6641\n",
            "Epoch [28/100], Learning Rate: 0.0009373339121517746\n",
            "Epoch [29/100], Train Accuracy: 75.604 %, Loss: 0.7072\n",
            "Epoch [29/100], Learning Rate: 0.0009299475664759069\n",
            "Epoch [30/100], Train Accuracy: 75.894 %, Loss: 0.6420\n",
            "Epoch [30/100], Learning Rate: 0.0009221862584235528\n",
            "Epoch [31/100], Train Accuracy: 76.39 %, Loss: 0.6858\n",
            "Epoch [31/100], Learning Rate: 0.0009140576474687263\n",
            "Epoch [32/100], Train Accuracy: 76.428 %, Loss: 0.6129\n",
            "Epoch [32/100], Learning Rate: 0.0009055697555690606\n",
            "Epoch [33/100], Train Accuracy: 76.586 %, Loss: 0.7159\n",
            "Epoch [33/100], Learning Rate: 0.0008967309592491052\n",
            "Epoch [34/100], Train Accuracy: 76.916 %, Loss: 0.5748\n",
            "Epoch [34/100], Learning Rate: 0.0008875499813337069\n",
            "Epoch [35/100], Train Accuracy: 77.098 %, Loss: 0.6816\n",
            "Epoch [35/100], Learning Rate: 0.0008780358823396353\n",
            "Epoch [36/100], Train Accuracy: 76.998 %, Loss: 0.7072\n",
            "Epoch [36/100], Learning Rate: 0.0008681980515339464\n",
            "Epoch [37/100], Train Accuracy: 77.534 %, Loss: 0.6476\n",
            "Epoch [37/100], Learning Rate: 0.0008580461976679099\n",
            "Epoch [38/100], Train Accuracy: 77.548 %, Loss: 0.7486\n",
            "Epoch [38/100], Learning Rate: 0.0008475903393956434\n",
            "Epoch [39/100], Train Accuracy: 77.914 %, Loss: 0.6583\n",
            "Epoch [39/100], Learning Rate: 0.0008368407953869105\n",
            "Epoch [40/100], Train Accuracy: 78.038 %, Loss: 0.6054\n",
            "Epoch [40/100], Learning Rate: 0.0008258081741438395\n",
            "Epoch [41/100], Train Accuracy: 77.66 %, Loss: 0.5897\n",
            "Epoch [41/100], Learning Rate: 0.0008145033635316131\n",
            "Epoch [42/100], Train Accuracy: 78.684 %, Loss: 0.6387\n",
            "Epoch [42/100], Learning Rate: 0.0008029375200334589\n",
            "Epoch [43/100], Train Accuracy: 78.608 %, Loss: 0.7098\n",
            "Epoch [43/100], Learning Rate: 0.0007911220577405485\n",
            "Epoch [44/100], Train Accuracy: 79.044 %, Loss: 0.5569\n",
            "Epoch [44/100], Learning Rate: 0.0007790686370876671\n",
            "Epoch [45/100], Train Accuracy: 79.27 %, Loss: 0.5320\n",
            "Epoch [45/100], Learning Rate: 0.0007667891533457719\n",
            "Epoch [46/100], Train Accuracy: 79.298 %, Loss: 0.5853\n",
            "Epoch [46/100], Learning Rate: 0.000754295724882796\n",
            "Epoch [47/100], Train Accuracy: 79.662 %, Loss: 0.6132\n",
            "Epoch [47/100], Learning Rate: 0.0007416006812042827\n",
            "Epoch [48/100], Train Accuracy: 79.612 %, Loss: 0.5159\n",
            "Epoch [48/100], Learning Rate: 0.0007287165507856513\n",
            "Epoch [49/100], Train Accuracy: 79.864 %, Loss: 0.5838\n",
            "Epoch [49/100], Learning Rate: 0.0007156560487081052\n",
            "Epoch [50/100], Train Accuracy: 80.062 %, Loss: 0.6372\n",
            "Epoch [50/100], Learning Rate: 0.0007024320641103813\n",
            "Epoch [51/100], Train Accuracy: 80.244 %, Loss: 0.6494\n",
            "Epoch [51/100], Learning Rate: 0.0006890576474687264\n",
            "Epoch [52/100], Train Accuracy: 80.788 %, Loss: 0.5223\n",
            "Epoch [52/100], Learning Rate: 0.0006755459977176532\n",
            "Epoch [53/100], Train Accuracy: 80.892 %, Loss: 0.4676\n",
            "Epoch [53/100], Learning Rate: 0.0006619104492241846\n",
            "Epoch [54/100], Train Accuracy: 80.766 %, Loss: 0.5977\n",
            "Epoch [54/100], Learning Rate: 0.0006481644586284443\n",
            "Epoch [55/100], Train Accuracy: 80.99 %, Loss: 0.5061\n",
            "Epoch [55/100], Learning Rate: 0.0006343215915635762\n",
            "Epoch [56/100], Train Accuracy: 81.256 %, Loss: 0.5718\n",
            "Epoch [56/100], Learning Rate: 0.000620395509268104\n",
            "Epoch [57/100], Train Accuracy: 81.416 %, Loss: 0.6455\n",
            "Epoch [57/100], Learning Rate: 0.000606399955103937\n",
            "Epoch [58/100], Train Accuracy: 81.484 %, Loss: 0.4848\n",
            "Epoch [58/100], Learning Rate: 0.0005923487409933315\n",
            "Epoch [59/100], Train Accuracy: 81.734 %, Loss: 0.5094\n",
            "Epoch [59/100], Learning Rate: 0.0005782557337881911\n",
            "Epoch [60/100], Train Accuracy: 82.052 %, Loss: 0.5751\n",
            "Epoch [60/100], Learning Rate: 0.0005641348415851578\n",
            "Epoch [61/100], Train Accuracy: 82.058 %, Loss: 0.5524\n",
            "Epoch [61/100], Learning Rate: 0.00055\n",
            "Epoch [62/100], Train Accuracy: 82.186 %, Loss: 0.4906\n",
            "Epoch [62/100], Learning Rate: 0.0005358651584148423\n",
            "Epoch [63/100], Train Accuracy: 82.39 %, Loss: 0.5388\n",
            "Epoch [63/100], Learning Rate: 0.000521744266211809\n",
            "Epoch [64/100], Train Accuracy: 82.404 %, Loss: 0.4642\n",
            "Epoch [64/100], Learning Rate: 0.0005076512590066686\n",
            "Epoch [65/100], Train Accuracy: 83.002 %, Loss: 0.4429\n",
            "Epoch [65/100], Learning Rate: 0.0004936000448960632\n",
            "Epoch [66/100], Train Accuracy: 83.138 %, Loss: 0.5089\n",
            "Epoch [66/100], Learning Rate: 0.0004796044907318961\n",
            "Epoch [67/100], Train Accuracy: 83.122 %, Loss: 0.5679\n",
            "Epoch [67/100], Learning Rate: 0.0004656784084364239\n",
            "Epoch [68/100], Train Accuracy: 83.358 %, Loss: 0.5003\n",
            "Epoch [68/100], Learning Rate: 0.00045183554137155597\n",
            "Epoch [69/100], Train Accuracy: 83.616 %, Loss: 0.4467\n",
            "Epoch [69/100], Learning Rate: 0.0004380895507758153\n",
            "Epoch [70/100], Train Accuracy: 83.716 %, Loss: 0.5528\n",
            "Epoch [70/100], Learning Rate: 0.00042445400228234684\n",
            "Epoch [71/100], Train Accuracy: 84.054 %, Loss: 0.5790\n",
            "Epoch [71/100], Learning Rate: 0.0004109423525312738\n",
            "Epoch [72/100], Train Accuracy: 83.858 %, Loss: 0.5732\n",
            "Epoch [72/100], Learning Rate: 0.0003975679358896189\n",
            "Epoch [73/100], Train Accuracy: 84.196 %, Loss: 0.5012\n",
            "Epoch [73/100], Learning Rate: 0.000384343951291895\n",
            "Epoch [74/100], Train Accuracy: 84.218 %, Loss: 0.4155\n",
            "Epoch [74/100], Learning Rate: 0.00037128344921434864\n",
            "Epoch [75/100], Train Accuracy: 84.516 %, Loss: 0.4322\n",
            "Epoch [75/100], Learning Rate: 0.0003583993187957173\n",
            "Epoch [76/100], Train Accuracy: 84.892 %, Loss: 0.4428\n",
            "Epoch [76/100], Learning Rate: 0.00034570427511720393\n",
            "Epoch [77/100], Train Accuracy: 85.02 %, Loss: 0.4331\n",
            "Epoch [77/100], Learning Rate: 0.0003332108466542281\n",
            "Epoch [78/100], Train Accuracy: 85.242 %, Loss: 0.4546\n",
            "Epoch [78/100], Learning Rate: 0.0003209313629123329\n",
            "Epoch [79/100], Train Accuracy: 85.284 %, Loss: 0.3781\n",
            "Epoch [79/100], Learning Rate: 0.0003088779422594514\n",
            "Epoch [80/100], Train Accuracy: 85.534 %, Loss: 0.4022\n",
            "Epoch [80/100], Learning Rate: 0.0002970624799665412\n",
            "Epoch [81/100], Train Accuracy: 85.696 %, Loss: 0.4470\n",
            "Epoch [81/100], Learning Rate: 0.00028549663646838715\n",
            "Epoch [82/100], Train Accuracy: 85.692 %, Loss: 0.3612\n",
            "Epoch [82/100], Learning Rate: 0.00027419182585616055\n",
            "Epoch [83/100], Train Accuracy: 85.906 %, Loss: 0.3814\n",
            "Epoch [83/100], Learning Rate: 0.0002631592046130896\n",
            "Epoch [84/100], Train Accuracy: 86.052 %, Loss: 0.3150\n",
            "Epoch [84/100], Learning Rate: 0.00025240966060435674\n",
            "Epoch [85/100], Train Accuracy: 86.292 %, Loss: 0.4392\n",
            "Epoch [85/100], Learning Rate: 0.00024195380233209008\n",
            "Epoch [86/100], Train Accuracy: 86.432 %, Loss: 0.3597\n",
            "Epoch [86/100], Learning Rate: 0.00023180194846605365\n",
            "Epoch [87/100], Train Accuracy: 86.598 %, Loss: 0.3676\n",
            "Epoch [87/100], Learning Rate: 0.0002219641176603649\n",
            "Epoch [88/100], Train Accuracy: 86.688 %, Loss: 0.3688\n",
            "Epoch [88/100], Learning Rate: 0.0002124500186662932\n",
            "Epoch [89/100], Train Accuracy: 87.176 %, Loss: 0.3757\n",
            "Epoch [89/100], Learning Rate: 0.00020326904075089488\n",
            "Epoch [90/100], Train Accuracy: 87.292 %, Loss: 0.3391\n",
            "Epoch [90/100], Learning Rate: 0.00019443024443093932\n",
            "Epoch [91/100], Train Accuracy: 87.202 %, Loss: 0.3454\n",
            "Epoch [91/100], Learning Rate: 0.00018594235253127368\n",
            "Epoch [92/100], Train Accuracy: 87.364 %, Loss: 0.3350\n",
            "Epoch [92/100], Learning Rate: 0.00017781374157644722\n",
            "Epoch [93/100], Train Accuracy: 87.788 %, Loss: 0.3807\n",
            "Epoch [93/100], Learning Rate: 0.00017005243352409332\n",
            "Epoch [94/100], Train Accuracy: 87.922 %, Loss: 0.2768\n",
            "Epoch [94/100], Learning Rate: 0.0001626660878482253\n",
            "Epoch [95/100], Train Accuracy: 88.02 %, Loss: 0.2758\n",
            "Epoch [95/100], Learning Rate: 0.0001556619939802614\n",
            "Epoch [96/100], Train Accuracy: 88.41 %, Loss: 0.3308\n",
            "Epoch [96/100], Learning Rate: 0.0001490470641152345\n",
            "Epoch [97/100], Train Accuracy: 88.236 %, Loss: 0.2736\n",
            "Epoch [97/100], Learning Rate: 0.0001428278263902913\n",
            "Epoch [98/100], Train Accuracy: 88.38 %, Loss: 0.2629\n",
            "Epoch [98/100], Learning Rate: 0.00013701041844220858\n",
            "Epoch [99/100], Train Accuracy: 88.766 %, Loss: 0.3620\n",
            "Epoch [99/100], Learning Rate: 0.0001316005813502869\n",
            "Epoch [100/100], Train Accuracy: 88.71 %, Loss: 0.2909\n",
            "Epoch [100/100], Learning Rate: 0.00012660365397059855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習前後のモデルの重みを比較する関数を作成\n",
        "def check_weight_change(model, model_init):\n",
        "    weights_changed = (model.conv1.state_dict()['weight'] != model_init.conv1.state_dict()['weight']).any()\n",
        "    if weights_changed:\n",
        "        return print('モデルの重みが変化しています')\n",
        "    else:\n",
        "        return print('モデルの重みは変化していません')\n",
        "# 学習前後でモデルの重みが変化していないかを確認\n",
        "check_weight_change(model, model_init)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3093f6ac-4e73-478d-dbf7-68272734c93f",
        "id": "rzGurAlv1qF0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "モデルの重みは変化していません\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの評価\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for images, labels in test_loader_cifar10:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度の確認\n",
        "    print('Test Accuracy: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48a11291-fcab-4789-cb0d-5076e4a7ddd9",
        "id": "yACgDwql1qF0"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 79.36 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUメモリの解放\n",
        "del model, model_init\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "Q2knskGV1qF1"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prune_rate=0.5"
      ],
      "metadata": {
        "id": "xHXKrn9M6L_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 刈り込み率を設定\n",
        "prune_rate = 0.5\n",
        "# 初期化手法を設定\n",
        "init = 'kaiming_normal'\n",
        "\n",
        "# デバイスを設定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# モデルを定義\n",
        "model = supermaskresnet18_10().to(device)\n",
        "# 学習前のモデルの重みを保存\n",
        "model_init = copy.deepcopy(model)\n",
        "\n",
        "# 学習率を設定\n",
        "learning_rate = 0.001\n",
        "# 全体のepoch数を設定\n",
        "num_epochs = 100\n",
        "# warm-upするepoch数を設定\n",
        "warmup_epochs = 10\n",
        "\n",
        "# 損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# オプティマイザーを設定\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# スケジューラーを設定\n",
        "scheduler = CosineLRScheduler(optimizer, t_initial=num_epochs, lr_min=1e-4, warmup_t=warmup_epochs, warmup_lr_init=1e-4, warmup_prefix=True)\n",
        "\n",
        "# モデルの学習\n",
        "for epoch in range(num_epochs):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, (images, labels) in enumerate(train_loader_cifar10):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # 順伝播\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # 逆伝播\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # 全データ数\n",
        "        total += labels.size(0)\n",
        "        # 正解数\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度を計算\n",
        "    train_acc = 100 * correct / total\n",
        "    # 正解率精度と損失を確認\n",
        "    print(\"Epoch [{}/{}], Train Accuracy: {} %, Loss: {:.4f}\".format(epoch+1, num_epochs, 100 * correct / total, loss.item()))\n",
        "    # 1エポック終了後にスケジューラーを更新\n",
        "    scheduler.step(epoch)\n",
        "    # 学習率の確認\n",
        "    print(\"Epoch [{}/{}], Learning Rate: {}\".format(epoch+1, num_epochs, optimizer.param_groups[0]['lr']))\n",
        "\n",
        "# モデルを保存\n",
        "torch.save(model, '/content/drive/MyDrive/SupermaskResNet18_CIFAR10_100epochs_pr0.5_normal_CLRS_restest.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pn7JSQKp6L_s",
        "outputId": "a62ffd99-1737-4022-f29b-32be6ba6f970"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Accuracy: 32.784 %, Loss: 1.7269\n",
            "Epoch [1/100], Learning Rate: 0.0001\n",
            "Epoch [2/100], Train Accuracy: 43.128 %, Loss: 1.4809\n",
            "Epoch [2/100], Learning Rate: 0.00019\n",
            "Epoch [3/100], Train Accuracy: 47.176 %, Loss: 1.4519\n",
            "Epoch [3/100], Learning Rate: 0.00028\n",
            "Epoch [4/100], Train Accuracy: 50.9 %, Loss: 1.3397\n",
            "Epoch [4/100], Learning Rate: 0.00036999999999999994\n",
            "Epoch [5/100], Train Accuracy: 54.446 %, Loss: 1.2091\n",
            "Epoch [5/100], Learning Rate: 0.00045999999999999996\n",
            "Epoch [6/100], Train Accuracy: 56.918 %, Loss: 1.1571\n",
            "Epoch [6/100], Learning Rate: 0.00055\n",
            "Epoch [7/100], Train Accuracy: 59.246 %, Loss: 1.0938\n",
            "Epoch [7/100], Learning Rate: 0.0006399999999999999\n",
            "Epoch [8/100], Train Accuracy: 61.332 %, Loss: 1.0241\n",
            "Epoch [8/100], Learning Rate: 0.00073\n",
            "Epoch [9/100], Train Accuracy: 62.552 %, Loss: 1.0357\n",
            "Epoch [9/100], Learning Rate: 0.00082\n",
            "Epoch [10/100], Train Accuracy: 63.758 %, Loss: 0.8746\n",
            "Epoch [10/100], Learning Rate: 0.00091\n",
            "Epoch [11/100], Train Accuracy: 65.222 %, Loss: 0.9663\n",
            "Epoch [11/100], Learning Rate: 0.001\n",
            "Epoch [12/100], Train Accuracy: 66.1 %, Loss: 0.8609\n",
            "Epoch [12/100], Learning Rate: 0.0009997779521645793\n",
            "Epoch [13/100], Train Accuracy: 67.37 %, Loss: 0.8894\n",
            "Epoch [13/100], Learning Rate: 0.0009991120277927223\n",
            "Epoch [14/100], Train Accuracy: 68.162 %, Loss: 0.8700\n",
            "Epoch [14/100], Learning Rate: 0.000998002884071386\n",
            "Epoch [15/100], Train Accuracy: 69.37 %, Loss: 0.8492\n",
            "Epoch [15/100], Learning Rate: 0.000996451615591515\n",
            "Epoch [16/100], Train Accuracy: 69.642 %, Loss: 0.9125\n",
            "Epoch [16/100], Learning Rate: 0.000994459753267812\n",
            "Epoch [17/100], Train Accuracy: 70.544 %, Loss: 0.8956\n",
            "Epoch [17/100], Learning Rate: 0.00099202926282791\n",
            "Epoch [18/100], Train Accuracy: 71.408 %, Loss: 0.8853\n",
            "Epoch [18/100], Learning Rate: 0.0009891625428724363\n",
            "Epoch [19/100], Train Accuracy: 71.83 %, Loss: 0.8570\n",
            "Epoch [19/100], Learning Rate: 0.000985862422507884\n",
            "Epoch [20/100], Train Accuracy: 71.546 %, Loss: 0.8637\n",
            "Epoch [20/100], Learning Rate: 0.0009821321585546244\n",
            "Epoch [21/100], Train Accuracy: 72.028 %, Loss: 0.7541\n",
            "Epoch [21/100], Learning Rate: 0.0009779754323328191\n",
            "Epoch [22/100], Train Accuracy: 72.804 %, Loss: 0.7327\n",
            "Epoch [22/100], Learning Rate: 0.0009733963460294015\n",
            "Epoch [23/100], Train Accuracy: 73.448 %, Loss: 0.7459\n",
            "Epoch [23/100], Learning Rate: 0.0009683994186497131\n",
            "Epoch [24/100], Train Accuracy: 73.13 %, Loss: 0.8210\n",
            "Epoch [24/100], Learning Rate: 0.0009629895815577914\n",
            "Epoch [25/100], Train Accuracy: 73.52 %, Loss: 0.7032\n",
            "Epoch [25/100], Learning Rate: 0.0009571721736097088\n",
            "Epoch [26/100], Train Accuracy: 74.136 %, Loss: 0.7707\n",
            "Epoch [26/100], Learning Rate: 0.0009509529358847655\n",
            "Epoch [27/100], Train Accuracy: 74.572 %, Loss: 0.6984\n",
            "Epoch [27/100], Learning Rate: 0.0009443380060197386\n",
            "Epoch [28/100], Train Accuracy: 74.414 %, Loss: 0.7501\n",
            "Epoch [28/100], Learning Rate: 0.0009373339121517746\n",
            "Epoch [29/100], Train Accuracy: 74.784 %, Loss: 0.7755\n",
            "Epoch [29/100], Learning Rate: 0.0009299475664759069\n",
            "Epoch [30/100], Train Accuracy: 75.142 %, Loss: 0.6699\n",
            "Epoch [30/100], Learning Rate: 0.0009221862584235528\n",
            "Epoch [31/100], Train Accuracy: 75.454 %, Loss: 0.6655\n",
            "Epoch [31/100], Learning Rate: 0.0009140576474687263\n",
            "Epoch [32/100], Train Accuracy: 75.418 %, Loss: 0.6670\n",
            "Epoch [32/100], Learning Rate: 0.0009055697555690606\n",
            "Epoch [33/100], Train Accuracy: 75.882 %, Loss: 0.8053\n",
            "Epoch [33/100], Learning Rate: 0.0008967309592491052\n",
            "Epoch [34/100], Train Accuracy: 76.072 %, Loss: 0.8073\n",
            "Epoch [34/100], Learning Rate: 0.0008875499813337069\n",
            "Epoch [35/100], Train Accuracy: 76.286 %, Loss: 0.6846\n",
            "Epoch [35/100], Learning Rate: 0.0008780358823396353\n",
            "Epoch [36/100], Train Accuracy: 76.488 %, Loss: 0.7743\n",
            "Epoch [36/100], Learning Rate: 0.0008681980515339464\n",
            "Epoch [37/100], Train Accuracy: 76.898 %, Loss: 0.7397\n",
            "Epoch [37/100], Learning Rate: 0.0008580461976679099\n",
            "Epoch [38/100], Train Accuracy: 76.918 %, Loss: 0.6235\n",
            "Epoch [38/100], Learning Rate: 0.0008475903393956434\n",
            "Epoch [39/100], Train Accuracy: 76.646 %, Loss: 0.6816\n",
            "Epoch [39/100], Learning Rate: 0.0008368407953869105\n",
            "Epoch [40/100], Train Accuracy: 77.26 %, Loss: 0.6399\n",
            "Epoch [40/100], Learning Rate: 0.0008258081741438395\n",
            "Epoch [41/100], Train Accuracy: 77.628 %, Loss: 0.6501\n",
            "Epoch [41/100], Learning Rate: 0.0008145033635316131\n",
            "Epoch [42/100], Train Accuracy: 77.694 %, Loss: 0.6327\n",
            "Epoch [42/100], Learning Rate: 0.0008029375200334589\n",
            "Epoch [43/100], Train Accuracy: 77.712 %, Loss: 0.6617\n",
            "Epoch [43/100], Learning Rate: 0.0007911220577405485\n",
            "Epoch [44/100], Train Accuracy: 78.266 %, Loss: 0.6288\n",
            "Epoch [44/100], Learning Rate: 0.0007790686370876671\n",
            "Epoch [45/100], Train Accuracy: 78.558 %, Loss: 0.6056\n",
            "Epoch [45/100], Learning Rate: 0.0007667891533457719\n",
            "Epoch [46/100], Train Accuracy: 78.16 %, Loss: 0.6390\n",
            "Epoch [46/100], Learning Rate: 0.000754295724882796\n",
            "Epoch [47/100], Train Accuracy: 78.682 %, Loss: 0.6101\n",
            "Epoch [47/100], Learning Rate: 0.0007416006812042827\n",
            "Epoch [48/100], Train Accuracy: 79.044 %, Loss: 0.6468\n",
            "Epoch [48/100], Learning Rate: 0.0007287165507856513\n",
            "Epoch [49/100], Train Accuracy: 79.228 %, Loss: 0.6840\n",
            "Epoch [49/100], Learning Rate: 0.0007156560487081052\n",
            "Epoch [50/100], Train Accuracy: 79.196 %, Loss: 0.7054\n",
            "Epoch [50/100], Learning Rate: 0.0007024320641103813\n",
            "Epoch [51/100], Train Accuracy: 79.47 %, Loss: 0.6279\n",
            "Epoch [51/100], Learning Rate: 0.0006890576474687264\n",
            "Epoch [52/100], Train Accuracy: 79.972 %, Loss: 0.6189\n",
            "Epoch [52/100], Learning Rate: 0.0006755459977176532\n",
            "Epoch [53/100], Train Accuracy: 79.992 %, Loss: 0.6165\n",
            "Epoch [53/100], Learning Rate: 0.0006619104492241846\n",
            "Epoch [54/100], Train Accuracy: 80.332 %, Loss: 0.4619\n",
            "Epoch [54/100], Learning Rate: 0.0006481644586284443\n",
            "Epoch [55/100], Train Accuracy: 79.732 %, Loss: 0.5615\n",
            "Epoch [55/100], Learning Rate: 0.0006343215915635762\n",
            "Epoch [56/100], Train Accuracy: 80.488 %, Loss: 0.6382\n",
            "Epoch [56/100], Learning Rate: 0.000620395509268104\n",
            "Epoch [57/100], Train Accuracy: 80.838 %, Loss: 0.5500\n",
            "Epoch [57/100], Learning Rate: 0.000606399955103937\n",
            "Epoch [58/100], Train Accuracy: 80.794 %, Loss: 0.5839\n",
            "Epoch [58/100], Learning Rate: 0.0005923487409933315\n",
            "Epoch [59/100], Train Accuracy: 80.86 %, Loss: 0.5357\n",
            "Epoch [59/100], Learning Rate: 0.0005782557337881911\n",
            "Epoch [60/100], Train Accuracy: 81.434 %, Loss: 0.4690\n",
            "Epoch [60/100], Learning Rate: 0.0005641348415851578\n",
            "Epoch [61/100], Train Accuracy: 81.264 %, Loss: 0.5316\n",
            "Epoch [61/100], Learning Rate: 0.00055\n",
            "Epoch [62/100], Train Accuracy: 81.208 %, Loss: 0.5445\n",
            "Epoch [62/100], Learning Rate: 0.0005358651584148423\n",
            "Epoch [63/100], Train Accuracy: 81.758 %, Loss: 0.5413\n",
            "Epoch [63/100], Learning Rate: 0.000521744266211809\n",
            "Epoch [64/100], Train Accuracy: 81.692 %, Loss: 0.5260\n",
            "Epoch [64/100], Learning Rate: 0.0005076512590066686\n",
            "Epoch [65/100], Train Accuracy: 82.442 %, Loss: 0.4568\n",
            "Epoch [65/100], Learning Rate: 0.0004936000448960632\n",
            "Epoch [66/100], Train Accuracy: 82.214 %, Loss: 0.5321\n",
            "Epoch [66/100], Learning Rate: 0.0004796044907318961\n",
            "Epoch [67/100], Train Accuracy: 82.364 %, Loss: 0.4746\n",
            "Epoch [67/100], Learning Rate: 0.0004656784084364239\n",
            "Epoch [68/100], Train Accuracy: 82.536 %, Loss: 0.5411\n",
            "Epoch [68/100], Learning Rate: 0.00045183554137155597\n",
            "Epoch [69/100], Train Accuracy: 82.62 %, Loss: 0.5324\n",
            "Epoch [69/100], Learning Rate: 0.0004380895507758153\n",
            "Epoch [70/100], Train Accuracy: 82.91 %, Loss: 0.4751\n",
            "Epoch [70/100], Learning Rate: 0.00042445400228234684\n",
            "Epoch [71/100], Train Accuracy: 83.244 %, Loss: 0.4761\n",
            "Epoch [71/100], Learning Rate: 0.0004109423525312738\n",
            "Epoch [72/100], Train Accuracy: 83.578 %, Loss: 0.4838\n",
            "Epoch [72/100], Learning Rate: 0.0003975679358896189\n",
            "Epoch [73/100], Train Accuracy: 83.474 %, Loss: 0.5042\n",
            "Epoch [73/100], Learning Rate: 0.000384343951291895\n",
            "Epoch [74/100], Train Accuracy: 83.624 %, Loss: 0.4889\n",
            "Epoch [74/100], Learning Rate: 0.00037128344921434864\n",
            "Epoch [75/100], Train Accuracy: 83.862 %, Loss: 0.4102\n",
            "Epoch [75/100], Learning Rate: 0.0003583993187957173\n",
            "Epoch [76/100], Train Accuracy: 84.052 %, Loss: 0.5267\n",
            "Epoch [76/100], Learning Rate: 0.00034570427511720393\n",
            "Epoch [77/100], Train Accuracy: 84.274 %, Loss: 0.5611\n",
            "Epoch [77/100], Learning Rate: 0.0003332108466542281\n",
            "Epoch [78/100], Train Accuracy: 84.552 %, Loss: 0.4503\n",
            "Epoch [78/100], Learning Rate: 0.0003209313629123329\n",
            "Epoch [79/100], Train Accuracy: 84.958 %, Loss: 0.3909\n",
            "Epoch [79/100], Learning Rate: 0.0003088779422594514\n",
            "Epoch [80/100], Train Accuracy: 85.008 %, Loss: 0.4722\n",
            "Epoch [80/100], Learning Rate: 0.0002970624799665412\n",
            "Epoch [81/100], Train Accuracy: 85.208 %, Loss: 0.4437\n",
            "Epoch [81/100], Learning Rate: 0.00028549663646838715\n",
            "Epoch [82/100], Train Accuracy: 85.328 %, Loss: 0.4272\n",
            "Epoch [82/100], Learning Rate: 0.00027419182585616055\n",
            "Epoch [83/100], Train Accuracy: 85.458 %, Loss: 0.4023\n",
            "Epoch [83/100], Learning Rate: 0.0002631592046130896\n",
            "Epoch [84/100], Train Accuracy: 85.604 %, Loss: 0.4136\n",
            "Epoch [84/100], Learning Rate: 0.00025240966060435674\n",
            "Epoch [85/100], Train Accuracy: 85.828 %, Loss: 0.4207\n",
            "Epoch [85/100], Learning Rate: 0.00024195380233209008\n",
            "Epoch [86/100], Train Accuracy: 85.988 %, Loss: 0.4649\n",
            "Epoch [86/100], Learning Rate: 0.00023180194846605365\n",
            "Epoch [87/100], Train Accuracy: 86.03 %, Loss: 0.4050\n",
            "Epoch [87/100], Learning Rate: 0.0002219641176603649\n",
            "Epoch [88/100], Train Accuracy: 86.394 %, Loss: 0.4389\n",
            "Epoch [88/100], Learning Rate: 0.0002124500186662932\n",
            "Epoch [89/100], Train Accuracy: 86.546 %, Loss: 0.4022\n",
            "Epoch [89/100], Learning Rate: 0.00020326904075089488\n",
            "Epoch [90/100], Train Accuracy: 86.67 %, Loss: 0.4300\n",
            "Epoch [90/100], Learning Rate: 0.00019443024443093932\n",
            "Epoch [91/100], Train Accuracy: 86.716 %, Loss: 0.3821\n",
            "Epoch [91/100], Learning Rate: 0.00018594235253127368\n",
            "Epoch [92/100], Train Accuracy: 87.16 %, Loss: 0.3034\n",
            "Epoch [92/100], Learning Rate: 0.00017781374157644722\n",
            "Epoch [93/100], Train Accuracy: 87.404 %, Loss: 0.3930\n",
            "Epoch [93/100], Learning Rate: 0.00017005243352409332\n",
            "Epoch [94/100], Train Accuracy: 87.534 %, Loss: 0.2927\n",
            "Epoch [94/100], Learning Rate: 0.0001626660878482253\n",
            "Epoch [95/100], Train Accuracy: 87.38 %, Loss: 0.3512\n",
            "Epoch [95/100], Learning Rate: 0.0001556619939802614\n",
            "Epoch [96/100], Train Accuracy: 87.772 %, Loss: 0.3385\n",
            "Epoch [96/100], Learning Rate: 0.0001490470641152345\n",
            "Epoch [97/100], Train Accuracy: 87.884 %, Loss: 0.3402\n",
            "Epoch [97/100], Learning Rate: 0.0001428278263902913\n",
            "Epoch [98/100], Train Accuracy: 88.12 %, Loss: 0.3034\n",
            "Epoch [98/100], Learning Rate: 0.00013701041844220858\n",
            "Epoch [99/100], Train Accuracy: 88.304 %, Loss: 0.3192\n",
            "Epoch [99/100], Learning Rate: 0.0001316005813502869\n",
            "Epoch [100/100], Train Accuracy: 88.556 %, Loss: 0.3040\n",
            "Epoch [100/100], Learning Rate: 0.00012660365397059855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習前後のモデルの重みを比較する関数を作成\n",
        "def check_weight_change(model, model_init):\n",
        "    weights_changed = (model.conv1.state_dict()['weight'] != model_init.conv1.state_dict()['weight']).any()\n",
        "    if weights_changed:\n",
        "        return print('モデルの重みが変化しています')\n",
        "    else:\n",
        "        return print('モデルの重みは変化していません')\n",
        "# 学習前後でモデルの重みが変化していないかを確認\n",
        "check_weight_change(model, model_init)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyJ3c5ZL6L_t",
        "outputId": "407e8076-4826-4e87-ed4e-24f2d2b977fc"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "モデルの重みは変化していません\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの評価\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for images, labels in test_loader_cifar10:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度の確認\n",
        "    print('Test Accuracy: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0k4YheG6L_t",
        "outputId": "4d006e85-5e09-4bb7-e763-a54e1b378dd6"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 79.28 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUメモリの解放\n",
        "del model, model_init\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "BtBgp5iQ6L_t"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 学習と評価（100 epochs, init=signed_constant）"
      ],
      "metadata": {
        "id": "_i257mg66Lan"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prune_rate=0.999"
      ],
      "metadata": {
        "id": "hTJyxZwlTU2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 刈り込み率を設定\n",
        "prune_rate = 0.999\n",
        "# 初期化手法を設定\n",
        "init = 'signed_constant'\n",
        "\n",
        "# デバイスを設定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# モデルを定義\n",
        "model = supermaskresnet18_10().to(device)\n",
        "# 学習前のモデルの重みを保存\n",
        "model_init = copy.deepcopy(model)\n",
        "\n",
        "# 学習率を設定\n",
        "learning_rate = 0.001\n",
        "# 全体のepoch数を設定\n",
        "num_epochs = 100\n",
        "# warm-upするepoch数を設定\n",
        "warmup_epochs = 10\n",
        "\n",
        "# 損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# オプティマイザーを設定\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# スケジューラーを設定\n",
        "scheduler = CosineLRScheduler(optimizer, t_initial=num_epochs, lr_min=1e-4, warmup_t=warmup_epochs, warmup_lr_init=1e-4, warmup_prefix=True)\n",
        "\n",
        "# モデルの学習\n",
        "for epoch in range(num_epochs):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, (images, labels) in enumerate(train_loader_cifar10):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # 順伝播\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # 逆伝播\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # 全データ数\n",
        "        total += labels.size(0)\n",
        "        # 正解数\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度を計算\n",
        "    train_acc = 100 * correct / total\n",
        "    # 正解率精度と損失を確認\n",
        "    print(\"Epoch [{}/{}], Train Accuracy: {} %, Loss: {:.4f}\".format(epoch+1, num_epochs, 100 * correct / total, loss.item()))\n",
        "    # 1エポック終了後にスケジューラーを更新\n",
        "    scheduler.step(epoch)\n",
        "    # 学習率の確認\n",
        "    print(\"Epoch [{}/{}], Learning Rate: {}\".format(epoch+1, num_epochs, optimizer.param_groups[0]['lr']))\n",
        "\n",
        "# モデルを保存\n",
        "torch.save(model, '/content/drive/MyDrive/SupermaskResNet18_CIFAR10_100epochs_pr0.999_constant_CLRS_restest.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08ab8742-23e1-4760-bc9e-adce7255496d",
        "id": "bRD4ihZETU2i"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Accuracy: 16.682 %, Loss: 2.1118\n",
            "Epoch [1/100], Learning Rate: 0.0001\n",
            "Epoch [2/100], Train Accuracy: 24.346 %, Loss: 1.9559\n",
            "Epoch [2/100], Learning Rate: 0.00019\n",
            "Epoch [3/100], Train Accuracy: 25.782 %, Loss: 1.8917\n",
            "Epoch [3/100], Learning Rate: 0.00028\n",
            "Epoch [4/100], Train Accuracy: 27.698 %, Loss: 1.8479\n",
            "Epoch [4/100], Learning Rate: 0.00036999999999999994\n",
            "Epoch [5/100], Train Accuracy: 29.362 %, Loss: 1.8372\n",
            "Epoch [5/100], Learning Rate: 0.00045999999999999996\n",
            "Epoch [6/100], Train Accuracy: 29.714 %, Loss: 1.9074\n",
            "Epoch [6/100], Learning Rate: 0.00055\n",
            "Epoch [7/100], Train Accuracy: 31.064 %, Loss: 1.8949\n",
            "Epoch [7/100], Learning Rate: 0.0006399999999999999\n",
            "Epoch [8/100], Train Accuracy: 31.732 %, Loss: 1.7480\n",
            "Epoch [8/100], Learning Rate: 0.00073\n",
            "Epoch [9/100], Train Accuracy: 31.858 %, Loss: 1.7091\n",
            "Epoch [9/100], Learning Rate: 0.00082\n",
            "Epoch [10/100], Train Accuracy: 32.68 %, Loss: 1.7678\n",
            "Epoch [10/100], Learning Rate: 0.00091\n",
            "Epoch [11/100], Train Accuracy: 33.484 %, Loss: 1.7294\n",
            "Epoch [11/100], Learning Rate: 0.001\n",
            "Epoch [12/100], Train Accuracy: 33.572 %, Loss: 1.7847\n",
            "Epoch [12/100], Learning Rate: 0.0009997779521645793\n",
            "Epoch [13/100], Train Accuracy: 34.184 %, Loss: 1.8031\n",
            "Epoch [13/100], Learning Rate: 0.0009991120277927223\n",
            "Epoch [14/100], Train Accuracy: 34.168 %, Loss: 1.7526\n",
            "Epoch [14/100], Learning Rate: 0.000998002884071386\n",
            "Epoch [15/100], Train Accuracy: 34.762 %, Loss: 1.7269\n",
            "Epoch [15/100], Learning Rate: 0.000996451615591515\n",
            "Epoch [16/100], Train Accuracy: 34.518 %, Loss: 1.6702\n",
            "Epoch [16/100], Learning Rate: 0.000994459753267812\n",
            "Epoch [17/100], Train Accuracy: 35.178 %, Loss: 1.6856\n",
            "Epoch [17/100], Learning Rate: 0.00099202926282791\n",
            "Epoch [18/100], Train Accuracy: 35.21 %, Loss: 1.6250\n",
            "Epoch [18/100], Learning Rate: 0.0009891625428724363\n",
            "Epoch [19/100], Train Accuracy: 35.504 %, Loss: 1.8515\n",
            "Epoch [19/100], Learning Rate: 0.000985862422507884\n",
            "Epoch [20/100], Train Accuracy: 35.002 %, Loss: 1.7865\n",
            "Epoch [20/100], Learning Rate: 0.0009821321585546244\n",
            "Epoch [21/100], Train Accuracy: 35.462 %, Loss: 1.7876\n",
            "Epoch [21/100], Learning Rate: 0.0009779754323328191\n",
            "Epoch [22/100], Train Accuracy: 35.296 %, Loss: 1.9184\n",
            "Epoch [22/100], Learning Rate: 0.0009733963460294015\n",
            "Epoch [23/100], Train Accuracy: 36.166 %, Loss: 1.7446\n",
            "Epoch [23/100], Learning Rate: 0.0009683994186497131\n",
            "Epoch [24/100], Train Accuracy: 36.484 %, Loss: 1.8535\n",
            "Epoch [24/100], Learning Rate: 0.0009629895815577914\n",
            "Epoch [25/100], Train Accuracy: 36.04 %, Loss: 1.9257\n",
            "Epoch [25/100], Learning Rate: 0.0009571721736097088\n",
            "Epoch [26/100], Train Accuracy: 36.378 %, Loss: 1.7151\n",
            "Epoch [26/100], Learning Rate: 0.0009509529358847655\n",
            "Epoch [27/100], Train Accuracy: 35.292 %, Loss: 1.7927\n",
            "Epoch [27/100], Learning Rate: 0.0009443380060197386\n",
            "Epoch [28/100], Train Accuracy: 36.982 %, Loss: 1.7751\n",
            "Epoch [28/100], Learning Rate: 0.0009373339121517746\n",
            "Epoch [29/100], Train Accuracy: 36.68 %, Loss: 1.7634\n",
            "Epoch [29/100], Learning Rate: 0.0009299475664759069\n",
            "Epoch [30/100], Train Accuracy: 37.064 %, Loss: 1.7491\n",
            "Epoch [30/100], Learning Rate: 0.0009221862584235528\n",
            "Epoch [31/100], Train Accuracy: 36.3 %, Loss: 1.7855\n",
            "Epoch [31/100], Learning Rate: 0.0009140576474687263\n",
            "Epoch [32/100], Train Accuracy: 33.12 %, Loss: 1.8053\n",
            "Epoch [32/100], Learning Rate: 0.0009055697555690606\n",
            "Epoch [33/100], Train Accuracy: 33.77 %, Loss: 1.8073\n",
            "Epoch [33/100], Learning Rate: 0.0008967309592491052\n",
            "Epoch [34/100], Train Accuracy: 33.56 %, Loss: 1.7609\n",
            "Epoch [34/100], Learning Rate: 0.0008875499813337069\n",
            "Epoch [35/100], Train Accuracy: 33.468 %, Loss: 1.8249\n",
            "Epoch [35/100], Learning Rate: 0.0008780358823396353\n",
            "Epoch [36/100], Train Accuracy: 34.156 %, Loss: 1.8149\n",
            "Epoch [36/100], Learning Rate: 0.0008681980515339464\n",
            "Epoch [37/100], Train Accuracy: 34.202 %, Loss: 1.7716\n",
            "Epoch [37/100], Learning Rate: 0.0008580461976679099\n",
            "Epoch [38/100], Train Accuracy: 34.552 %, Loss: 1.7341\n",
            "Epoch [38/100], Learning Rate: 0.0008475903393956434\n",
            "Epoch [39/100], Train Accuracy: 34.3 %, Loss: 1.9246\n",
            "Epoch [39/100], Learning Rate: 0.0008368407953869105\n",
            "Epoch [40/100], Train Accuracy: 34.118 %, Loss: 1.8946\n",
            "Epoch [40/100], Learning Rate: 0.0008258081741438395\n",
            "Epoch [41/100], Train Accuracy: 34.85 %, Loss: 1.8791\n",
            "Epoch [41/100], Learning Rate: 0.0008145033635316131\n",
            "Epoch [42/100], Train Accuracy: 34.112 %, Loss: 1.7626\n",
            "Epoch [42/100], Learning Rate: 0.0008029375200334589\n",
            "Epoch [43/100], Train Accuracy: 34.684 %, Loss: 1.7602\n",
            "Epoch [43/100], Learning Rate: 0.0007911220577405485\n",
            "Epoch [44/100], Train Accuracy: 34.34 %, Loss: 1.7127\n",
            "Epoch [44/100], Learning Rate: 0.0007790686370876671\n",
            "Epoch [45/100], Train Accuracy: 34.946 %, Loss: 1.7400\n",
            "Epoch [45/100], Learning Rate: 0.0007667891533457719\n",
            "Epoch [46/100], Train Accuracy: 34.174 %, Loss: 1.8155\n",
            "Epoch [46/100], Learning Rate: 0.000754295724882796\n",
            "Epoch [47/100], Train Accuracy: 34.642 %, Loss: 1.6978\n",
            "Epoch [47/100], Learning Rate: 0.0007416006812042827\n",
            "Epoch [48/100], Train Accuracy: 35.034 %, Loss: 1.6776\n",
            "Epoch [48/100], Learning Rate: 0.0007287165507856513\n",
            "Epoch [49/100], Train Accuracy: 34.786 %, Loss: 1.6495\n",
            "Epoch [49/100], Learning Rate: 0.0007156560487081052\n",
            "Epoch [50/100], Train Accuracy: 34.53 %, Loss: 1.6775\n",
            "Epoch [50/100], Learning Rate: 0.0007024320641103813\n",
            "Epoch [51/100], Train Accuracy: 34.81 %, Loss: 1.7843\n",
            "Epoch [51/100], Learning Rate: 0.0006890576474687264\n",
            "Epoch [52/100], Train Accuracy: 34.606 %, Loss: 1.9971\n",
            "Epoch [52/100], Learning Rate: 0.0006755459977176532\n",
            "Epoch [53/100], Train Accuracy: 35.096 %, Loss: 1.7295\n",
            "Epoch [53/100], Learning Rate: 0.0006619104492241846\n",
            "Epoch [54/100], Train Accuracy: 35.18 %, Loss: 1.6753\n",
            "Epoch [54/100], Learning Rate: 0.0006481644586284443\n",
            "Epoch [55/100], Train Accuracy: 35.232 %, Loss: 1.8499\n",
            "Epoch [55/100], Learning Rate: 0.0006343215915635762\n",
            "Epoch [56/100], Train Accuracy: 35.208 %, Loss: 1.7509\n",
            "Epoch [56/100], Learning Rate: 0.000620395509268104\n",
            "Epoch [57/100], Train Accuracy: 35.048 %, Loss: 1.7751\n",
            "Epoch [57/100], Learning Rate: 0.000606399955103937\n",
            "Epoch [58/100], Train Accuracy: 34.828 %, Loss: 1.7488\n",
            "Epoch [58/100], Learning Rate: 0.0005923487409933315\n",
            "Epoch [59/100], Train Accuracy: 35.418 %, Loss: 1.7050\n",
            "Epoch [59/100], Learning Rate: 0.0005782557337881911\n",
            "Epoch [60/100], Train Accuracy: 35.24 %, Loss: 1.6860\n",
            "Epoch [60/100], Learning Rate: 0.0005641348415851578\n",
            "Epoch [61/100], Train Accuracy: 35.354 %, Loss: 1.7641\n",
            "Epoch [61/100], Learning Rate: 0.00055\n",
            "Epoch [62/100], Train Accuracy: 35.644 %, Loss: 1.7143\n",
            "Epoch [62/100], Learning Rate: 0.0005358651584148423\n",
            "Epoch [63/100], Train Accuracy: 35.252 %, Loss: 1.7600\n",
            "Epoch [63/100], Learning Rate: 0.000521744266211809\n",
            "Epoch [64/100], Train Accuracy: 35.454 %, Loss: 1.6440\n",
            "Epoch [64/100], Learning Rate: 0.0005076512590066686\n",
            "Epoch [65/100], Train Accuracy: 35.36 %, Loss: 1.7219\n",
            "Epoch [65/100], Learning Rate: 0.0004936000448960632\n",
            "Epoch [66/100], Train Accuracy: 35.384 %, Loss: 1.7899\n",
            "Epoch [66/100], Learning Rate: 0.0004796044907318961\n",
            "Epoch [67/100], Train Accuracy: 35.74 %, Loss: 1.5319\n",
            "Epoch [67/100], Learning Rate: 0.0004656784084364239\n",
            "Epoch [68/100], Train Accuracy: 35.254 %, Loss: 1.7904\n",
            "Epoch [68/100], Learning Rate: 0.00045183554137155597\n",
            "Epoch [69/100], Train Accuracy: 36.058 %, Loss: 1.8452\n",
            "Epoch [69/100], Learning Rate: 0.0004380895507758153\n",
            "Epoch [70/100], Train Accuracy: 35.91 %, Loss: 1.7919\n",
            "Epoch [70/100], Learning Rate: 0.00042445400228234684\n",
            "Epoch [71/100], Train Accuracy: 35.832 %, Loss: 1.7008\n",
            "Epoch [71/100], Learning Rate: 0.0004109423525312738\n",
            "Epoch [72/100], Train Accuracy: 36.016 %, Loss: 1.8267\n",
            "Epoch [72/100], Learning Rate: 0.0003975679358896189\n",
            "Epoch [73/100], Train Accuracy: 35.218 %, Loss: 1.7542\n",
            "Epoch [73/100], Learning Rate: 0.000384343951291895\n",
            "Epoch [74/100], Train Accuracy: 35.316 %, Loss: 1.7168\n",
            "Epoch [74/100], Learning Rate: 0.00037128344921434864\n",
            "Epoch [75/100], Train Accuracy: 35.754 %, Loss: 1.7864\n",
            "Epoch [75/100], Learning Rate: 0.0003583993187957173\n",
            "Epoch [76/100], Train Accuracy: 36.092 %, Loss: 1.7251\n",
            "Epoch [76/100], Learning Rate: 0.00034570427511720393\n",
            "Epoch [77/100], Train Accuracy: 35.686 %, Loss: 1.8804\n",
            "Epoch [77/100], Learning Rate: 0.0003332108466542281\n",
            "Epoch [78/100], Train Accuracy: 36.002 %, Loss: 1.6560\n",
            "Epoch [78/100], Learning Rate: 0.0003209313629123329\n",
            "Epoch [79/100], Train Accuracy: 36.218 %, Loss: 1.7685\n",
            "Epoch [79/100], Learning Rate: 0.0003088779422594514\n",
            "Epoch [80/100], Train Accuracy: 35.95 %, Loss: 1.7279\n",
            "Epoch [80/100], Learning Rate: 0.0002970624799665412\n",
            "Epoch [81/100], Train Accuracy: 35.814 %, Loss: 1.6851\n",
            "Epoch [81/100], Learning Rate: 0.00028549663646838715\n",
            "Epoch [82/100], Train Accuracy: 36.034 %, Loss: 1.7888\n",
            "Epoch [82/100], Learning Rate: 0.00027419182585616055\n",
            "Epoch [83/100], Train Accuracy: 35.686 %, Loss: 1.6986\n",
            "Epoch [83/100], Learning Rate: 0.0002631592046130896\n",
            "Epoch [84/100], Train Accuracy: 35.766 %, Loss: 1.7848\n",
            "Epoch [84/100], Learning Rate: 0.00025240966060435674\n",
            "Epoch [85/100], Train Accuracy: 36.242 %, Loss: 1.7467\n",
            "Epoch [85/100], Learning Rate: 0.00024195380233209008\n",
            "Epoch [86/100], Train Accuracy: 36.148 %, Loss: 1.7453\n",
            "Epoch [86/100], Learning Rate: 0.00023180194846605365\n",
            "Epoch [87/100], Train Accuracy: 36.276 %, Loss: 1.6676\n",
            "Epoch [87/100], Learning Rate: 0.0002219641176603649\n",
            "Epoch [88/100], Train Accuracy: 35.742 %, Loss: 1.7611\n",
            "Epoch [88/100], Learning Rate: 0.0002124500186662932\n",
            "Epoch [89/100], Train Accuracy: 35.952 %, Loss: 1.7141\n",
            "Epoch [89/100], Learning Rate: 0.00020326904075089488\n",
            "Epoch [90/100], Train Accuracy: 36.254 %, Loss: 1.7002\n",
            "Epoch [90/100], Learning Rate: 0.00019443024443093932\n",
            "Epoch [91/100], Train Accuracy: 35.99 %, Loss: 1.7045\n",
            "Epoch [91/100], Learning Rate: 0.00018594235253127368\n",
            "Epoch [92/100], Train Accuracy: 36.0 %, Loss: 1.7397\n",
            "Epoch [92/100], Learning Rate: 0.00017781374157644722\n",
            "Epoch [93/100], Train Accuracy: 36.302 %, Loss: 1.7951\n",
            "Epoch [93/100], Learning Rate: 0.00017005243352409332\n",
            "Epoch [94/100], Train Accuracy: 35.944 %, Loss: 1.8064\n",
            "Epoch [94/100], Learning Rate: 0.0001626660878482253\n",
            "Epoch [95/100], Train Accuracy: 36.194 %, Loss: 1.7621\n",
            "Epoch [95/100], Learning Rate: 0.0001556619939802614\n",
            "Epoch [96/100], Train Accuracy: 36.074 %, Loss: 1.6694\n",
            "Epoch [96/100], Learning Rate: 0.0001490470641152345\n",
            "Epoch [97/100], Train Accuracy: 36.526 %, Loss: 1.7595\n",
            "Epoch [97/100], Learning Rate: 0.0001428278263902913\n",
            "Epoch [98/100], Train Accuracy: 35.826 %, Loss: 1.7830\n",
            "Epoch [98/100], Learning Rate: 0.00013701041844220858\n",
            "Epoch [99/100], Train Accuracy: 36.054 %, Loss: 1.7649\n",
            "Epoch [99/100], Learning Rate: 0.0001316005813502869\n",
            "Epoch [100/100], Train Accuracy: 36.014 %, Loss: 1.7863\n",
            "Epoch [100/100], Learning Rate: 0.00012660365397059855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習前後のモデルの重みを比較する関数を作成\n",
        "def check_weight_change(model, model_init):\n",
        "    weights_changed = (model.conv1.state_dict()['weight'] != model_init.conv1.state_dict()['weight']).any()\n",
        "    if weights_changed:\n",
        "        return print('モデルの重みが変化しています')\n",
        "    else:\n",
        "        return print('モデルの重みは変化していません')\n",
        "# 学習前後でモデルの重みが変化していないかを確認\n",
        "check_weight_change(model, model_init)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a48db59-0295-45e0-c139-e0424f1815d1",
        "id": "LXeAfuZ0TU2j"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "モデルの重みは変化していません\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの評価\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for images, labels in test_loader_cifar10:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度の確認\n",
        "    print('Test Accuracy: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "925b2dec-0865-4a87-bae3-d7b5512f10c1",
        "id": "13cEFsiKTU2j"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 33.58 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUメモリの解放\n",
        "del model, model_init\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "JpXkWTmHTU2j"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prune_rate=0.995"
      ],
      "metadata": {
        "id": "vn_qAu7YTPEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 刈り込み率を設定\n",
        "prune_rate = 0.995\n",
        "# 初期化手法を設定\n",
        "init = 'signed_constant'\n",
        "\n",
        "# デバイスを設定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# モデルを定義\n",
        "model = supermaskresnet18_10().to(device)\n",
        "# 学習前のモデルの重みを保存\n",
        "model_init = copy.deepcopy(model)\n",
        "\n",
        "# 学習率を設定\n",
        "learning_rate = 0.001\n",
        "# 全体のepoch数を設定\n",
        "num_epochs = 100\n",
        "# warm-upするepoch数を設定\n",
        "warmup_epochs = 10\n",
        "\n",
        "# 損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# オプティマイザーを設定\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# スケジューラーを設定\n",
        "scheduler = CosineLRScheduler(optimizer, t_initial=num_epochs, lr_min=1e-4, warmup_t=warmup_epochs, warmup_lr_init=1e-4, warmup_prefix=True)\n",
        "\n",
        "# モデルの学習\n",
        "for epoch in range(num_epochs):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, (images, labels) in enumerate(train_loader_cifar10):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # 順伝播\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # 逆伝播\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # 全データ数\n",
        "        total += labels.size(0)\n",
        "        # 正解数\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度を計算\n",
        "    train_acc = 100 * correct / total\n",
        "    # 正解率精度と損失を確認\n",
        "    print(\"Epoch [{}/{}], Train Accuracy: {} %, Loss: {:.4f}\".format(epoch+1, num_epochs, 100 * correct / total, loss.item()))\n",
        "    # 1エポック終了後にスケジューラーを更新\n",
        "    scheduler.step(epoch)\n",
        "    # 学習率の確認\n",
        "    print(\"Epoch [{}/{}], Learning Rate: {}\".format(epoch+1, num_epochs, optimizer.param_groups[0]['lr']))\n",
        "\n",
        "# モデルを保存\n",
        "torch.save(model, '/content/drive/MyDrive/SupermaskResNet18_CIFAR10_100epochs_pr0.995_constant_CLRS_restest.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "900d0733-adc6-410f-80a6-d4094611ba2e",
        "id": "cuhNSrhuTPEr"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Accuracy: 27.222 %, Loss: 1.7572\n",
            "Epoch [1/100], Learning Rate: 0.0001\n",
            "Epoch [2/100], Train Accuracy: 35.858 %, Loss: 1.6818\n",
            "Epoch [2/100], Learning Rate: 0.00019\n",
            "Epoch [3/100], Train Accuracy: 36.898 %, Loss: 1.7395\n",
            "Epoch [3/100], Learning Rate: 0.00028\n",
            "Epoch [4/100], Train Accuracy: 38.05 %, Loss: 1.6495\n",
            "Epoch [4/100], Learning Rate: 0.00036999999999999994\n",
            "Epoch [5/100], Train Accuracy: 39.46 %, Loss: 1.6440\n",
            "Epoch [5/100], Learning Rate: 0.00045999999999999996\n",
            "Epoch [6/100], Train Accuracy: 40.362 %, Loss: 1.5036\n",
            "Epoch [6/100], Learning Rate: 0.00055\n",
            "Epoch [7/100], Train Accuracy: 41.408 %, Loss: 1.5748\n",
            "Epoch [7/100], Learning Rate: 0.0006399999999999999\n",
            "Epoch [8/100], Train Accuracy: 42.802 %, Loss: 1.7051\n",
            "Epoch [8/100], Learning Rate: 0.00073\n",
            "Epoch [9/100], Train Accuracy: 43.664 %, Loss: 1.5734\n",
            "Epoch [9/100], Learning Rate: 0.00082\n",
            "Epoch [10/100], Train Accuracy: 44.78 %, Loss: 1.5278\n",
            "Epoch [10/100], Learning Rate: 0.00091\n",
            "Epoch [11/100], Train Accuracy: 44.844 %, Loss: 1.5093\n",
            "Epoch [11/100], Learning Rate: 0.001\n",
            "Epoch [12/100], Train Accuracy: 44.894 %, Loss: 1.5346\n",
            "Epoch [12/100], Learning Rate: 0.0009997779521645793\n",
            "Epoch [13/100], Train Accuracy: 46.598 %, Loss: 1.4885\n",
            "Epoch [13/100], Learning Rate: 0.0009991120277927223\n",
            "Epoch [14/100], Train Accuracy: 47.178 %, Loss: 1.4068\n",
            "Epoch [14/100], Learning Rate: 0.000998002884071386\n",
            "Epoch [15/100], Train Accuracy: 47.466 %, Loss: 1.4611\n",
            "Epoch [15/100], Learning Rate: 0.000996451615591515\n",
            "Epoch [16/100], Train Accuracy: 48.046 %, Loss: 1.5129\n",
            "Epoch [16/100], Learning Rate: 0.000994459753267812\n",
            "Epoch [17/100], Train Accuracy: 48.206 %, Loss: 1.3695\n",
            "Epoch [17/100], Learning Rate: 0.00099202926282791\n",
            "Epoch [18/100], Train Accuracy: 49.292 %, Loss: 1.3211\n",
            "Epoch [18/100], Learning Rate: 0.0009891625428724363\n",
            "Epoch [19/100], Train Accuracy: 49.292 %, Loss: 1.5791\n",
            "Epoch [19/100], Learning Rate: 0.000985862422507884\n",
            "Epoch [20/100], Train Accuracy: 49.11 %, Loss: 1.4078\n",
            "Epoch [20/100], Learning Rate: 0.0009821321585546244\n",
            "Epoch [21/100], Train Accuracy: 50.434 %, Loss: 1.3797\n",
            "Epoch [21/100], Learning Rate: 0.0009779754323328191\n",
            "Epoch [22/100], Train Accuracy: 50.1 %, Loss: 1.3880\n",
            "Epoch [22/100], Learning Rate: 0.0009733963460294015\n",
            "Epoch [23/100], Train Accuracy: 50.14 %, Loss: 1.3853\n",
            "Epoch [23/100], Learning Rate: 0.0009683994186497131\n",
            "Epoch [24/100], Train Accuracy: 50.9 %, Loss: 1.3615\n",
            "Epoch [24/100], Learning Rate: 0.0009629895815577914\n",
            "Epoch [25/100], Train Accuracy: 50.878 %, Loss: 1.4336\n",
            "Epoch [25/100], Learning Rate: 0.0009571721736097088\n",
            "Epoch [26/100], Train Accuracy: 51.034 %, Loss: 1.3246\n",
            "Epoch [26/100], Learning Rate: 0.0009509529358847655\n",
            "Epoch [27/100], Train Accuracy: 51.842 %, Loss: 1.3641\n",
            "Epoch [27/100], Learning Rate: 0.0009443380060197386\n",
            "Epoch [28/100], Train Accuracy: 51.856 %, Loss: 1.3897\n",
            "Epoch [28/100], Learning Rate: 0.0009373339121517746\n",
            "Epoch [29/100], Train Accuracy: 51.798 %, Loss: 1.2913\n",
            "Epoch [29/100], Learning Rate: 0.0009299475664759069\n",
            "Epoch [30/100], Train Accuracy: 51.852 %, Loss: 1.3581\n",
            "Epoch [30/100], Learning Rate: 0.0009221862584235528\n",
            "Epoch [31/100], Train Accuracy: 51.75 %, Loss: 1.2928\n",
            "Epoch [31/100], Learning Rate: 0.0009140576474687263\n",
            "Epoch [32/100], Train Accuracy: 52.494 %, Loss: 1.3428\n",
            "Epoch [32/100], Learning Rate: 0.0009055697555690606\n",
            "Epoch [33/100], Train Accuracy: 52.758 %, Loss: 1.2006\n",
            "Epoch [33/100], Learning Rate: 0.0008967309592491052\n",
            "Epoch [34/100], Train Accuracy: 53.404 %, Loss: 1.2950\n",
            "Epoch [34/100], Learning Rate: 0.0008875499813337069\n",
            "Epoch [35/100], Train Accuracy: 53.212 %, Loss: 1.3011\n",
            "Epoch [35/100], Learning Rate: 0.0008780358823396353\n",
            "Epoch [36/100], Train Accuracy: 53.218 %, Loss: 1.3857\n",
            "Epoch [36/100], Learning Rate: 0.0008681980515339464\n",
            "Epoch [37/100], Train Accuracy: 52.978 %, Loss: 1.3878\n",
            "Epoch [37/100], Learning Rate: 0.0008580461976679099\n",
            "Epoch [38/100], Train Accuracy: 53.728 %, Loss: 1.4049\n",
            "Epoch [38/100], Learning Rate: 0.0008475903393956434\n",
            "Epoch [39/100], Train Accuracy: 53.188 %, Loss: 1.2852\n",
            "Epoch [39/100], Learning Rate: 0.0008368407953869105\n",
            "Epoch [40/100], Train Accuracy: 53.784 %, Loss: 1.3914\n",
            "Epoch [40/100], Learning Rate: 0.0008258081741438395\n",
            "Epoch [41/100], Train Accuracy: 53.542 %, Loss: 1.2128\n",
            "Epoch [41/100], Learning Rate: 0.0008145033635316131\n",
            "Epoch [42/100], Train Accuracy: 53.744 %, Loss: 1.2357\n",
            "Epoch [42/100], Learning Rate: 0.0008029375200334589\n",
            "Epoch [43/100], Train Accuracy: 54.054 %, Loss: 1.3637\n",
            "Epoch [43/100], Learning Rate: 0.0007911220577405485\n",
            "Epoch [44/100], Train Accuracy: 54.164 %, Loss: 1.2440\n",
            "Epoch [44/100], Learning Rate: 0.0007790686370876671\n",
            "Epoch [45/100], Train Accuracy: 54.132 %, Loss: 1.2318\n",
            "Epoch [45/100], Learning Rate: 0.0007667891533457719\n",
            "Epoch [46/100], Train Accuracy: 54.276 %, Loss: 1.2388\n",
            "Epoch [46/100], Learning Rate: 0.000754295724882796\n",
            "Epoch [47/100], Train Accuracy: 54.448 %, Loss: 1.3114\n",
            "Epoch [47/100], Learning Rate: 0.0007416006812042827\n",
            "Epoch [48/100], Train Accuracy: 54.294 %, Loss: 1.1911\n",
            "Epoch [48/100], Learning Rate: 0.0007287165507856513\n",
            "Epoch [49/100], Train Accuracy: 54.962 %, Loss: 1.1691\n",
            "Epoch [49/100], Learning Rate: 0.0007156560487081052\n",
            "Epoch [50/100], Train Accuracy: 54.976 %, Loss: 1.3305\n",
            "Epoch [50/100], Learning Rate: 0.0007024320641103813\n",
            "Epoch [51/100], Train Accuracy: 54.916 %, Loss: 1.2883\n",
            "Epoch [51/100], Learning Rate: 0.0006890576474687264\n",
            "Epoch [52/100], Train Accuracy: 54.93 %, Loss: 1.3080\n",
            "Epoch [52/100], Learning Rate: 0.0006755459977176532\n",
            "Epoch [53/100], Train Accuracy: 55.488 %, Loss: 1.3341\n",
            "Epoch [53/100], Learning Rate: 0.0006619104492241846\n",
            "Epoch [54/100], Train Accuracy: 55.888 %, Loss: 1.1955\n",
            "Epoch [54/100], Learning Rate: 0.0006481644586284443\n",
            "Epoch [55/100], Train Accuracy: 55.192 %, Loss: 1.3069\n",
            "Epoch [55/100], Learning Rate: 0.0006343215915635762\n",
            "Epoch [56/100], Train Accuracy: 55.536 %, Loss: 1.1610\n",
            "Epoch [56/100], Learning Rate: 0.000620395509268104\n",
            "Epoch [57/100], Train Accuracy: 55.668 %, Loss: 1.2608\n",
            "Epoch [57/100], Learning Rate: 0.000606399955103937\n",
            "Epoch [58/100], Train Accuracy: 55.458 %, Loss: 1.1605\n",
            "Epoch [58/100], Learning Rate: 0.0005923487409933315\n",
            "Epoch [59/100], Train Accuracy: 56.146 %, Loss: 1.2759\n",
            "Epoch [59/100], Learning Rate: 0.0005782557337881911\n",
            "Epoch [60/100], Train Accuracy: 56.046 %, Loss: 1.2374\n",
            "Epoch [60/100], Learning Rate: 0.0005641348415851578\n",
            "Epoch [61/100], Train Accuracy: 55.812 %, Loss: 1.3058\n",
            "Epoch [61/100], Learning Rate: 0.00055\n",
            "Epoch [62/100], Train Accuracy: 56.358 %, Loss: 1.2817\n",
            "Epoch [62/100], Learning Rate: 0.0005358651584148423\n",
            "Epoch [63/100], Train Accuracy: 56.33 %, Loss: 1.2664\n",
            "Epoch [63/100], Learning Rate: 0.000521744266211809\n",
            "Epoch [64/100], Train Accuracy: 56.096 %, Loss: 1.1981\n",
            "Epoch [64/100], Learning Rate: 0.0005076512590066686\n",
            "Epoch [65/100], Train Accuracy: 56.606 %, Loss: 1.3009\n",
            "Epoch [65/100], Learning Rate: 0.0004936000448960632\n",
            "Epoch [66/100], Train Accuracy: 56.086 %, Loss: 1.2713\n",
            "Epoch [66/100], Learning Rate: 0.0004796044907318961\n",
            "Epoch [67/100], Train Accuracy: 56.004 %, Loss: 1.1821\n",
            "Epoch [67/100], Learning Rate: 0.0004656784084364239\n",
            "Epoch [68/100], Train Accuracy: 56.912 %, Loss: 1.2141\n",
            "Epoch [68/100], Learning Rate: 0.00045183554137155597\n",
            "Epoch [69/100], Train Accuracy: 56.588 %, Loss: 1.1969\n",
            "Epoch [69/100], Learning Rate: 0.0004380895507758153\n",
            "Epoch [70/100], Train Accuracy: 56.958 %, Loss: 1.1256\n",
            "Epoch [70/100], Learning Rate: 0.00042445400228234684\n",
            "Epoch [71/100], Train Accuracy: 56.746 %, Loss: 1.1395\n",
            "Epoch [71/100], Learning Rate: 0.0004109423525312738\n",
            "Epoch [72/100], Train Accuracy: 57.112 %, Loss: 1.2389\n",
            "Epoch [72/100], Learning Rate: 0.0003975679358896189\n",
            "Epoch [73/100], Train Accuracy: 57.154 %, Loss: 1.2026\n",
            "Epoch [73/100], Learning Rate: 0.000384343951291895\n",
            "Epoch [74/100], Train Accuracy: 57.474 %, Loss: 1.2501\n",
            "Epoch [74/100], Learning Rate: 0.00037128344921434864\n",
            "Epoch [75/100], Train Accuracy: 57.248 %, Loss: 1.2212\n",
            "Epoch [75/100], Learning Rate: 0.0003583993187957173\n",
            "Epoch [76/100], Train Accuracy: 57.206 %, Loss: 1.2022\n",
            "Epoch [76/100], Learning Rate: 0.00034570427511720393\n",
            "Epoch [77/100], Train Accuracy: 57.846 %, Loss: 1.1805\n",
            "Epoch [77/100], Learning Rate: 0.0003332108466542281\n",
            "Epoch [78/100], Train Accuracy: 57.586 %, Loss: 1.2258\n",
            "Epoch [78/100], Learning Rate: 0.0003209313629123329\n",
            "Epoch [79/100], Train Accuracy: 58.08 %, Loss: 1.1020\n",
            "Epoch [79/100], Learning Rate: 0.0003088779422594514\n",
            "Epoch [80/100], Train Accuracy: 58.33 %, Loss: 1.0870\n",
            "Epoch [80/100], Learning Rate: 0.0002970624799665412\n",
            "Epoch [81/100], Train Accuracy: 58.032 %, Loss: 1.1911\n",
            "Epoch [81/100], Learning Rate: 0.00028549663646838715\n",
            "Epoch [82/100], Train Accuracy: 57.504 %, Loss: 1.0853\n",
            "Epoch [82/100], Learning Rate: 0.00027419182585616055\n",
            "Epoch [83/100], Train Accuracy: 57.852 %, Loss: 1.2181\n",
            "Epoch [83/100], Learning Rate: 0.0002631592046130896\n",
            "Epoch [84/100], Train Accuracy: 57.96 %, Loss: 1.1814\n",
            "Epoch [84/100], Learning Rate: 0.00025240966060435674\n",
            "Epoch [85/100], Train Accuracy: 57.926 %, Loss: 1.1499\n",
            "Epoch [85/100], Learning Rate: 0.00024195380233209008\n",
            "Epoch [86/100], Train Accuracy: 57.994 %, Loss: 1.1632\n",
            "Epoch [86/100], Learning Rate: 0.00023180194846605365\n",
            "Epoch [87/100], Train Accuracy: 58.246 %, Loss: 1.1721\n",
            "Epoch [87/100], Learning Rate: 0.0002219641176603649\n",
            "Epoch [88/100], Train Accuracy: 58.582 %, Loss: 1.1606\n",
            "Epoch [88/100], Learning Rate: 0.0002124500186662932\n",
            "Epoch [89/100], Train Accuracy: 58.312 %, Loss: 1.1085\n",
            "Epoch [89/100], Learning Rate: 0.00020326904075089488\n",
            "Epoch [90/100], Train Accuracy: 58.264 %, Loss: 1.1785\n",
            "Epoch [90/100], Learning Rate: 0.00019443024443093932\n",
            "Epoch [91/100], Train Accuracy: 58.57 %, Loss: 1.2230\n",
            "Epoch [91/100], Learning Rate: 0.00018594235253127368\n",
            "Epoch [92/100], Train Accuracy: 58.548 %, Loss: 1.0982\n",
            "Epoch [92/100], Learning Rate: 0.00017781374157644722\n",
            "Epoch [93/100], Train Accuracy: 58.836 %, Loss: 1.2143\n",
            "Epoch [93/100], Learning Rate: 0.00017005243352409332\n",
            "Epoch [94/100], Train Accuracy: 58.828 %, Loss: 1.1048\n",
            "Epoch [94/100], Learning Rate: 0.0001626660878482253\n",
            "Epoch [95/100], Train Accuracy: 59.054 %, Loss: 1.1496\n",
            "Epoch [95/100], Learning Rate: 0.0001556619939802614\n",
            "Epoch [96/100], Train Accuracy: 58.782 %, Loss: 1.0308\n",
            "Epoch [96/100], Learning Rate: 0.0001490470641152345\n",
            "Epoch [97/100], Train Accuracy: 58.902 %, Loss: 1.1497\n",
            "Epoch [97/100], Learning Rate: 0.0001428278263902913\n",
            "Epoch [98/100], Train Accuracy: 58.67 %, Loss: 1.1473\n",
            "Epoch [98/100], Learning Rate: 0.00013701041844220858\n",
            "Epoch [99/100], Train Accuracy: 59.16 %, Loss: 1.0807\n",
            "Epoch [99/100], Learning Rate: 0.0001316005813502869\n",
            "Epoch [100/100], Train Accuracy: 59.234 %, Loss: 1.0908\n",
            "Epoch [100/100], Learning Rate: 0.00012660365397059855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習前後のモデルの重みを比較する関数を作成\n",
        "def check_weight_change(model, model_init):\n",
        "    weights_changed = (model.conv1.state_dict()['weight'] != model_init.conv1.state_dict()['weight']).any()\n",
        "    if weights_changed:\n",
        "        return print('モデルの重みが変化しています')\n",
        "    else:\n",
        "        return print('モデルの重みは変化していません')\n",
        "# 学習前後でモデルの重みが変化していないかを確認\n",
        "check_weight_change(model, model_init)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27312ff3-99ab-4506-c943-07d7578ed380",
        "id": "V88TIdEzTPEr"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "モデルの重みは変化していません\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの評価\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for images, labels in test_loader_cifar10:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度の確認\n",
        "    print('Test Accuracy: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b73b3a6-f03d-4bd5-9995-58069e406510",
        "id": "ntkc31cBTPEs"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 48.88 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUメモリの解放\n",
        "del model, model_init\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "KXw1CZBdTPEs"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prune_rate=0.99"
      ],
      "metadata": {
        "id": "0MDbC4VvlsK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 刈り込み率を設定\n",
        "prune_rate = 0.99\n",
        "# 初期化手法を設定\n",
        "init = 'signed_constant'\n",
        "\n",
        "# デバイスを設定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# モデルを定義\n",
        "model = supermaskresnet18_10().to(device)\n",
        "# 学習前のモデルの重みを保存\n",
        "model_init = copy.deepcopy(model)\n",
        "\n",
        "# 学習率を設定\n",
        "learning_rate = 0.001\n",
        "# 全体のepoch数を設定\n",
        "num_epochs = 100\n",
        "# warm-upするepoch数を設定\n",
        "warmup_epochs = 10\n",
        "\n",
        "# 損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# オプティマイザーを設定\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# スケジューラーを設定\n",
        "scheduler = CosineLRScheduler(optimizer, t_initial=num_epochs, lr_min=1e-4, warmup_t=warmup_epochs, warmup_lr_init=1e-4, warmup_prefix=True)\n",
        "\n",
        "# モデルの学習\n",
        "for epoch in range(num_epochs):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, (images, labels) in enumerate(train_loader_cifar10):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # 順伝播\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # 逆伝播\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # 全データ数\n",
        "        total += labels.size(0)\n",
        "        # 正解数\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度を計算\n",
        "    train_acc = 100 * correct / total\n",
        "    # 正解率精度と損失を確認\n",
        "    print(\"Epoch [{}/{}], Train Accuracy: {} %, Loss: {:.4f}\".format(epoch+1, num_epochs, 100 * correct / total, loss.item()))\n",
        "    # 1エポック終了後にスケジューラーを更新\n",
        "    scheduler.step(epoch)\n",
        "    # 学習率の確認\n",
        "    print(\"Epoch [{}/{}], Learning Rate: {}\".format(epoch+1, num_epochs, optimizer.param_groups[0]['lr']))\n",
        "\n",
        "# モデルを保存\n",
        "torch.save(model, '/content/drive/MyDrive/SupermaskResNet18_CIFAR10_100epochs_pr0.99_constant_CLRS_restest.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqwOMv0ulsK-",
        "outputId": "09d2d1b1-6947-4383-8298-bf90a6e863e2"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Accuracy: 30.882 %, Loss: 1.7386\n",
            "Epoch [1/100], Learning Rate: 0.0001\n",
            "Epoch [2/100], Train Accuracy: 39.726 %, Loss: 1.6720\n",
            "Epoch [2/100], Learning Rate: 0.00019\n",
            "Epoch [3/100], Train Accuracy: 41.252 %, Loss: 1.5588\n",
            "Epoch [3/100], Learning Rate: 0.00028\n",
            "Epoch [4/100], Train Accuracy: 42.758 %, Loss: 1.5144\n",
            "Epoch [4/100], Learning Rate: 0.00036999999999999994\n",
            "Epoch [5/100], Train Accuracy: 44.16 %, Loss: 1.5480\n",
            "Epoch [5/100], Learning Rate: 0.00045999999999999996\n",
            "Epoch [6/100], Train Accuracy: 45.784 %, Loss: 1.5231\n",
            "Epoch [6/100], Learning Rate: 0.00055\n",
            "Epoch [7/100], Train Accuracy: 46.48 %, Loss: 1.5146\n",
            "Epoch [7/100], Learning Rate: 0.0006399999999999999\n",
            "Epoch [8/100], Train Accuracy: 46.936 %, Loss: 1.4405\n",
            "Epoch [8/100], Learning Rate: 0.00073\n",
            "Epoch [9/100], Train Accuracy: 48.76 %, Loss: 1.2892\n",
            "Epoch [9/100], Learning Rate: 0.00082\n",
            "Epoch [10/100], Train Accuracy: 49.366 %, Loss: 1.3412\n",
            "Epoch [10/100], Learning Rate: 0.00091\n",
            "Epoch [11/100], Train Accuracy: 50.538 %, Loss: 1.3810\n",
            "Epoch [11/100], Learning Rate: 0.001\n",
            "Epoch [12/100], Train Accuracy: 50.248 %, Loss: 1.4631\n",
            "Epoch [12/100], Learning Rate: 0.0009997779521645793\n",
            "Epoch [13/100], Train Accuracy: 51.61 %, Loss: 1.2811\n",
            "Epoch [13/100], Learning Rate: 0.0009991120277927223\n",
            "Epoch [14/100], Train Accuracy: 52.548 %, Loss: 1.2330\n",
            "Epoch [14/100], Learning Rate: 0.000998002884071386\n",
            "Epoch [15/100], Train Accuracy: 52.602 %, Loss: 1.2303\n",
            "Epoch [15/100], Learning Rate: 0.000996451615591515\n",
            "Epoch [16/100], Train Accuracy: 53.258 %, Loss: 1.2324\n",
            "Epoch [16/100], Learning Rate: 0.000994459753267812\n",
            "Epoch [17/100], Train Accuracy: 54.04 %, Loss: 1.1769\n",
            "Epoch [17/100], Learning Rate: 0.00099202926282791\n",
            "Epoch [18/100], Train Accuracy: 54.368 %, Loss: 1.2327\n",
            "Epoch [18/100], Learning Rate: 0.0009891625428724363\n",
            "Epoch [19/100], Train Accuracy: 55.102 %, Loss: 1.2260\n",
            "Epoch [19/100], Learning Rate: 0.000985862422507884\n",
            "Epoch [20/100], Train Accuracy: 55.422 %, Loss: 1.3043\n",
            "Epoch [20/100], Learning Rate: 0.0009821321585546244\n",
            "Epoch [21/100], Train Accuracy: 56.33 %, Loss: 1.2118\n",
            "Epoch [21/100], Learning Rate: 0.0009779754323328191\n",
            "Epoch [22/100], Train Accuracy: 57.206 %, Loss: 1.2849\n",
            "Epoch [22/100], Learning Rate: 0.0009733963460294015\n",
            "Epoch [23/100], Train Accuracy: 57.408 %, Loss: 1.2485\n",
            "Epoch [23/100], Learning Rate: 0.0009683994186497131\n",
            "Epoch [24/100], Train Accuracy: 57.278 %, Loss: 1.1093\n",
            "Epoch [24/100], Learning Rate: 0.0009629895815577914\n",
            "Epoch [25/100], Train Accuracy: 58.088 %, Loss: 1.0977\n",
            "Epoch [25/100], Learning Rate: 0.0009571721736097088\n",
            "Epoch [26/100], Train Accuracy: 58.154 %, Loss: 1.2020\n",
            "Epoch [26/100], Learning Rate: 0.0009509529358847655\n",
            "Epoch [27/100], Train Accuracy: 58.182 %, Loss: 1.2219\n",
            "Epoch [27/100], Learning Rate: 0.0009443380060197386\n",
            "Epoch [28/100], Train Accuracy: 58.636 %, Loss: 1.1088\n",
            "Epoch [28/100], Learning Rate: 0.0009373339121517746\n",
            "Epoch [29/100], Train Accuracy: 58.952 %, Loss: 1.1388\n",
            "Epoch [29/100], Learning Rate: 0.0009299475664759069\n",
            "Epoch [30/100], Train Accuracy: 58.712 %, Loss: 1.0515\n",
            "Epoch [30/100], Learning Rate: 0.0009221862584235528\n",
            "Epoch [31/100], Train Accuracy: 59.266 %, Loss: 1.0673\n",
            "Epoch [31/100], Learning Rate: 0.0009140576474687263\n",
            "Epoch [32/100], Train Accuracy: 59.5 %, Loss: 1.1746\n",
            "Epoch [32/100], Learning Rate: 0.0009055697555690606\n",
            "Epoch [33/100], Train Accuracy: 60.04 %, Loss: 1.1210\n",
            "Epoch [33/100], Learning Rate: 0.0008967309592491052\n",
            "Epoch [34/100], Train Accuracy: 60.316 %, Loss: 1.0683\n",
            "Epoch [34/100], Learning Rate: 0.0008875499813337069\n",
            "Epoch [35/100], Train Accuracy: 59.796 %, Loss: 1.0489\n",
            "Epoch [35/100], Learning Rate: 0.0008780358823396353\n",
            "Epoch [36/100], Train Accuracy: 60.352 %, Loss: 1.1344\n",
            "Epoch [36/100], Learning Rate: 0.0008681980515339464\n",
            "Epoch [37/100], Train Accuracy: 61.062 %, Loss: 1.1257\n",
            "Epoch [37/100], Learning Rate: 0.0008580461976679099\n",
            "Epoch [38/100], Train Accuracy: 60.312 %, Loss: 1.1022\n",
            "Epoch [38/100], Learning Rate: 0.0008475903393956434\n",
            "Epoch [39/100], Train Accuracy: 61.25 %, Loss: 1.1553\n",
            "Epoch [39/100], Learning Rate: 0.0008368407953869105\n",
            "Epoch [40/100], Train Accuracy: 61.198 %, Loss: 0.9402\n",
            "Epoch [40/100], Learning Rate: 0.0008258081741438395\n",
            "Epoch [41/100], Train Accuracy: 61.21 %, Loss: 1.0650\n",
            "Epoch [41/100], Learning Rate: 0.0008145033635316131\n",
            "Epoch [42/100], Train Accuracy: 61.54 %, Loss: 1.0989\n",
            "Epoch [42/100], Learning Rate: 0.0008029375200334589\n",
            "Epoch [43/100], Train Accuracy: 61.538 %, Loss: 1.2183\n",
            "Epoch [43/100], Learning Rate: 0.0007911220577405485\n",
            "Epoch [44/100], Train Accuracy: 61.626 %, Loss: 1.1816\n",
            "Epoch [44/100], Learning Rate: 0.0007790686370876671\n",
            "Epoch [45/100], Train Accuracy: 62.032 %, Loss: 1.0206\n",
            "Epoch [45/100], Learning Rate: 0.0007667891533457719\n",
            "Epoch [46/100], Train Accuracy: 61.754 %, Loss: 1.1172\n",
            "Epoch [46/100], Learning Rate: 0.000754295724882796\n",
            "Epoch [47/100], Train Accuracy: 62.132 %, Loss: 1.0743\n",
            "Epoch [47/100], Learning Rate: 0.0007416006812042827\n",
            "Epoch [48/100], Train Accuracy: 61.54 %, Loss: 1.0781\n",
            "Epoch [48/100], Learning Rate: 0.0007287165507856513\n",
            "Epoch [49/100], Train Accuracy: 62.334 %, Loss: 1.0135\n",
            "Epoch [49/100], Learning Rate: 0.0007156560487081052\n",
            "Epoch [50/100], Train Accuracy: 62.296 %, Loss: 1.0313\n",
            "Epoch [50/100], Learning Rate: 0.0007024320641103813\n",
            "Epoch [51/100], Train Accuracy: 62.684 %, Loss: 1.1140\n",
            "Epoch [51/100], Learning Rate: 0.0006890576474687264\n",
            "Epoch [52/100], Train Accuracy: 62.894 %, Loss: 1.0172\n",
            "Epoch [52/100], Learning Rate: 0.0006755459977176532\n",
            "Epoch [53/100], Train Accuracy: 63.154 %, Loss: 1.1133\n",
            "Epoch [53/100], Learning Rate: 0.0006619104492241846\n",
            "Epoch [54/100], Train Accuracy: 62.652 %, Loss: 1.1113\n",
            "Epoch [54/100], Learning Rate: 0.0006481644586284443\n",
            "Epoch [55/100], Train Accuracy: 62.616 %, Loss: 1.1807\n",
            "Epoch [55/100], Learning Rate: 0.0006343215915635762\n",
            "Epoch [56/100], Train Accuracy: 62.598 %, Loss: 1.0147\n",
            "Epoch [56/100], Learning Rate: 0.000620395509268104\n",
            "Epoch [57/100], Train Accuracy: 63.284 %, Loss: 0.9855\n",
            "Epoch [57/100], Learning Rate: 0.000606399955103937\n",
            "Epoch [58/100], Train Accuracy: 63.412 %, Loss: 0.9580\n",
            "Epoch [58/100], Learning Rate: 0.0005923487409933315\n",
            "Epoch [59/100], Train Accuracy: 63.804 %, Loss: 0.9405\n",
            "Epoch [59/100], Learning Rate: 0.0005782557337881911\n",
            "Epoch [60/100], Train Accuracy: 63.556 %, Loss: 1.0804\n",
            "Epoch [60/100], Learning Rate: 0.0005641348415851578\n",
            "Epoch [61/100], Train Accuracy: 63.794 %, Loss: 0.9846\n",
            "Epoch [61/100], Learning Rate: 0.00055\n",
            "Epoch [62/100], Train Accuracy: 63.9 %, Loss: 1.0930\n",
            "Epoch [62/100], Learning Rate: 0.0005358651584148423\n",
            "Epoch [63/100], Train Accuracy: 64.06 %, Loss: 1.1177\n",
            "Epoch [63/100], Learning Rate: 0.000521744266211809\n",
            "Epoch [64/100], Train Accuracy: 64.488 %, Loss: 0.9453\n",
            "Epoch [64/100], Learning Rate: 0.0005076512590066686\n",
            "Epoch [65/100], Train Accuracy: 64.266 %, Loss: 1.0501\n",
            "Epoch [65/100], Learning Rate: 0.0004936000448960632\n",
            "Epoch [66/100], Train Accuracy: 64.778 %, Loss: 1.0542\n",
            "Epoch [66/100], Learning Rate: 0.0004796044907318961\n",
            "Epoch [67/100], Train Accuracy: 64.908 %, Loss: 1.0191\n",
            "Epoch [67/100], Learning Rate: 0.0004656784084364239\n",
            "Epoch [68/100], Train Accuracy: 64.666 %, Loss: 1.0317\n",
            "Epoch [68/100], Learning Rate: 0.00045183554137155597\n",
            "Epoch [69/100], Train Accuracy: 64.8 %, Loss: 0.9109\n",
            "Epoch [69/100], Learning Rate: 0.0004380895507758153\n",
            "Epoch [70/100], Train Accuracy: 65.01 %, Loss: 0.9194\n",
            "Epoch [70/100], Learning Rate: 0.00042445400228234684\n",
            "Epoch [71/100], Train Accuracy: 65.188 %, Loss: 0.9439\n",
            "Epoch [71/100], Learning Rate: 0.0004109423525312738\n",
            "Epoch [72/100], Train Accuracy: 64.788 %, Loss: 1.0574\n",
            "Epoch [72/100], Learning Rate: 0.0003975679358896189\n",
            "Epoch [73/100], Train Accuracy: 65.018 %, Loss: 0.9564\n",
            "Epoch [73/100], Learning Rate: 0.000384343951291895\n",
            "Epoch [74/100], Train Accuracy: 65.588 %, Loss: 0.9700\n",
            "Epoch [74/100], Learning Rate: 0.00037128344921434864\n",
            "Epoch [75/100], Train Accuracy: 65.58 %, Loss: 0.9414\n",
            "Epoch [75/100], Learning Rate: 0.0003583993187957173\n",
            "Epoch [76/100], Train Accuracy: 65.742 %, Loss: 0.9502\n",
            "Epoch [76/100], Learning Rate: 0.00034570427511720393\n",
            "Epoch [77/100], Train Accuracy: 65.688 %, Loss: 0.9415\n",
            "Epoch [77/100], Learning Rate: 0.0003332108466542281\n",
            "Epoch [78/100], Train Accuracy: 65.888 %, Loss: 1.0118\n",
            "Epoch [78/100], Learning Rate: 0.0003209313629123329\n",
            "Epoch [79/100], Train Accuracy: 65.718 %, Loss: 0.9539\n",
            "Epoch [79/100], Learning Rate: 0.0003088779422594514\n",
            "Epoch [80/100], Train Accuracy: 66.712 %, Loss: 1.0014\n",
            "Epoch [80/100], Learning Rate: 0.0002970624799665412\n",
            "Epoch [81/100], Train Accuracy: 66.004 %, Loss: 1.0034\n",
            "Epoch [81/100], Learning Rate: 0.00028549663646838715\n",
            "Epoch [82/100], Train Accuracy: 66.568 %, Loss: 0.9641\n",
            "Epoch [82/100], Learning Rate: 0.00027419182585616055\n",
            "Epoch [83/100], Train Accuracy: 66.172 %, Loss: 1.0871\n",
            "Epoch [83/100], Learning Rate: 0.0002631592046130896\n",
            "Epoch [84/100], Train Accuracy: 66.468 %, Loss: 1.0139\n",
            "Epoch [84/100], Learning Rate: 0.00025240966060435674\n",
            "Epoch [85/100], Train Accuracy: 66.376 %, Loss: 1.0552\n",
            "Epoch [85/100], Learning Rate: 0.00024195380233209008\n",
            "Epoch [86/100], Train Accuracy: 66.52 %, Loss: 0.9696\n",
            "Epoch [86/100], Learning Rate: 0.00023180194846605365\n",
            "Epoch [87/100], Train Accuracy: 66.77 %, Loss: 0.9157\n",
            "Epoch [87/100], Learning Rate: 0.0002219641176603649\n",
            "Epoch [88/100], Train Accuracy: 66.89 %, Loss: 1.0594\n",
            "Epoch [88/100], Learning Rate: 0.0002124500186662932\n",
            "Epoch [89/100], Train Accuracy: 66.866 %, Loss: 0.9221\n",
            "Epoch [89/100], Learning Rate: 0.00020326904075089488\n",
            "Epoch [90/100], Train Accuracy: 67.344 %, Loss: 0.8830\n",
            "Epoch [90/100], Learning Rate: 0.00019443024443093932\n",
            "Epoch [91/100], Train Accuracy: 67.392 %, Loss: 0.9398\n",
            "Epoch [91/100], Learning Rate: 0.00018594235253127368\n",
            "Epoch [92/100], Train Accuracy: 67.058 %, Loss: 0.8720\n",
            "Epoch [92/100], Learning Rate: 0.00017781374157644722\n",
            "Epoch [93/100], Train Accuracy: 67.128 %, Loss: 0.9764\n",
            "Epoch [93/100], Learning Rate: 0.00017005243352409332\n",
            "Epoch [94/100], Train Accuracy: 67.218 %, Loss: 0.8823\n",
            "Epoch [94/100], Learning Rate: 0.0001626660878482253\n",
            "Epoch [95/100], Train Accuracy: 67.74 %, Loss: 0.8772\n",
            "Epoch [95/100], Learning Rate: 0.0001556619939802614\n",
            "Epoch [96/100], Train Accuracy: 67.792 %, Loss: 0.9435\n",
            "Epoch [96/100], Learning Rate: 0.0001490470641152345\n",
            "Epoch [97/100], Train Accuracy: 67.828 %, Loss: 0.9771\n",
            "Epoch [97/100], Learning Rate: 0.0001428278263902913\n",
            "Epoch [98/100], Train Accuracy: 67.386 %, Loss: 0.9173\n",
            "Epoch [98/100], Learning Rate: 0.00013701041844220858\n",
            "Epoch [99/100], Train Accuracy: 67.742 %, Loss: 0.8607\n",
            "Epoch [99/100], Learning Rate: 0.0001316005813502869\n",
            "Epoch [100/100], Train Accuracy: 67.44 %, Loss: 0.8297\n",
            "Epoch [100/100], Learning Rate: 0.00012660365397059855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習前後のモデルの重みを比較する関数を作成\n",
        "def check_weight_change(model, model_init):\n",
        "    weights_changed = (model.conv1.state_dict()['weight'] != model_init.conv1.state_dict()['weight']).any()\n",
        "    if weights_changed:\n",
        "        return print('モデルの重みが変化しています')\n",
        "    else:\n",
        "        return print('モデルの重みは変化していません')\n",
        "# 学習前後でモデルの重みが変化していないかを確認\n",
        "check_weight_change(model, model_init)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cm-kCSnflsK_",
        "outputId": "e10bed5b-f27b-4bf1-a3f7-0298c162387c"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "モデルの重みは変化していません\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの評価\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for images, labels in test_loader_cifar10:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度の確認\n",
        "    print('Test Accuracy: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zD7LvPEQlsK_",
        "outputId": "45972029-f1c3-426b-cc92-1185ea3fa7b9"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 59.44 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUメモリの解放\n",
        "del model, model_init\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "r7lXFGOblsK_"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prune_rate=0.95"
      ],
      "metadata": {
        "id": "ByOgR3LElNMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 刈り込み率を設定\n",
        "prune_rate = 0.95\n",
        "# 初期化手法を設定\n",
        "init = 'signed_constant'\n",
        "\n",
        "# デバイスを設定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# モデルを定義\n",
        "model = supermaskresnet18_10().to(device)\n",
        "# 学習前のモデルの重みを保存\n",
        "model_init = copy.deepcopy(model)\n",
        "\n",
        "# 学習率を設定\n",
        "learning_rate = 0.001\n",
        "# 全体のepoch数を設定\n",
        "num_epochs = 100\n",
        "# warm-upするepoch数を設定\n",
        "warmup_epochs = 10\n",
        "\n",
        "# 損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# オプティマイザーを設定\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# スケジューラーを設定\n",
        "scheduler = CosineLRScheduler(optimizer, t_initial=num_epochs, lr_min=1e-4, warmup_t=warmup_epochs, warmup_lr_init=1e-4, warmup_prefix=True)\n",
        "\n",
        "# モデルの学習\n",
        "for epoch in range(num_epochs):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, (images, labels) in enumerate(train_loader_cifar10):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # 順伝播\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # 逆伝播\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # 全データ数\n",
        "        total += labels.size(0)\n",
        "        # 正解数\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度を計算\n",
        "    train_acc = 100 * correct / total\n",
        "    # 正解率精度と損失を確認\n",
        "    print(\"Epoch [{}/{}], Train Accuracy: {} %, Loss: {:.4f}\".format(epoch+1, num_epochs, 100 * correct / total, loss.item()))\n",
        "    # 1エポック終了後にスケジューラーを更新\n",
        "    scheduler.step(epoch)\n",
        "    # 学習率の確認\n",
        "    print(\"Epoch [{}/{}], Learning Rate: {}\".format(epoch+1, num_epochs, optimizer.param_groups[0]['lr']))\n",
        "\n",
        "# モデルを保存\n",
        "torch.save(model, '/content/drive/MyDrive/SupermaskResNet18_CIFAR10_100epochs_pr0.95_constant_CLRS_restest.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9eacf67-643d-47d3-ac98-7ace2b70df93",
        "id": "F-Pa0liKlNMo"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Accuracy: 35.8 %, Loss: 1.6018\n",
            "Epoch [1/100], Learning Rate: 0.0001\n",
            "Epoch [2/100], Train Accuracy: 46.408 %, Loss: 1.5536\n",
            "Epoch [2/100], Learning Rate: 0.00019\n",
            "Epoch [3/100], Train Accuracy: 48.71 %, Loss: 1.4920\n",
            "Epoch [3/100], Learning Rate: 0.00028\n",
            "Epoch [4/100], Train Accuracy: 51.12 %, Loss: 1.3968\n",
            "Epoch [4/100], Learning Rate: 0.00036999999999999994\n",
            "Epoch [5/100], Train Accuracy: 53.624 %, Loss: 1.1691\n",
            "Epoch [5/100], Learning Rate: 0.00045999999999999996\n",
            "Epoch [6/100], Train Accuracy: 55.16 %, Loss: 1.3167\n",
            "Epoch [6/100], Learning Rate: 0.00055\n",
            "Epoch [7/100], Train Accuracy: 56.726 %, Loss: 1.1943\n",
            "Epoch [7/100], Learning Rate: 0.0006399999999999999\n",
            "Epoch [8/100], Train Accuracy: 58.254 %, Loss: 1.2152\n",
            "Epoch [8/100], Learning Rate: 0.00073\n",
            "Epoch [9/100], Train Accuracy: 59.6 %, Loss: 1.1889\n",
            "Epoch [9/100], Learning Rate: 0.00082\n",
            "Epoch [10/100], Train Accuracy: 61.032 %, Loss: 0.9389\n",
            "Epoch [10/100], Learning Rate: 0.00091\n",
            "Epoch [11/100], Train Accuracy: 62.224 %, Loss: 1.0442\n",
            "Epoch [11/100], Learning Rate: 0.001\n",
            "Epoch [12/100], Train Accuracy: 62.816 %, Loss: 1.2166\n",
            "Epoch [12/100], Learning Rate: 0.0009997779521645793\n",
            "Epoch [13/100], Train Accuracy: 63.728 %, Loss: 0.9454\n",
            "Epoch [13/100], Learning Rate: 0.0009991120277927223\n",
            "Epoch [14/100], Train Accuracy: 65.322 %, Loss: 0.8556\n",
            "Epoch [14/100], Learning Rate: 0.000998002884071386\n",
            "Epoch [15/100], Train Accuracy: 65.992 %, Loss: 1.0005\n",
            "Epoch [15/100], Learning Rate: 0.000996451615591515\n",
            "Epoch [16/100], Train Accuracy: 66.562 %, Loss: 0.9536\n",
            "Epoch [16/100], Learning Rate: 0.000994459753267812\n",
            "Epoch [17/100], Train Accuracy: 67.226 %, Loss: 0.9513\n",
            "Epoch [17/100], Learning Rate: 0.00099202926282791\n",
            "Epoch [18/100], Train Accuracy: 67.712 %, Loss: 0.9740\n",
            "Epoch [18/100], Learning Rate: 0.0009891625428724363\n",
            "Epoch [19/100], Train Accuracy: 68.496 %, Loss: 0.9353\n",
            "Epoch [19/100], Learning Rate: 0.000985862422507884\n",
            "Epoch [20/100], Train Accuracy: 68.582 %, Loss: 0.8996\n",
            "Epoch [20/100], Learning Rate: 0.0009821321585546244\n",
            "Epoch [21/100], Train Accuracy: 69.36 %, Loss: 0.9000\n",
            "Epoch [21/100], Learning Rate: 0.0009779754323328191\n",
            "Epoch [22/100], Train Accuracy: 69.974 %, Loss: 0.8050\n",
            "Epoch [22/100], Learning Rate: 0.0009733963460294015\n",
            "Epoch [23/100], Train Accuracy: 70.616 %, Loss: 0.8473\n",
            "Epoch [23/100], Learning Rate: 0.0009683994186497131\n",
            "Epoch [24/100], Train Accuracy: 70.632 %, Loss: 0.8311\n",
            "Epoch [24/100], Learning Rate: 0.0009629895815577914\n",
            "Epoch [25/100], Train Accuracy: 71.148 %, Loss: 0.7220\n",
            "Epoch [25/100], Learning Rate: 0.0009571721736097088\n",
            "Epoch [26/100], Train Accuracy: 71.64 %, Loss: 0.8455\n",
            "Epoch [26/100], Learning Rate: 0.0009509529358847655\n",
            "Epoch [27/100], Train Accuracy: 71.892 %, Loss: 0.7600\n",
            "Epoch [27/100], Learning Rate: 0.0009443380060197386\n",
            "Epoch [28/100], Train Accuracy: 72.132 %, Loss: 0.9049\n",
            "Epoch [28/100], Learning Rate: 0.0009373339121517746\n",
            "Epoch [29/100], Train Accuracy: 72.38 %, Loss: 0.8589\n",
            "Epoch [29/100], Learning Rate: 0.0009299475664759069\n",
            "Epoch [30/100], Train Accuracy: 72.818 %, Loss: 0.7301\n",
            "Epoch [30/100], Learning Rate: 0.0009221862584235528\n",
            "Epoch [31/100], Train Accuracy: 72.632 %, Loss: 0.8048\n",
            "Epoch [31/100], Learning Rate: 0.0009140576474687263\n",
            "Epoch [32/100], Train Accuracy: 73.036 %, Loss: 0.8220\n",
            "Epoch [32/100], Learning Rate: 0.0009055697555690606\n",
            "Epoch [33/100], Train Accuracy: 73.48 %, Loss: 0.7223\n",
            "Epoch [33/100], Learning Rate: 0.0008967309592491052\n",
            "Epoch [34/100], Train Accuracy: 73.686 %, Loss: 0.7855\n",
            "Epoch [34/100], Learning Rate: 0.0008875499813337069\n",
            "Epoch [35/100], Train Accuracy: 73.956 %, Loss: 0.6957\n",
            "Epoch [35/100], Learning Rate: 0.0008780358823396353\n",
            "Epoch [36/100], Train Accuracy: 74.126 %, Loss: 0.6738\n",
            "Epoch [36/100], Learning Rate: 0.0008681980515339464\n",
            "Epoch [37/100], Train Accuracy: 74.314 %, Loss: 0.5784\n",
            "Epoch [37/100], Learning Rate: 0.0008580461976679099\n",
            "Epoch [38/100], Train Accuracy: 74.76 %, Loss: 0.8695\n",
            "Epoch [38/100], Learning Rate: 0.0008475903393956434\n",
            "Epoch [39/100], Train Accuracy: 75.156 %, Loss: 0.6777\n",
            "Epoch [39/100], Learning Rate: 0.0008368407953869105\n",
            "Epoch [40/100], Train Accuracy: 75.032 %, Loss: 0.5894\n",
            "Epoch [40/100], Learning Rate: 0.0008258081741438395\n",
            "Epoch [41/100], Train Accuracy: 75.5 %, Loss: 0.7365\n",
            "Epoch [41/100], Learning Rate: 0.0008145033635316131\n",
            "Epoch [42/100], Train Accuracy: 75.438 %, Loss: 0.7380\n",
            "Epoch [42/100], Learning Rate: 0.0008029375200334589\n",
            "Epoch [43/100], Train Accuracy: 75.692 %, Loss: 0.6393\n",
            "Epoch [43/100], Learning Rate: 0.0007911220577405485\n",
            "Epoch [44/100], Train Accuracy: 75.95 %, Loss: 0.6680\n",
            "Epoch [44/100], Learning Rate: 0.0007790686370876671\n",
            "Epoch [45/100], Train Accuracy: 75.762 %, Loss: 0.6420\n",
            "Epoch [45/100], Learning Rate: 0.0007667891533457719\n",
            "Epoch [46/100], Train Accuracy: 76.48 %, Loss: 0.6453\n",
            "Epoch [46/100], Learning Rate: 0.000754295724882796\n",
            "Epoch [47/100], Train Accuracy: 76.5 %, Loss: 0.7431\n",
            "Epoch [47/100], Learning Rate: 0.0007416006812042827\n",
            "Epoch [48/100], Train Accuracy: 76.388 %, Loss: 0.6404\n",
            "Epoch [48/100], Learning Rate: 0.0007287165507856513\n",
            "Epoch [49/100], Train Accuracy: 76.592 %, Loss: 0.5998\n",
            "Epoch [49/100], Learning Rate: 0.0007156560487081052\n",
            "Epoch [50/100], Train Accuracy: 76.954 %, Loss: 0.6625\n",
            "Epoch [50/100], Learning Rate: 0.0007024320641103813\n",
            "Epoch [51/100], Train Accuracy: 77.012 %, Loss: 0.7059\n",
            "Epoch [51/100], Learning Rate: 0.0006890576474687264\n",
            "Epoch [52/100], Train Accuracy: 77.068 %, Loss: 0.6731\n",
            "Epoch [52/100], Learning Rate: 0.0006755459977176532\n",
            "Epoch [53/100], Train Accuracy: 77.698 %, Loss: 0.6581\n",
            "Epoch [53/100], Learning Rate: 0.0006619104492241846\n",
            "Epoch [54/100], Train Accuracy: 77.592 %, Loss: 0.5301\n",
            "Epoch [54/100], Learning Rate: 0.0006481644586284443\n",
            "Epoch [55/100], Train Accuracy: 77.642 %, Loss: 0.5904\n",
            "Epoch [55/100], Learning Rate: 0.0006343215915635762\n",
            "Epoch [56/100], Train Accuracy: 78.228 %, Loss: 0.6411\n",
            "Epoch [56/100], Learning Rate: 0.000620395509268104\n",
            "Epoch [57/100], Train Accuracy: 77.916 %, Loss: 0.7501\n",
            "Epoch [57/100], Learning Rate: 0.000606399955103937\n",
            "Epoch [58/100], Train Accuracy: 78.48 %, Loss: 0.6294\n",
            "Epoch [58/100], Learning Rate: 0.0005923487409933315\n",
            "Epoch [59/100], Train Accuracy: 78.16 %, Loss: 0.7398\n",
            "Epoch [59/100], Learning Rate: 0.0005782557337881911\n",
            "Epoch [60/100], Train Accuracy: 78.284 %, Loss: 0.6343\n",
            "Epoch [60/100], Learning Rate: 0.0005641348415851578\n",
            "Epoch [61/100], Train Accuracy: 78.574 %, Loss: 0.5846\n",
            "Epoch [61/100], Learning Rate: 0.00055\n",
            "Epoch [62/100], Train Accuracy: 78.806 %, Loss: 0.6236\n",
            "Epoch [62/100], Learning Rate: 0.0005358651584148423\n",
            "Epoch [63/100], Train Accuracy: 78.97 %, Loss: 0.5859\n",
            "Epoch [63/100], Learning Rate: 0.000521744266211809\n",
            "Epoch [64/100], Train Accuracy: 79.214 %, Loss: 0.6244\n",
            "Epoch [64/100], Learning Rate: 0.0005076512590066686\n",
            "Epoch [65/100], Train Accuracy: 79.114 %, Loss: 0.6177\n",
            "Epoch [65/100], Learning Rate: 0.0004936000448960632\n",
            "Epoch [66/100], Train Accuracy: 79.272 %, Loss: 0.5681\n",
            "Epoch [66/100], Learning Rate: 0.0004796044907318961\n",
            "Epoch [67/100], Train Accuracy: 79.728 %, Loss: 0.5557\n",
            "Epoch [67/100], Learning Rate: 0.0004656784084364239\n",
            "Epoch [68/100], Train Accuracy: 79.962 %, Loss: 0.6906\n",
            "Epoch [68/100], Learning Rate: 0.00045183554137155597\n",
            "Epoch [69/100], Train Accuracy: 79.788 %, Loss: 0.6183\n",
            "Epoch [69/100], Learning Rate: 0.0004380895507758153\n",
            "Epoch [70/100], Train Accuracy: 80.374 %, Loss: 0.5943\n",
            "Epoch [70/100], Learning Rate: 0.00042445400228234684\n",
            "Epoch [71/100], Train Accuracy: 80.176 %, Loss: 0.6494\n",
            "Epoch [71/100], Learning Rate: 0.0004109423525312738\n",
            "Epoch [72/100], Train Accuracy: 80.122 %, Loss: 0.6188\n",
            "Epoch [72/100], Learning Rate: 0.0003975679358896189\n",
            "Epoch [73/100], Train Accuracy: 80.594 %, Loss: 0.5125\n",
            "Epoch [73/100], Learning Rate: 0.000384343951291895\n",
            "Epoch [74/100], Train Accuracy: 80.56 %, Loss: 0.5458\n",
            "Epoch [74/100], Learning Rate: 0.00037128344921434864\n",
            "Epoch [75/100], Train Accuracy: 80.798 %, Loss: 0.5161\n",
            "Epoch [75/100], Learning Rate: 0.0003583993187957173\n",
            "Epoch [76/100], Train Accuracy: 81.092 %, Loss: 0.6147\n",
            "Epoch [76/100], Learning Rate: 0.00034570427511720393\n",
            "Epoch [77/100], Train Accuracy: 80.942 %, Loss: 0.5900\n",
            "Epoch [77/100], Learning Rate: 0.0003332108466542281\n",
            "Epoch [78/100], Train Accuracy: 81.238 %, Loss: 0.5479\n",
            "Epoch [78/100], Learning Rate: 0.0003209313629123329\n",
            "Epoch [79/100], Train Accuracy: 81.58 %, Loss: 0.6692\n",
            "Epoch [79/100], Learning Rate: 0.0003088779422594514\n",
            "Epoch [80/100], Train Accuracy: 81.548 %, Loss: 0.4798\n",
            "Epoch [80/100], Learning Rate: 0.0002970624799665412\n",
            "Epoch [81/100], Train Accuracy: 81.758 %, Loss: 0.6063\n",
            "Epoch [81/100], Learning Rate: 0.00028549663646838715\n",
            "Epoch [82/100], Train Accuracy: 81.526 %, Loss: 0.5440\n",
            "Epoch [82/100], Learning Rate: 0.00027419182585616055\n",
            "Epoch [83/100], Train Accuracy: 82.18 %, Loss: 0.5559\n",
            "Epoch [83/100], Learning Rate: 0.0002631592046130896\n",
            "Epoch [84/100], Train Accuracy: 82.134 %, Loss: 0.4925\n",
            "Epoch [84/100], Learning Rate: 0.00025240966060435674\n",
            "Epoch [85/100], Train Accuracy: 82.18 %, Loss: 0.5232\n",
            "Epoch [85/100], Learning Rate: 0.00024195380233209008\n",
            "Epoch [86/100], Train Accuracy: 82.72 %, Loss: 0.4783\n",
            "Epoch [86/100], Learning Rate: 0.00023180194846605365\n",
            "Epoch [87/100], Train Accuracy: 82.502 %, Loss: 0.4801\n",
            "Epoch [87/100], Learning Rate: 0.0002219641176603649\n",
            "Epoch [88/100], Train Accuracy: 82.538 %, Loss: 0.4948\n",
            "Epoch [88/100], Learning Rate: 0.0002124500186662932\n",
            "Epoch [89/100], Train Accuracy: 82.712 %, Loss: 0.4365\n",
            "Epoch [89/100], Learning Rate: 0.00020326904075089488\n",
            "Epoch [90/100], Train Accuracy: 82.946 %, Loss: 0.5265\n",
            "Epoch [90/100], Learning Rate: 0.00019443024443093932\n",
            "Epoch [91/100], Train Accuracy: 82.82 %, Loss: 0.5050\n",
            "Epoch [91/100], Learning Rate: 0.00018594235253127368\n",
            "Epoch [92/100], Train Accuracy: 83.092 %, Loss: 0.5505\n",
            "Epoch [92/100], Learning Rate: 0.00017781374157644722\n",
            "Epoch [93/100], Train Accuracy: 83.114 %, Loss: 0.4831\n",
            "Epoch [93/100], Learning Rate: 0.00017005243352409332\n",
            "Epoch [94/100], Train Accuracy: 83.372 %, Loss: 0.4876\n",
            "Epoch [94/100], Learning Rate: 0.0001626660878482253\n",
            "Epoch [95/100], Train Accuracy: 83.788 %, Loss: 0.5282\n",
            "Epoch [95/100], Learning Rate: 0.0001556619939802614\n",
            "Epoch [96/100], Train Accuracy: 83.716 %, Loss: 0.4418\n",
            "Epoch [96/100], Learning Rate: 0.0001490470641152345\n",
            "Epoch [97/100], Train Accuracy: 83.756 %, Loss: 0.4507\n",
            "Epoch [97/100], Learning Rate: 0.0001428278263902913\n",
            "Epoch [98/100], Train Accuracy: 84.038 %, Loss: 0.4860\n",
            "Epoch [98/100], Learning Rate: 0.00013701041844220858\n",
            "Epoch [99/100], Train Accuracy: 84.198 %, Loss: 0.4408\n",
            "Epoch [99/100], Learning Rate: 0.0001316005813502869\n",
            "Epoch [100/100], Train Accuracy: 84.222 %, Loss: 0.3680\n",
            "Epoch [100/100], Learning Rate: 0.00012660365397059855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習前後のモデルの重みを比較する関数を作成\n",
        "def check_weight_change(model, model_init):\n",
        "    weights_changed = (model.conv1.state_dict()['weight'] != model_init.conv1.state_dict()['weight']).any()\n",
        "    if weights_changed:\n",
        "        return print('モデルの重みが変化しています')\n",
        "    else:\n",
        "        return print('モデルの重みは変化していません')\n",
        "# 学習前後でモデルの重みが変化していないかを確認\n",
        "check_weight_change(model, model_init)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8db31c37-35f0-4deb-9f7c-a45a313e551b",
        "id": "OrCLGCf6lNMp"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "モデルの重みは変化していません\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの評価\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for images, labels in test_loader_cifar10:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度の確認\n",
        "    print('Test Accuracy: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7168771-34a1-4615-fb4e-a3ae902d5eca",
        "id": "-tU0XYuflNMp"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 76.81 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUメモリの解放\n",
        "del model, model_init\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "QgJAtaM8lNMp"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prune_rate=0.9"
      ],
      "metadata": {
        "id": "o7FBIYbw4QVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 刈り込み率を設定\n",
        "prune_rate = 0.9\n",
        "# 初期化手法を設定\n",
        "init = 'signed_constant'\n",
        "\n",
        "# デバイスを設定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# モデルを定義\n",
        "model = supermaskresnet18_10().to(device)\n",
        "# 学習前のモデルの重みを保存\n",
        "model_init = copy.deepcopy(model)\n",
        "\n",
        "# 学習率を設定\n",
        "learning_rate = 0.001\n",
        "# 全体のepoch数を設定\n",
        "num_epochs = 100\n",
        "# warm-upするepoch数を設定\n",
        "warmup_epochs = 10\n",
        "\n",
        "# 損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# オプティマイザーを設定\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# スケジューラーを設定\n",
        "scheduler = CosineLRScheduler(optimizer, t_initial=num_epochs, lr_min=1e-4, warmup_t=warmup_epochs, warmup_lr_init=1e-4, warmup_prefix=True)\n",
        "\n",
        "# モデルの学習\n",
        "for epoch in range(num_epochs):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, (images, labels) in enumerate(train_loader_cifar10):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # 順伝播\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # 逆伝播\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # 全データ数\n",
        "        total += labels.size(0)\n",
        "        # 正解数\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度を計算\n",
        "    train_acc = 100 * correct / total\n",
        "    # 正解率精度と損失を確認\n",
        "    print(\"Epoch [{}/{}], Train Accuracy: {} %, Loss: {:.4f}\".format(epoch+1, num_epochs, 100 * correct / total, loss.item()))\n",
        "    # 1エポック終了後にスケジューラーを更新\n",
        "    scheduler.step(epoch)\n",
        "    # 学習率の確認\n",
        "    print(\"Epoch [{}/{}], Learning Rate: {}\".format(epoch+1, num_epochs, optimizer.param_groups[0]['lr']))\n",
        "\n",
        "# モデルを保存\n",
        "torch.save(model, '/content/drive/MyDrive/SupermaskResNet18_CIFAR10_100epochs_pr0.9_constant_CLRS_restest.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYvldbfH4QVb",
        "outputId": "c2e1c2f2-1454-44f9-f24d-f11efa8bea27"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Accuracy: 35.942 %, Loss: 1.4331\n",
            "Epoch [1/100], Learning Rate: 0.0001\n",
            "Epoch [2/100], Train Accuracy: 46.116 %, Loss: 1.5080\n",
            "Epoch [2/100], Learning Rate: 0.00019\n",
            "Epoch [3/100], Train Accuracy: 49.646 %, Loss: 1.3378\n",
            "Epoch [3/100], Learning Rate: 0.00028\n",
            "Epoch [4/100], Train Accuracy: 52.54 %, Loss: 1.2255\n",
            "Epoch [4/100], Learning Rate: 0.00036999999999999994\n",
            "Epoch [5/100], Train Accuracy: 55.168 %, Loss: 1.2384\n",
            "Epoch [5/100], Learning Rate: 0.00045999999999999996\n",
            "Epoch [6/100], Train Accuracy: 57.506 %, Loss: 1.2358\n",
            "Epoch [6/100], Learning Rate: 0.00055\n",
            "Epoch [7/100], Train Accuracy: 59.4 %, Loss: 1.0552\n",
            "Epoch [7/100], Learning Rate: 0.0006399999999999999\n",
            "Epoch [8/100], Train Accuracy: 61.11 %, Loss: 1.1100\n",
            "Epoch [8/100], Learning Rate: 0.00073\n",
            "Epoch [9/100], Train Accuracy: 62.4 %, Loss: 1.0019\n",
            "Epoch [9/100], Learning Rate: 0.00082\n",
            "Epoch [10/100], Train Accuracy: 63.684 %, Loss: 0.9472\n",
            "Epoch [10/100], Learning Rate: 0.00091\n",
            "Epoch [11/100], Train Accuracy: 64.92 %, Loss: 1.0200\n",
            "Epoch [11/100], Learning Rate: 0.001\n",
            "Epoch [12/100], Train Accuracy: 65.922 %, Loss: 0.8042\n",
            "Epoch [12/100], Learning Rate: 0.0009997779521645793\n",
            "Epoch [13/100], Train Accuracy: 67.632 %, Loss: 0.9133\n",
            "Epoch [13/100], Learning Rate: 0.0009991120277927223\n",
            "Epoch [14/100], Train Accuracy: 68.344 %, Loss: 0.9812\n",
            "Epoch [14/100], Learning Rate: 0.000998002884071386\n",
            "Epoch [15/100], Train Accuracy: 69.304 %, Loss: 0.8716\n",
            "Epoch [15/100], Learning Rate: 0.000996451615591515\n",
            "Epoch [16/100], Train Accuracy: 70.434 %, Loss: 0.9578\n",
            "Epoch [16/100], Learning Rate: 0.000994459753267812\n",
            "Epoch [17/100], Train Accuracy: 71.036 %, Loss: 0.8718\n",
            "Epoch [17/100], Learning Rate: 0.00099202926282791\n",
            "Epoch [18/100], Train Accuracy: 71.176 %, Loss: 0.7461\n",
            "Epoch [18/100], Learning Rate: 0.0009891625428724363\n",
            "Epoch [19/100], Train Accuracy: 72.316 %, Loss: 0.8150\n",
            "Epoch [19/100], Learning Rate: 0.000985862422507884\n",
            "Epoch [20/100], Train Accuracy: 72.65 %, Loss: 0.8336\n",
            "Epoch [20/100], Learning Rate: 0.0009821321585546244\n",
            "Epoch [21/100], Train Accuracy: 72.954 %, Loss: 0.7870\n",
            "Epoch [21/100], Learning Rate: 0.0009779754323328191\n",
            "Epoch [22/100], Train Accuracy: 73.606 %, Loss: 0.8836\n",
            "Epoch [22/100], Learning Rate: 0.0009733963460294015\n",
            "Epoch [23/100], Train Accuracy: 74.262 %, Loss: 0.7081\n",
            "Epoch [23/100], Learning Rate: 0.0009683994186497131\n",
            "Epoch [24/100], Train Accuracy: 74.2 %, Loss: 0.8120\n",
            "Epoch [24/100], Learning Rate: 0.0009629895815577914\n",
            "Epoch [25/100], Train Accuracy: 74.582 %, Loss: 0.7136\n",
            "Epoch [25/100], Learning Rate: 0.0009571721736097088\n",
            "Epoch [26/100], Train Accuracy: 75.364 %, Loss: 0.7910\n",
            "Epoch [26/100], Learning Rate: 0.0009509529358847655\n",
            "Epoch [27/100], Train Accuracy: 74.888 %, Loss: 0.7193\n",
            "Epoch [27/100], Learning Rate: 0.0009443380060197386\n",
            "Epoch [28/100], Train Accuracy: 76.028 %, Loss: 0.7491\n",
            "Epoch [28/100], Learning Rate: 0.0009373339121517746\n",
            "Epoch [29/100], Train Accuracy: 75.872 %, Loss: 0.8610\n",
            "Epoch [29/100], Learning Rate: 0.0009299475664759069\n",
            "Epoch [30/100], Train Accuracy: 75.902 %, Loss: 0.7494\n",
            "Epoch [30/100], Learning Rate: 0.0009221862584235528\n",
            "Epoch [31/100], Train Accuracy: 76.792 %, Loss: 0.7469\n",
            "Epoch [31/100], Learning Rate: 0.0009140576474687263\n",
            "Epoch [32/100], Train Accuracy: 76.682 %, Loss: 0.7450\n",
            "Epoch [32/100], Learning Rate: 0.0009055697555690606\n",
            "Epoch [33/100], Train Accuracy: 77.202 %, Loss: 0.6212\n",
            "Epoch [33/100], Learning Rate: 0.0008967309592491052\n",
            "Epoch [34/100], Train Accuracy: 77.224 %, Loss: 0.7196\n",
            "Epoch [34/100], Learning Rate: 0.0008875499813337069\n",
            "Epoch [35/100], Train Accuracy: 77.586 %, Loss: 0.5854\n",
            "Epoch [35/100], Learning Rate: 0.0008780358823396353\n",
            "Epoch [36/100], Train Accuracy: 77.792 %, Loss: 0.6209\n",
            "Epoch [36/100], Learning Rate: 0.0008681980515339464\n",
            "Epoch [37/100], Train Accuracy: 77.962 %, Loss: 0.5463\n",
            "Epoch [37/100], Learning Rate: 0.0008580461976679099\n",
            "Epoch [38/100], Train Accuracy: 78.254 %, Loss: 0.6481\n",
            "Epoch [38/100], Learning Rate: 0.0008475903393956434\n",
            "Epoch [39/100], Train Accuracy: 78.552 %, Loss: 0.6184\n",
            "Epoch [39/100], Learning Rate: 0.0008368407953869105\n",
            "Epoch [40/100], Train Accuracy: 78.474 %, Loss: 0.7019\n",
            "Epoch [40/100], Learning Rate: 0.0008258081741438395\n",
            "Epoch [41/100], Train Accuracy: 78.908 %, Loss: 0.5861\n",
            "Epoch [41/100], Learning Rate: 0.0008145033635316131\n",
            "Epoch [42/100], Train Accuracy: 79.354 %, Loss: 0.5503\n",
            "Epoch [42/100], Learning Rate: 0.0008029375200334589\n",
            "Epoch [43/100], Train Accuracy: 79.416 %, Loss: 0.5722\n",
            "Epoch [43/100], Learning Rate: 0.0007911220577405485\n",
            "Epoch [44/100], Train Accuracy: 79.318 %, Loss: 0.5865\n",
            "Epoch [44/100], Learning Rate: 0.0007790686370876671\n",
            "Epoch [45/100], Train Accuracy: 79.562 %, Loss: 0.5758\n",
            "Epoch [45/100], Learning Rate: 0.0007667891533457719\n",
            "Epoch [46/100], Train Accuracy: 80.022 %, Loss: 0.5513\n",
            "Epoch [46/100], Learning Rate: 0.000754295724882796\n",
            "Epoch [47/100], Train Accuracy: 79.958 %, Loss: 0.6216\n",
            "Epoch [47/100], Learning Rate: 0.0007416006812042827\n",
            "Epoch [48/100], Train Accuracy: 80.03 %, Loss: 0.7171\n",
            "Epoch [48/100], Learning Rate: 0.0007287165507856513\n",
            "Epoch [49/100], Train Accuracy: 80.508 %, Loss: 0.6691\n",
            "Epoch [49/100], Learning Rate: 0.0007156560487081052\n",
            "Epoch [50/100], Train Accuracy: 80.718 %, Loss: 0.5575\n",
            "Epoch [50/100], Learning Rate: 0.0007024320641103813\n",
            "Epoch [51/100], Train Accuracy: 80.92 %, Loss: 0.5000\n",
            "Epoch [51/100], Learning Rate: 0.0006890576474687264\n",
            "Epoch [52/100], Train Accuracy: 80.956 %, Loss: 0.4552\n",
            "Epoch [52/100], Learning Rate: 0.0006755459977176532\n",
            "Epoch [53/100], Train Accuracy: 81.27 %, Loss: 0.4890\n",
            "Epoch [53/100], Learning Rate: 0.0006619104492241846\n",
            "Epoch [54/100], Train Accuracy: 81.282 %, Loss: 0.5119\n",
            "Epoch [54/100], Learning Rate: 0.0006481644586284443\n",
            "Epoch [55/100], Train Accuracy: 81.438 %, Loss: 0.5469\n",
            "Epoch [55/100], Learning Rate: 0.0006343215915635762\n",
            "Epoch [56/100], Train Accuracy: 81.758 %, Loss: 0.5006\n",
            "Epoch [56/100], Learning Rate: 0.000620395509268104\n",
            "Epoch [57/100], Train Accuracy: 81.678 %, Loss: 0.4395\n",
            "Epoch [57/100], Learning Rate: 0.000606399955103937\n",
            "Epoch [58/100], Train Accuracy: 81.728 %, Loss: 0.5559\n",
            "Epoch [58/100], Learning Rate: 0.0005923487409933315\n",
            "Epoch [59/100], Train Accuracy: 82.464 %, Loss: 0.4944\n",
            "Epoch [59/100], Learning Rate: 0.0005782557337881911\n",
            "Epoch [60/100], Train Accuracy: 82.108 %, Loss: 0.4386\n",
            "Epoch [60/100], Learning Rate: 0.0005641348415851578\n",
            "Epoch [61/100], Train Accuracy: 82.628 %, Loss: 0.5754\n",
            "Epoch [61/100], Learning Rate: 0.00055\n",
            "Epoch [62/100], Train Accuracy: 82.802 %, Loss: 0.4609\n",
            "Epoch [62/100], Learning Rate: 0.0005358651584148423\n",
            "Epoch [63/100], Train Accuracy: 82.62 %, Loss: 0.5139\n",
            "Epoch [63/100], Learning Rate: 0.000521744266211809\n",
            "Epoch [64/100], Train Accuracy: 82.992 %, Loss: 0.5172\n",
            "Epoch [64/100], Learning Rate: 0.0005076512590066686\n",
            "Epoch [65/100], Train Accuracy: 83.334 %, Loss: 0.5310\n",
            "Epoch [65/100], Learning Rate: 0.0004936000448960632\n",
            "Epoch [66/100], Train Accuracy: 83.454 %, Loss: 0.4802\n",
            "Epoch [66/100], Learning Rate: 0.0004796044907318961\n",
            "Epoch [67/100], Train Accuracy: 83.456 %, Loss: 0.5270\n",
            "Epoch [67/100], Learning Rate: 0.0004656784084364239\n",
            "Epoch [68/100], Train Accuracy: 83.358 %, Loss: 0.4738\n",
            "Epoch [68/100], Learning Rate: 0.00045183554137155597\n",
            "Epoch [69/100], Train Accuracy: 83.786 %, Loss: 0.4848\n",
            "Epoch [69/100], Learning Rate: 0.0004380895507758153\n",
            "Epoch [70/100], Train Accuracy: 84.312 %, Loss: 0.4694\n",
            "Epoch [70/100], Learning Rate: 0.00042445400228234684\n",
            "Epoch [71/100], Train Accuracy: 84.22 %, Loss: 0.4805\n",
            "Epoch [71/100], Learning Rate: 0.0004109423525312738\n",
            "Epoch [72/100], Train Accuracy: 84.19 %, Loss: 0.6495\n",
            "Epoch [72/100], Learning Rate: 0.0003975679358896189\n",
            "Epoch [73/100], Train Accuracy: 84.734 %, Loss: 0.4352\n",
            "Epoch [73/100], Learning Rate: 0.000384343951291895\n",
            "Epoch [74/100], Train Accuracy: 84.88 %, Loss: 0.3696\n",
            "Epoch [74/100], Learning Rate: 0.00037128344921434864\n",
            "Epoch [75/100], Train Accuracy: 84.97 %, Loss: 0.4868\n",
            "Epoch [75/100], Learning Rate: 0.0003583993187957173\n",
            "Epoch [76/100], Train Accuracy: 85.036 %, Loss: 0.5276\n",
            "Epoch [76/100], Learning Rate: 0.00034570427511720393\n",
            "Epoch [77/100], Train Accuracy: 85.384 %, Loss: 0.4207\n",
            "Epoch [77/100], Learning Rate: 0.0003332108466542281\n",
            "Epoch [78/100], Train Accuracy: 85.258 %, Loss: 0.4690\n",
            "Epoch [78/100], Learning Rate: 0.0003209313629123329\n",
            "Epoch [79/100], Train Accuracy: 85.78 %, Loss: 0.4672\n",
            "Epoch [79/100], Learning Rate: 0.0003088779422594514\n",
            "Epoch [80/100], Train Accuracy: 85.586 %, Loss: 0.4101\n",
            "Epoch [80/100], Learning Rate: 0.0002970624799665412\n",
            "Epoch [81/100], Train Accuracy: 85.826 %, Loss: 0.3353\n",
            "Epoch [81/100], Learning Rate: 0.00028549663646838715\n",
            "Epoch [82/100], Train Accuracy: 86.446 %, Loss: 0.3916\n",
            "Epoch [82/100], Learning Rate: 0.00027419182585616055\n",
            "Epoch [83/100], Train Accuracy: 86.462 %, Loss: 0.3702\n",
            "Epoch [83/100], Learning Rate: 0.0002631592046130896\n",
            "Epoch [84/100], Train Accuracy: 86.45 %, Loss: 0.4131\n",
            "Epoch [84/100], Learning Rate: 0.00025240966060435674\n",
            "Epoch [85/100], Train Accuracy: 86.994 %, Loss: 0.4122\n",
            "Epoch [85/100], Learning Rate: 0.00024195380233209008\n",
            "Epoch [86/100], Train Accuracy: 87.034 %, Loss: 0.3703\n",
            "Epoch [86/100], Learning Rate: 0.00023180194846605365\n",
            "Epoch [87/100], Train Accuracy: 87.052 %, Loss: 0.3535\n",
            "Epoch [87/100], Learning Rate: 0.0002219641176603649\n",
            "Epoch [88/100], Train Accuracy: 87.44 %, Loss: 0.3950\n",
            "Epoch [88/100], Learning Rate: 0.0002124500186662932\n",
            "Epoch [89/100], Train Accuracy: 87.416 %, Loss: 0.3840\n",
            "Epoch [89/100], Learning Rate: 0.00020326904075089488\n",
            "Epoch [90/100], Train Accuracy: 87.592 %, Loss: 0.3571\n",
            "Epoch [90/100], Learning Rate: 0.00019443024443093932\n",
            "Epoch [91/100], Train Accuracy: 87.764 %, Loss: 0.2580\n",
            "Epoch [91/100], Learning Rate: 0.00018594235253127368\n",
            "Epoch [92/100], Train Accuracy: 87.882 %, Loss: 0.3633\n",
            "Epoch [92/100], Learning Rate: 0.00017781374157644722\n",
            "Epoch [93/100], Train Accuracy: 87.984 %, Loss: 0.3495\n",
            "Epoch [93/100], Learning Rate: 0.00017005243352409332\n",
            "Epoch [94/100], Train Accuracy: 88.076 %, Loss: 0.3970\n",
            "Epoch [94/100], Learning Rate: 0.0001626660878482253\n",
            "Epoch [95/100], Train Accuracy: 88.274 %, Loss: 0.3319\n",
            "Epoch [95/100], Learning Rate: 0.0001556619939802614\n",
            "Epoch [96/100], Train Accuracy: 88.722 %, Loss: 0.2895\n",
            "Epoch [96/100], Learning Rate: 0.0001490470641152345\n",
            "Epoch [97/100], Train Accuracy: 88.322 %, Loss: 0.3127\n",
            "Epoch [97/100], Learning Rate: 0.0001428278263902913\n",
            "Epoch [98/100], Train Accuracy: 88.6 %, Loss: 0.3116\n",
            "Epoch [98/100], Learning Rate: 0.00013701041844220858\n",
            "Epoch [99/100], Train Accuracy: 88.68 %, Loss: 0.2967\n",
            "Epoch [99/100], Learning Rate: 0.0001316005813502869\n",
            "Epoch [100/100], Train Accuracy: 89.156 %, Loss: 0.3028\n",
            "Epoch [100/100], Learning Rate: 0.00012660365397059855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習前後のモデルの重みを比較する関数を作成\n",
        "def check_weight_change(model, model_init):\n",
        "    weights_changed = (model.conv1.state_dict()['weight'] != model_init.conv1.state_dict()['weight']).any()\n",
        "    if weights_changed:\n",
        "        return print('モデルの重みが変化しています')\n",
        "    else:\n",
        "        return print('モデルの重みは変化していません')\n",
        "# 学習前後でモデルの重みが変化していないかを確認\n",
        "check_weight_change(model, model_init)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kHl6ABi4QVb",
        "outputId": "e5d6afd5-a290-405a-da65-d2573e946ddb"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "モデルの重みは変化していません\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの評価\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for images, labels in test_loader_cifar10:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度の確認\n",
        "    print('Test Accuracy: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6K7xX_Re4QVb",
        "outputId": "73ad35c3-ff19-41f8-d5c3-c3d92dcf8eac"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 79.97 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUメモリの解放\n",
        "del model, model_init\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "27CpjoYA4QVb"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prune_rate=0.7"
      ],
      "metadata": {
        "id": "BCsx6dqv4QVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 刈り込み率を設定\n",
        "prune_rate = 0.7\n",
        "# 初期化手法を設定\n",
        "init = 'signed_constant'\n",
        "\n",
        "# デバイスを設定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# モデルを定義\n",
        "model = supermaskresnet18_10().to(device)\n",
        "# 学習前のモデルの重みを保存\n",
        "model_init = copy.deepcopy(model)\n",
        "\n",
        "# 学習率を設定\n",
        "learning_rate = 0.001\n",
        "# 全体のepoch数を設定\n",
        "num_epochs = 100\n",
        "# warm-upするepoch数を設定\n",
        "warmup_epochs = 10\n",
        "\n",
        "# 損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# オプティマイザーを設定\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# スケジューラーを設定\n",
        "scheduler = CosineLRScheduler(optimizer, t_initial=num_epochs, lr_min=1e-4, warmup_t=warmup_epochs, warmup_lr_init=1e-4, warmup_prefix=True)\n",
        "\n",
        "# モデルの学習\n",
        "for epoch in range(num_epochs):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, (images, labels) in enumerate(train_loader_cifar10):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # 順伝播\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # 逆伝播\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # 全データ数\n",
        "        total += labels.size(0)\n",
        "        # 正解数\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度を計算\n",
        "    train_acc = 100 * correct / total\n",
        "    # 正解率精度と損失を確認\n",
        "    print(\"Epoch [{}/{}], Train Accuracy: {} %, Loss: {:.4f}\".format(epoch+1, num_epochs, 100 * correct / total, loss.item()))\n",
        "    # 1エポック終了後にスケジューラーを更新\n",
        "    scheduler.step(epoch)\n",
        "    # 学習率の確認\n",
        "    print(\"Epoch [{}/{}], Learning Rate: {}\".format(epoch+1, num_epochs, optimizer.param_groups[0]['lr']))\n",
        "\n",
        "# モデルを保存\n",
        "torch.save(model, '/content/drive/MyDrive/SupermaskResNet18_CIFAR10_100epochs_pr0.7_constant_CLRS_restest.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e24e838-6129-4c2a-8092-6dc602ce26ca",
        "id": "VI8Z7dMo4QVb"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Accuracy: 36.196 %, Loss: 1.5892\n",
            "Epoch [1/100], Learning Rate: 0.0001\n",
            "Epoch [2/100], Train Accuracy: 46.324 %, Loss: 1.4685\n",
            "Epoch [2/100], Learning Rate: 0.00019\n",
            "Epoch [3/100], Train Accuracy: 49.97 %, Loss: 1.2599\n",
            "Epoch [3/100], Learning Rate: 0.00028\n",
            "Epoch [4/100], Train Accuracy: 54.17 %, Loss: 1.1310\n",
            "Epoch [4/100], Learning Rate: 0.00036999999999999994\n",
            "Epoch [5/100], Train Accuracy: 57.12 %, Loss: 1.0668\n",
            "Epoch [5/100], Learning Rate: 0.00045999999999999996\n",
            "Epoch [6/100], Train Accuracy: 60.256 %, Loss: 1.0706\n",
            "Epoch [6/100], Learning Rate: 0.00055\n",
            "Epoch [7/100], Train Accuracy: 61.952 %, Loss: 1.0906\n",
            "Epoch [7/100], Learning Rate: 0.0006399999999999999\n",
            "Epoch [8/100], Train Accuracy: 63.484 %, Loss: 0.9589\n",
            "Epoch [8/100], Learning Rate: 0.00073\n",
            "Epoch [9/100], Train Accuracy: 65.176 %, Loss: 0.9318\n",
            "Epoch [9/100], Learning Rate: 0.00082\n",
            "Epoch [10/100], Train Accuracy: 66.512 %, Loss: 0.8538\n",
            "Epoch [10/100], Learning Rate: 0.00091\n",
            "Epoch [11/100], Train Accuracy: 67.524 %, Loss: 0.9030\n",
            "Epoch [11/100], Learning Rate: 0.001\n",
            "Epoch [12/100], Train Accuracy: 68.808 %, Loss: 0.7071\n",
            "Epoch [12/100], Learning Rate: 0.0009997779521645793\n",
            "Epoch [13/100], Train Accuracy: 70.262 %, Loss: 0.7961\n",
            "Epoch [13/100], Learning Rate: 0.0009991120277927223\n",
            "Epoch [14/100], Train Accuracy: 71.518 %, Loss: 0.8321\n",
            "Epoch [14/100], Learning Rate: 0.000998002884071386\n",
            "Epoch [15/100], Train Accuracy: 71.974 %, Loss: 0.8616\n",
            "Epoch [15/100], Learning Rate: 0.000996451615591515\n",
            "Epoch [16/100], Train Accuracy: 72.848 %, Loss: 0.7420\n",
            "Epoch [16/100], Learning Rate: 0.000994459753267812\n",
            "Epoch [17/100], Train Accuracy: 73.73 %, Loss: 0.7313\n",
            "Epoch [17/100], Learning Rate: 0.00099202926282791\n",
            "Epoch [18/100], Train Accuracy: 74.266 %, Loss: 0.7543\n",
            "Epoch [18/100], Learning Rate: 0.0009891625428724363\n",
            "Epoch [19/100], Train Accuracy: 74.574 %, Loss: 0.6826\n",
            "Epoch [19/100], Learning Rate: 0.000985862422507884\n",
            "Epoch [20/100], Train Accuracy: 75.112 %, Loss: 0.6853\n",
            "Epoch [20/100], Learning Rate: 0.0009821321585546244\n",
            "Epoch [21/100], Train Accuracy: 75.598 %, Loss: 0.6880\n",
            "Epoch [21/100], Learning Rate: 0.0009779754323328191\n",
            "Epoch [22/100], Train Accuracy: 75.854 %, Loss: 0.6560\n",
            "Epoch [22/100], Learning Rate: 0.0009733963460294015\n",
            "Epoch [23/100], Train Accuracy: 76.608 %, Loss: 0.6734\n",
            "Epoch [23/100], Learning Rate: 0.0009683994186497131\n",
            "Epoch [24/100], Train Accuracy: 76.85 %, Loss: 0.5907\n",
            "Epoch [24/100], Learning Rate: 0.0009629895815577914\n",
            "Epoch [25/100], Train Accuracy: 77.23 %, Loss: 0.7390\n",
            "Epoch [25/100], Learning Rate: 0.0009571721736097088\n",
            "Epoch [26/100], Train Accuracy: 77.524 %, Loss: 0.6302\n",
            "Epoch [26/100], Learning Rate: 0.0009509529358847655\n",
            "Epoch [27/100], Train Accuracy: 78.124 %, Loss: 0.7016\n",
            "Epoch [27/100], Learning Rate: 0.0009443380060197386\n",
            "Epoch [28/100], Train Accuracy: 78.214 %, Loss: 0.5246\n",
            "Epoch [28/100], Learning Rate: 0.0009373339121517746\n",
            "Epoch [29/100], Train Accuracy: 78.624 %, Loss: 0.6281\n",
            "Epoch [29/100], Learning Rate: 0.0009299475664759069\n",
            "Epoch [30/100], Train Accuracy: 78.62 %, Loss: 0.5959\n",
            "Epoch [30/100], Learning Rate: 0.0009221862584235528\n",
            "Epoch [31/100], Train Accuracy: 79.188 %, Loss: 0.5637\n",
            "Epoch [31/100], Learning Rate: 0.0009140576474687263\n",
            "Epoch [32/100], Train Accuracy: 79.34 %, Loss: 0.6283\n",
            "Epoch [32/100], Learning Rate: 0.0009055697555690606\n",
            "Epoch [33/100], Train Accuracy: 79.354 %, Loss: 0.6457\n",
            "Epoch [33/100], Learning Rate: 0.0008967309592491052\n",
            "Epoch [34/100], Train Accuracy: 79.932 %, Loss: 0.6273\n",
            "Epoch [34/100], Learning Rate: 0.0008875499813337069\n",
            "Epoch [35/100], Train Accuracy: 80.01 %, Loss: 0.5698\n",
            "Epoch [35/100], Learning Rate: 0.0008780358823396353\n",
            "Epoch [36/100], Train Accuracy: 80.144 %, Loss: 0.6328\n",
            "Epoch [36/100], Learning Rate: 0.0008681980515339464\n",
            "Epoch [37/100], Train Accuracy: 80.658 %, Loss: 0.5666\n",
            "Epoch [37/100], Learning Rate: 0.0008580461976679099\n",
            "Epoch [38/100], Train Accuracy: 80.476 %, Loss: 0.6112\n",
            "Epoch [38/100], Learning Rate: 0.0008475903393956434\n",
            "Epoch [39/100], Train Accuracy: 81.07 %, Loss: 0.5140\n",
            "Epoch [39/100], Learning Rate: 0.0008368407953869105\n",
            "Epoch [40/100], Train Accuracy: 81.264 %, Loss: 0.6224\n",
            "Epoch [40/100], Learning Rate: 0.0008258081741438395\n",
            "Epoch [41/100], Train Accuracy: 81.494 %, Loss: 0.5623\n",
            "Epoch [41/100], Learning Rate: 0.0008145033635316131\n",
            "Epoch [42/100], Train Accuracy: 81.454 %, Loss: 0.5385\n",
            "Epoch [42/100], Learning Rate: 0.0008029375200334589\n",
            "Epoch [43/100], Train Accuracy: 81.77 %, Loss: 0.5382\n",
            "Epoch [43/100], Learning Rate: 0.0007911220577405485\n",
            "Epoch [44/100], Train Accuracy: 81.742 %, Loss: 0.5491\n",
            "Epoch [44/100], Learning Rate: 0.0007790686370876671\n",
            "Epoch [45/100], Train Accuracy: 82.556 %, Loss: 0.5512\n",
            "Epoch [45/100], Learning Rate: 0.0007667891533457719\n",
            "Epoch [46/100], Train Accuracy: 82.712 %, Loss: 0.4828\n",
            "Epoch [46/100], Learning Rate: 0.000754295724882796\n",
            "Epoch [47/100], Train Accuracy: 82.824 %, Loss: 0.5332\n",
            "Epoch [47/100], Learning Rate: 0.0007416006812042827\n",
            "Epoch [48/100], Train Accuracy: 82.934 %, Loss: 0.5683\n",
            "Epoch [48/100], Learning Rate: 0.0007287165507856513\n",
            "Epoch [49/100], Train Accuracy: 83.458 %, Loss: 0.5259\n",
            "Epoch [49/100], Learning Rate: 0.0007156560487081052\n",
            "Epoch [50/100], Train Accuracy: 83.43 %, Loss: 0.5654\n",
            "Epoch [50/100], Learning Rate: 0.0007024320641103813\n",
            "Epoch [51/100], Train Accuracy: 83.684 %, Loss: 0.4506\n",
            "Epoch [51/100], Learning Rate: 0.0006890576474687264\n",
            "Epoch [52/100], Train Accuracy: 83.676 %, Loss: 0.5239\n",
            "Epoch [52/100], Learning Rate: 0.0006755459977176532\n",
            "Epoch [53/100], Train Accuracy: 84.102 %, Loss: 0.4449\n",
            "Epoch [53/100], Learning Rate: 0.0006619104492241846\n",
            "Epoch [54/100], Train Accuracy: 84.218 %, Loss: 0.4903\n",
            "Epoch [54/100], Learning Rate: 0.0006481644586284443\n",
            "Epoch [55/100], Train Accuracy: 84.564 %, Loss: 0.4402\n",
            "Epoch [55/100], Learning Rate: 0.0006343215915635762\n",
            "Epoch [56/100], Train Accuracy: 84.584 %, Loss: 0.3953\n",
            "Epoch [56/100], Learning Rate: 0.000620395509268104\n",
            "Epoch [57/100], Train Accuracy: 85.074 %, Loss: 0.4464\n",
            "Epoch [57/100], Learning Rate: 0.000606399955103937\n",
            "Epoch [58/100], Train Accuracy: 84.97 %, Loss: 0.3872\n",
            "Epoch [58/100], Learning Rate: 0.0005923487409933315\n",
            "Epoch [59/100], Train Accuracy: 85.374 %, Loss: 0.4086\n",
            "Epoch [59/100], Learning Rate: 0.0005782557337881911\n",
            "Epoch [60/100], Train Accuracy: 85.636 %, Loss: 0.3774\n",
            "Epoch [60/100], Learning Rate: 0.0005641348415851578\n",
            "Epoch [61/100], Train Accuracy: 85.464 %, Loss: 0.4153\n",
            "Epoch [61/100], Learning Rate: 0.00055\n",
            "Epoch [62/100], Train Accuracy: 85.858 %, Loss: 0.4389\n",
            "Epoch [62/100], Learning Rate: 0.0005358651584148423\n",
            "Epoch [63/100], Train Accuracy: 86.252 %, Loss: 0.3623\n",
            "Epoch [63/100], Learning Rate: 0.000521744266211809\n",
            "Epoch [64/100], Train Accuracy: 86.054 %, Loss: 0.3993\n",
            "Epoch [64/100], Learning Rate: 0.0005076512590066686\n",
            "Epoch [65/100], Train Accuracy: 86.794 %, Loss: 0.3769\n",
            "Epoch [65/100], Learning Rate: 0.0004936000448960632\n",
            "Epoch [66/100], Train Accuracy: 86.982 %, Loss: 0.3918\n",
            "Epoch [66/100], Learning Rate: 0.0004796044907318961\n",
            "Epoch [67/100], Train Accuracy: 87.05 %, Loss: 0.3664\n",
            "Epoch [67/100], Learning Rate: 0.0004656784084364239\n",
            "Epoch [68/100], Train Accuracy: 87.232 %, Loss: 0.2811\n",
            "Epoch [68/100], Learning Rate: 0.00045183554137155597\n",
            "Epoch [69/100], Train Accuracy: 86.98 %, Loss: 0.3071\n",
            "Epoch [69/100], Learning Rate: 0.0004380895507758153\n",
            "Epoch [70/100], Train Accuracy: 87.638 %, Loss: 0.3373\n",
            "Epoch [70/100], Learning Rate: 0.00042445400228234684\n",
            "Epoch [71/100], Train Accuracy: 87.964 %, Loss: 0.2663\n",
            "Epoch [71/100], Learning Rate: 0.0004109423525312738\n",
            "Epoch [72/100], Train Accuracy: 88.012 %, Loss: 0.3336\n",
            "Epoch [72/100], Learning Rate: 0.0003975679358896189\n",
            "Epoch [73/100], Train Accuracy: 88.41 %, Loss: 0.3419\n",
            "Epoch [73/100], Learning Rate: 0.000384343951291895\n",
            "Epoch [74/100], Train Accuracy: 88.328 %, Loss: 0.3124\n",
            "Epoch [74/100], Learning Rate: 0.00037128344921434864\n",
            "Epoch [75/100], Train Accuracy: 88.604 %, Loss: 0.3227\n",
            "Epoch [75/100], Learning Rate: 0.0003583993187957173\n",
            "Epoch [76/100], Train Accuracy: 89.01 %, Loss: 0.2901\n",
            "Epoch [76/100], Learning Rate: 0.00034570427511720393\n",
            "Epoch [77/100], Train Accuracy: 88.904 %, Loss: 0.3674\n",
            "Epoch [77/100], Learning Rate: 0.0003332108466542281\n",
            "Epoch [78/100], Train Accuracy: 88.992 %, Loss: 0.2508\n",
            "Epoch [78/100], Learning Rate: 0.0003209313629123329\n",
            "Epoch [79/100], Train Accuracy: 89.316 %, Loss: 0.2780\n",
            "Epoch [79/100], Learning Rate: 0.0003088779422594514\n",
            "Epoch [80/100], Train Accuracy: 89.624 %, Loss: 0.2894\n",
            "Epoch [80/100], Learning Rate: 0.0002970624799665412\n",
            "Epoch [81/100], Train Accuracy: 90.016 %, Loss: 0.3850\n",
            "Epoch [81/100], Learning Rate: 0.00028549663646838715\n",
            "Epoch [82/100], Train Accuracy: 89.854 %, Loss: 0.3252\n",
            "Epoch [82/100], Learning Rate: 0.00027419182585616055\n",
            "Epoch [83/100], Train Accuracy: 90.072 %, Loss: 0.2434\n",
            "Epoch [83/100], Learning Rate: 0.0002631592046130896\n",
            "Epoch [84/100], Train Accuracy: 90.35 %, Loss: 0.2151\n",
            "Epoch [84/100], Learning Rate: 0.00025240966060435674\n",
            "Epoch [85/100], Train Accuracy: 90.366 %, Loss: 0.3435\n",
            "Epoch [85/100], Learning Rate: 0.00024195380233209008\n",
            "Epoch [86/100], Train Accuracy: 90.704 %, Loss: 0.2612\n",
            "Epoch [86/100], Learning Rate: 0.00023180194846605365\n",
            "Epoch [87/100], Train Accuracy: 90.958 %, Loss: 0.2419\n",
            "Epoch [87/100], Learning Rate: 0.0002219641176603649\n",
            "Epoch [88/100], Train Accuracy: 91.07 %, Loss: 0.3544\n",
            "Epoch [88/100], Learning Rate: 0.0002124500186662932\n",
            "Epoch [89/100], Train Accuracy: 91.578 %, Loss: 0.2914\n",
            "Epoch [89/100], Learning Rate: 0.00020326904075089488\n",
            "Epoch [90/100], Train Accuracy: 91.546 %, Loss: 0.2493\n",
            "Epoch [90/100], Learning Rate: 0.00019443024443093932\n",
            "Epoch [91/100], Train Accuracy: 91.778 %, Loss: 0.2398\n",
            "Epoch [91/100], Learning Rate: 0.00018594235253127368\n",
            "Epoch [92/100], Train Accuracy: 91.738 %, Loss: 0.2182\n",
            "Epoch [92/100], Learning Rate: 0.00017781374157644722\n",
            "Epoch [93/100], Train Accuracy: 92.01 %, Loss: 0.2004\n",
            "Epoch [93/100], Learning Rate: 0.00017005243352409332\n",
            "Epoch [94/100], Train Accuracy: 92.102 %, Loss: 0.2108\n",
            "Epoch [94/100], Learning Rate: 0.0001626660878482253\n",
            "Epoch [95/100], Train Accuracy: 92.48 %, Loss: 0.1862\n",
            "Epoch [95/100], Learning Rate: 0.0001556619939802614\n",
            "Epoch [96/100], Train Accuracy: 92.72 %, Loss: 0.1880\n",
            "Epoch [96/100], Learning Rate: 0.0001490470641152345\n",
            "Epoch [97/100], Train Accuracy: 92.752 %, Loss: 0.2083\n",
            "Epoch [97/100], Learning Rate: 0.0001428278263902913\n",
            "Epoch [98/100], Train Accuracy: 92.98 %, Loss: 0.2497\n",
            "Epoch [98/100], Learning Rate: 0.00013701041844220858\n",
            "Epoch [99/100], Train Accuracy: 93.04 %, Loss: 0.1581\n",
            "Epoch [99/100], Learning Rate: 0.0001316005813502869\n",
            "Epoch [100/100], Train Accuracy: 93.324 %, Loss: 0.2251\n",
            "Epoch [100/100], Learning Rate: 0.00012660365397059855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習前後のモデルの重みを比較する関数を作成\n",
        "def check_weight_change(model, model_init):\n",
        "    weights_changed = (model.conv1.state_dict()['weight'] != model_init.conv1.state_dict()['weight']).any()\n",
        "    if weights_changed:\n",
        "        return print('モデルの重みが変化しています')\n",
        "    else:\n",
        "        return print('モデルの重みは変化していません')\n",
        "# 学習前後でモデルの重みが変化していないかを確認\n",
        "check_weight_change(model, model_init)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c9077cf-5207-4960-8fb4-c9f908a3c4a3",
        "id": "6jyjzb1W4QVc"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "モデルの重みは変化していません\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの評価\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for images, labels in test_loader_cifar10:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度の確認\n",
        "    print('Test Accuracy: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20ffb422-8e17-4f88-ff29-a34d636bc9d5",
        "id": "MnKIen4W4QVc"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 81.77 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUメモリの解放\n",
        "del model, model_init\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "H75JdZqp4QVc"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### prune_rate=0.5"
      ],
      "metadata": {
        "id": "PSXOlVSN4QVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 刈り込み率を設定\n",
        "prune_rate = 0.5\n",
        "# 初期化手法を設定\n",
        "init = 'signed_constant'\n",
        "\n",
        "# デバイスを設定\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# モデルを定義\n",
        "model = supermaskresnet18_10().to(device)\n",
        "# 学習前のモデルの重みを保存\n",
        "model_init = copy.deepcopy(model)\n",
        "\n",
        "# 学習率を設定\n",
        "learning_rate = 0.001\n",
        "# 全体のepoch数を設定\n",
        "num_epochs = 100\n",
        "# warm-upするepoch数を設定\n",
        "warmup_epochs = 10\n",
        "\n",
        "# 損失関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# オプティマイザーを設定\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "# スケジューラーを設定\n",
        "scheduler = CosineLRScheduler(optimizer, t_initial=num_epochs, lr_min=1e-4, warmup_t=warmup_epochs, warmup_lr_init=1e-4, warmup_prefix=True)\n",
        "\n",
        "# モデルの学習\n",
        "for epoch in range(num_epochs):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, (images, labels) in enumerate(train_loader_cifar10):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # 順伝播\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        # 逆伝播\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # 全データ数\n",
        "        total += labels.size(0)\n",
        "        # 正解数\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度を計算\n",
        "    train_acc = 100 * correct / total\n",
        "    # 正解率精度と損失を確認\n",
        "    print(\"Epoch [{}/{}], Train Accuracy: {} %, Loss: {:.4f}\".format(epoch+1, num_epochs, 100 * correct / total, loss.item()))\n",
        "    # 1エポック終了後にスケジューラーを更新\n",
        "    scheduler.step(epoch)\n",
        "    # 学習率の確認\n",
        "    print(\"Epoch [{}/{}], Learning Rate: {}\".format(epoch+1, num_epochs, optimizer.param_groups[0]['lr']))\n",
        "\n",
        "# モデルを保存\n",
        "torch.save(model, '/content/drive/MyDrive/SupermaskResNet18_CIFAR10_100epochs_pr0.5_constant_CLRS_restest.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "217f4b19-82de-44cf-c3d6-e8bc1adf3455",
        "id": "oA5SOIRF4QVc"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Accuracy: 34.264 %, Loss: 1.5853\n",
            "Epoch [1/100], Learning Rate: 0.0001\n",
            "Epoch [2/100], Train Accuracy: 44.804 %, Loss: 1.5171\n",
            "Epoch [2/100], Learning Rate: 0.00019\n",
            "Epoch [3/100], Train Accuracy: 48.652 %, Loss: 1.4178\n",
            "Epoch [3/100], Learning Rate: 0.00028\n",
            "Epoch [4/100], Train Accuracy: 51.808 %, Loss: 1.2680\n",
            "Epoch [4/100], Learning Rate: 0.00036999999999999994\n",
            "Epoch [5/100], Train Accuracy: 55.424 %, Loss: 1.2615\n",
            "Epoch [5/100], Learning Rate: 0.00045999999999999996\n",
            "Epoch [6/100], Train Accuracy: 58.682 %, Loss: 1.1441\n",
            "Epoch [6/100], Learning Rate: 0.00055\n",
            "Epoch [7/100], Train Accuracy: 60.832 %, Loss: 1.0793\n",
            "Epoch [7/100], Learning Rate: 0.0006399999999999999\n",
            "Epoch [8/100], Train Accuracy: 63.092 %, Loss: 0.9353\n",
            "Epoch [8/100], Learning Rate: 0.00073\n",
            "Epoch [9/100], Train Accuracy: 64.804 %, Loss: 1.1589\n",
            "Epoch [9/100], Learning Rate: 0.00082\n",
            "Epoch [10/100], Train Accuracy: 66.596 %, Loss: 0.9040\n",
            "Epoch [10/100], Learning Rate: 0.00091\n",
            "Epoch [11/100], Train Accuracy: 67.682 %, Loss: 0.8388\n",
            "Epoch [11/100], Learning Rate: 0.001\n",
            "Epoch [12/100], Train Accuracy: 69.116 %, Loss: 0.8568\n",
            "Epoch [12/100], Learning Rate: 0.0009997779521645793\n",
            "Epoch [13/100], Train Accuracy: 69.944 %, Loss: 0.8142\n",
            "Epoch [13/100], Learning Rate: 0.0009991120277927223\n",
            "Epoch [14/100], Train Accuracy: 71.294 %, Loss: 0.8347\n",
            "Epoch [14/100], Learning Rate: 0.000998002884071386\n",
            "Epoch [15/100], Train Accuracy: 71.668 %, Loss: 0.7917\n",
            "Epoch [15/100], Learning Rate: 0.000996451615591515\n",
            "Epoch [16/100], Train Accuracy: 72.53 %, Loss: 0.7395\n",
            "Epoch [16/100], Learning Rate: 0.000994459753267812\n",
            "Epoch [17/100], Train Accuracy: 73.576 %, Loss: 0.7167\n",
            "Epoch [17/100], Learning Rate: 0.00099202926282791\n",
            "Epoch [18/100], Train Accuracy: 73.51 %, Loss: 0.7196\n",
            "Epoch [18/100], Learning Rate: 0.0009891625428724363\n",
            "Epoch [19/100], Train Accuracy: 74.362 %, Loss: 0.7537\n",
            "Epoch [19/100], Learning Rate: 0.000985862422507884\n",
            "Epoch [20/100], Train Accuracy: 74.932 %, Loss: 0.6097\n",
            "Epoch [20/100], Learning Rate: 0.0009821321585546244\n",
            "Epoch [21/100], Train Accuracy: 75.518 %, Loss: 0.6535\n",
            "Epoch [21/100], Learning Rate: 0.0009779754323328191\n",
            "Epoch [22/100], Train Accuracy: 75.554 %, Loss: 0.6976\n",
            "Epoch [22/100], Learning Rate: 0.0009733963460294015\n",
            "Epoch [23/100], Train Accuracy: 75.764 %, Loss: 0.6523\n",
            "Epoch [23/100], Learning Rate: 0.0009683994186497131\n",
            "Epoch [24/100], Train Accuracy: 76.31 %, Loss: 0.6652\n",
            "Epoch [24/100], Learning Rate: 0.0009629895815577914\n",
            "Epoch [25/100], Train Accuracy: 76.498 %, Loss: 0.7507\n",
            "Epoch [25/100], Learning Rate: 0.0009571721736097088\n",
            "Epoch [26/100], Train Accuracy: 76.806 %, Loss: 0.6546\n",
            "Epoch [26/100], Learning Rate: 0.0009509529358847655\n",
            "Epoch [27/100], Train Accuracy: 76.524 %, Loss: 0.7011\n",
            "Epoch [27/100], Learning Rate: 0.0009443380060197386\n",
            "Epoch [28/100], Train Accuracy: 77.188 %, Loss: 0.7011\n",
            "Epoch [28/100], Learning Rate: 0.0009373339121517746\n",
            "Epoch [29/100], Train Accuracy: 77.67 %, Loss: 0.6126\n",
            "Epoch [29/100], Learning Rate: 0.0009299475664759069\n",
            "Epoch [30/100], Train Accuracy: 77.91 %, Loss: 0.6944\n",
            "Epoch [30/100], Learning Rate: 0.0009221862584235528\n",
            "Epoch [31/100], Train Accuracy: 77.774 %, Loss: 0.5939\n",
            "Epoch [31/100], Learning Rate: 0.0009140576474687263\n",
            "Epoch [32/100], Train Accuracy: 78.282 %, Loss: 0.6270\n",
            "Epoch [32/100], Learning Rate: 0.0009055697555690606\n",
            "Epoch [33/100], Train Accuracy: 78.622 %, Loss: 0.6142\n",
            "Epoch [33/100], Learning Rate: 0.0008967309592491052\n",
            "Epoch [34/100], Train Accuracy: 78.832 %, Loss: 0.6544\n",
            "Epoch [34/100], Learning Rate: 0.0008875499813337069\n",
            "Epoch [35/100], Train Accuracy: 79.146 %, Loss: 0.6865\n",
            "Epoch [35/100], Learning Rate: 0.0008780358823396353\n",
            "Epoch [36/100], Train Accuracy: 79.144 %, Loss: 0.4768\n",
            "Epoch [36/100], Learning Rate: 0.0008681980515339464\n",
            "Epoch [37/100], Train Accuracy: 79.454 %, Loss: 0.5179\n",
            "Epoch [37/100], Learning Rate: 0.0008580461976679099\n",
            "Epoch [38/100], Train Accuracy: 79.764 %, Loss: 0.6822\n",
            "Epoch [38/100], Learning Rate: 0.0008475903393956434\n",
            "Epoch [39/100], Train Accuracy: 79.736 %, Loss: 0.5878\n",
            "Epoch [39/100], Learning Rate: 0.0008368407953869105\n",
            "Epoch [40/100], Train Accuracy: 79.892 %, Loss: 0.6376\n",
            "Epoch [40/100], Learning Rate: 0.0008258081741438395\n",
            "Epoch [41/100], Train Accuracy: 80.646 %, Loss: 0.5802\n",
            "Epoch [41/100], Learning Rate: 0.0008145033635316131\n",
            "Epoch [42/100], Train Accuracy: 80.134 %, Loss: 0.5354\n",
            "Epoch [42/100], Learning Rate: 0.0008029375200334589\n",
            "Epoch [43/100], Train Accuracy: 80.82 %, Loss: 0.5949\n",
            "Epoch [43/100], Learning Rate: 0.0007911220577405485\n",
            "Epoch [44/100], Train Accuracy: 80.772 %, Loss: 0.6541\n",
            "Epoch [44/100], Learning Rate: 0.0007790686370876671\n",
            "Epoch [45/100], Train Accuracy: 80.99 %, Loss: 0.6641\n",
            "Epoch [45/100], Learning Rate: 0.0007667891533457719\n",
            "Epoch [46/100], Train Accuracy: 80.986 %, Loss: 0.5253\n",
            "Epoch [46/100], Learning Rate: 0.000754295724882796\n",
            "Epoch [47/100], Train Accuracy: 81.526 %, Loss: 0.6200\n",
            "Epoch [47/100], Learning Rate: 0.0007416006812042827\n",
            "Epoch [48/100], Train Accuracy: 81.452 %, Loss: 0.5171\n",
            "Epoch [48/100], Learning Rate: 0.0007287165507856513\n",
            "Epoch [49/100], Train Accuracy: 81.904 %, Loss: 0.5397\n",
            "Epoch [49/100], Learning Rate: 0.0007156560487081052\n",
            "Epoch [50/100], Train Accuracy: 82.272 %, Loss: 0.5089\n",
            "Epoch [50/100], Learning Rate: 0.0007024320641103813\n",
            "Epoch [51/100], Train Accuracy: 82.382 %, Loss: 0.5023\n",
            "Epoch [51/100], Learning Rate: 0.0006890576474687264\n",
            "Epoch [52/100], Train Accuracy: 82.432 %, Loss: 0.4492\n",
            "Epoch [52/100], Learning Rate: 0.0006755459977176532\n",
            "Epoch [53/100], Train Accuracy: 82.858 %, Loss: 0.4958\n",
            "Epoch [53/100], Learning Rate: 0.0006619104492241846\n",
            "Epoch [54/100], Train Accuracy: 83.062 %, Loss: 0.5128\n",
            "Epoch [54/100], Learning Rate: 0.0006481644586284443\n",
            "Epoch [55/100], Train Accuracy: 83.224 %, Loss: 0.4781\n",
            "Epoch [55/100], Learning Rate: 0.0006343215915635762\n",
            "Epoch [56/100], Train Accuracy: 83.632 %, Loss: 0.4820\n",
            "Epoch [56/100], Learning Rate: 0.000620395509268104\n",
            "Epoch [57/100], Train Accuracy: 83.654 %, Loss: 0.4319\n",
            "Epoch [57/100], Learning Rate: 0.000606399955103937\n",
            "Epoch [58/100], Train Accuracy: 84.02 %, Loss: 0.4709\n",
            "Epoch [58/100], Learning Rate: 0.0005923487409933315\n",
            "Epoch [59/100], Train Accuracy: 84.386 %, Loss: 0.4588\n",
            "Epoch [59/100], Learning Rate: 0.0005782557337881911\n",
            "Epoch [60/100], Train Accuracy: 84.432 %, Loss: 0.4689\n",
            "Epoch [60/100], Learning Rate: 0.0005641348415851578\n",
            "Epoch [61/100], Train Accuracy: 84.416 %, Loss: 0.4224\n",
            "Epoch [61/100], Learning Rate: 0.00055\n",
            "Epoch [62/100], Train Accuracy: 84.602 %, Loss: 0.5977\n",
            "Epoch [62/100], Learning Rate: 0.0005358651584148423\n",
            "Epoch [63/100], Train Accuracy: 84.928 %, Loss: 0.5070\n",
            "Epoch [63/100], Learning Rate: 0.000521744266211809\n",
            "Epoch [64/100], Train Accuracy: 84.744 %, Loss: 0.5204\n",
            "Epoch [64/100], Learning Rate: 0.0005076512590066686\n",
            "Epoch [65/100], Train Accuracy: 85.534 %, Loss: 0.3997\n",
            "Epoch [65/100], Learning Rate: 0.0004936000448960632\n",
            "Epoch [66/100], Train Accuracy: 85.646 %, Loss: 0.4451\n",
            "Epoch [66/100], Learning Rate: 0.0004796044907318961\n",
            "Epoch [67/100], Train Accuracy: 86.042 %, Loss: 0.4345\n",
            "Epoch [67/100], Learning Rate: 0.0004656784084364239\n",
            "Epoch [68/100], Train Accuracy: 86.062 %, Loss: 0.4014\n",
            "Epoch [68/100], Learning Rate: 0.00045183554137155597\n",
            "Epoch [69/100], Train Accuracy: 86.37 %, Loss: 0.4998\n",
            "Epoch [69/100], Learning Rate: 0.0004380895507758153\n",
            "Epoch [70/100], Train Accuracy: 86.382 %, Loss: 0.3253\n",
            "Epoch [70/100], Learning Rate: 0.00042445400228234684\n",
            "Epoch [71/100], Train Accuracy: 86.416 %, Loss: 0.4369\n",
            "Epoch [71/100], Learning Rate: 0.0004109423525312738\n",
            "Epoch [72/100], Train Accuracy: 86.784 %, Loss: 0.3383\n",
            "Epoch [72/100], Learning Rate: 0.0003975679358896189\n",
            "Epoch [73/100], Train Accuracy: 87.126 %, Loss: 0.4122\n",
            "Epoch [73/100], Learning Rate: 0.000384343951291895\n",
            "Epoch [74/100], Train Accuracy: 87.114 %, Loss: 0.3849\n",
            "Epoch [74/100], Learning Rate: 0.00037128344921434864\n",
            "Epoch [75/100], Train Accuracy: 87.788 %, Loss: 0.3513\n",
            "Epoch [75/100], Learning Rate: 0.0003583993187957173\n",
            "Epoch [76/100], Train Accuracy: 87.898 %, Loss: 0.4027\n",
            "Epoch [76/100], Learning Rate: 0.00034570427511720393\n",
            "Epoch [77/100], Train Accuracy: 88.004 %, Loss: 0.3915\n",
            "Epoch [77/100], Learning Rate: 0.0003332108466542281\n",
            "Epoch [78/100], Train Accuracy: 88.222 %, Loss: 0.4017\n",
            "Epoch [78/100], Learning Rate: 0.0003209313629123329\n",
            "Epoch [79/100], Train Accuracy: 88.44 %, Loss: 0.2929\n",
            "Epoch [79/100], Learning Rate: 0.0003088779422594514\n",
            "Epoch [80/100], Train Accuracy: 88.61 %, Loss: 0.3072\n",
            "Epoch [80/100], Learning Rate: 0.0002970624799665412\n",
            "Epoch [81/100], Train Accuracy: 88.646 %, Loss: 0.4234\n",
            "Epoch [81/100], Learning Rate: 0.00028549663646838715\n",
            "Epoch [82/100], Train Accuracy: 89.428 %, Loss: 0.3288\n",
            "Epoch [82/100], Learning Rate: 0.00027419182585616055\n",
            "Epoch [83/100], Train Accuracy: 89.328 %, Loss: 0.2597\n",
            "Epoch [83/100], Learning Rate: 0.0002631592046130896\n",
            "Epoch [84/100], Train Accuracy: 89.666 %, Loss: 0.2651\n",
            "Epoch [84/100], Learning Rate: 0.00025240966060435674\n",
            "Epoch [85/100], Train Accuracy: 89.848 %, Loss: 0.3516\n",
            "Epoch [85/100], Learning Rate: 0.00024195380233209008\n",
            "Epoch [86/100], Train Accuracy: 89.988 %, Loss: 0.2687\n",
            "Epoch [86/100], Learning Rate: 0.00023180194846605365\n",
            "Epoch [87/100], Train Accuracy: 90.108 %, Loss: 0.2880\n",
            "Epoch [87/100], Learning Rate: 0.0002219641176603649\n",
            "Epoch [88/100], Train Accuracy: 90.242 %, Loss: 0.2874\n",
            "Epoch [88/100], Learning Rate: 0.0002124500186662932\n",
            "Epoch [89/100], Train Accuracy: 90.838 %, Loss: 0.1984\n",
            "Epoch [89/100], Learning Rate: 0.00020326904075089488\n",
            "Epoch [90/100], Train Accuracy: 90.748 %, Loss: 0.2345\n",
            "Epoch [90/100], Learning Rate: 0.00019443024443093932\n",
            "Epoch [91/100], Train Accuracy: 91.146 %, Loss: 0.2059\n",
            "Epoch [91/100], Learning Rate: 0.00018594235253127368\n",
            "Epoch [92/100], Train Accuracy: 91.334 %, Loss: 0.1998\n",
            "Epoch [92/100], Learning Rate: 0.00017781374157644722\n",
            "Epoch [93/100], Train Accuracy: 91.65 %, Loss: 0.2151\n",
            "Epoch [93/100], Learning Rate: 0.00017005243352409332\n",
            "Epoch [94/100], Train Accuracy: 91.584 %, Loss: 0.2442\n",
            "Epoch [94/100], Learning Rate: 0.0001626660878482253\n",
            "Epoch [95/100], Train Accuracy: 91.992 %, Loss: 0.2895\n",
            "Epoch [95/100], Learning Rate: 0.0001556619939802614\n",
            "Epoch [96/100], Train Accuracy: 91.958 %, Loss: 0.2374\n",
            "Epoch [96/100], Learning Rate: 0.0001490470641152345\n",
            "Epoch [97/100], Train Accuracy: 91.868 %, Loss: 0.2656\n",
            "Epoch [97/100], Learning Rate: 0.0001428278263902913\n",
            "Epoch [98/100], Train Accuracy: 92.182 %, Loss: 0.1809\n",
            "Epoch [98/100], Learning Rate: 0.00013701041844220858\n",
            "Epoch [99/100], Train Accuracy: 92.376 %, Loss: 0.2253\n",
            "Epoch [99/100], Learning Rate: 0.0001316005813502869\n",
            "Epoch [100/100], Train Accuracy: 92.528 %, Loss: 0.2577\n",
            "Epoch [100/100], Learning Rate: 0.00012660365397059855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習前後のモデルの重みを比較する関数を作成\n",
        "def check_weight_change(model, model_init):\n",
        "    weights_changed = (model.conv1.state_dict()['weight'] != model_init.conv1.state_dict()['weight']).any()\n",
        "    if weights_changed:\n",
        "        return print('モデルの重みが変化しています')\n",
        "    else:\n",
        "        return print('モデルの重みは変化していません')\n",
        "# 学習前後でモデルの重みが変化していないかを確認\n",
        "check_weight_change(model, model_init)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0fnIdmc4QVc",
        "outputId": "c73bba82-ebf3-425a-e098-55a1fdd6b025"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "モデルの重みは変化していません\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの評価\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for images, labels in test_loader_cifar10:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    # 正解率精度の確認\n",
        "    print('Test Accuracy: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpmEUkL_4QVc",
        "outputId": "04524ca3-669e-4778-e3c0-6b5a82589d01"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 81.79 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPUメモリの解放\n",
        "del model, model_init\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "vxmLrpUA4QVc"
      },
      "execution_count": 55,
      "outputs": []
    }
  ]
}
